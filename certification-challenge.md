# ğŸš¨ Traceback  
**Instant triage across docs, code & lineage.**

---

## ğŸ§© Problem Statement

On-call data engineers canâ€™t quickly determine **business impact** or **blast radius** during production pipeline incidents because requirements live in **Confluence or Google Docs** and pipeline code is buried in **Git repos** and column-level lineage is trapped in **catalogs or graph UIs** and these sources are fragmented and **not built for fast triage**.

### â— Why Is This a Problem?

When a pipeline breaks, engineers spend the first **30â€“60 minutes** just chasing answers:

> â€œWhat dashboards went stale?â€  
> â€œWhich tables and columns are wrong?â€  
> â€œWho else is affected?â€  
> â€œDo we roll back or hotfix?â€

The current state causes:

- ğŸ” **Inflated MTTR**  
- â±ï¸ **SLA breaches**  
- ğŸ“£ **Unclear updates to leadership**  
- ğŸ›‘ **Paused reports and broken trust**  
- âš ï¸ **Risky hotfixes shipped without context**

A focused triaging that understands **docs**, **code**, and **lineage** can:

- Reduce triage time  
- Improve fix decisions (rollback vs patch)  
- Standardize incident communication

### ğŸ¯ Audience

- **Primary:** On-call Data Engineers & Data Platform SREs  
- **Secondary:** Analytics Leads, Product Managers needing fast confirmation of data trust

---

## âœ… Solution: What is Traceback?

**Traceback** is a small, focused, agentic RAG app that unifies three fragmented sources to power fast, context-rich incident response:

- ğŸ“„ **Requirements docs** (PDF/MD)  
- ğŸ§¾ **Pipeline code snippets** (SQL/Py)  
- ğŸ§¬ **Column-level lineage graph** (JSON)

It answers natural-language questions like:

> â€œJob `curated.sales_orders` failed â€” who's impacted?â€

And returns a structured incident brief:

- ğŸ“Š **Impact Summary**  
- ğŸ’¥ **Blast Radius**  
- â“ **Likely Causes**  
- ğŸ› ï¸ **Safe Actions / Backfill Order** (with citations)



### âš™ï¸ Setup & Flow

For the challenge:

- All inputs are local:
  - `docs/` â€” PDFs, markdown  
  - `repo/` â€” SQL, Python, configs  
  - `lineage.json` â€” exported lightweight lineage graph
- System runs locally:
  - `FastAPI` endpoint  
  - Simple CLI
- Supports full end-to-end triage demo


### ğŸ§° Stack

| Component       | Tool                      | Why? |
|----------------|---------------------------|------|
| **LLM**         | OpenAI GPT-4.1-mini / GPT-4o-mini | Low latency, high reasoning |
| **Embeddings**  | `text-embedding-3-small`  | Cheap, accurate |
| **Orchestration** | LangGraph              | Agent loops, tool routing |
| **Vector Store**| Qdrant                    | Portable, zero-infra, fast |
| **Monitoring**  | LangSmith / JSON Logs     | Logs, traces, prompt captures |
| **Evaluation**  | RAGAS                     | Precision, recall, relevancy |
| **Interface**   | FastAPI + CLI + Swagger   | Fastest path to usable demo |


### ğŸ§  How Agentic Reasoning Works

- ğŸ” Supervisor Agent (LangGraph)
    - Interprets user question
    - Routes to appropriate tools
    - Stops when it gets:
        - âœ… Impact Summary
        - âœ… Blast Radius
        - âœ… Suggested Actions

- ğŸ› ï¸ Impact Assessor Agent
    - RAG over requirements/runbooks
    - Queries `lineage.json` graph
    - Code search over `repo/` (BM25)

- âœï¸ Writer Agent
    - Crafts incident brief
    - Adds citations
    - Suggests rollback or backfill steps

### ğŸ¤– Why Agentic?

- Breaks complex questions into subgoals  
- Chooses and sequences tools  
- Cross-validates retrieved context  
- Knows when enough evidence is gathered

---

## ğŸ—‚ï¸ Dealing with Data

### Data sources
- ğŸ“„ Requirements & Runbooks

    - Location: `data/docs/*.pdf|md`  
    - Use: definitions, SLAs, ownership, incident protocols  
    - Examples:  
        - `Data Quality Playbook.md`  
        - `Sales Orders Domain Spec.pdf`

- ğŸ§¾ Pipeline Code & Configs

    - Location: `data/repo/**/*.(sql|py|yaml)`  
    - Use: table transforms, job owners, PR hints  
    - Ingestion: parse as raw text, capture file paths

- ğŸ§¬ Column Lineage Graph

    - Location: `data/lineage.json`  
    - Use: compute blast radius (table/column level)  
    - Format: simple directed graph

        ```json
        {
        "nodes":[
            {"id":"raw.sales_orders","type":"table","owners":["data-sales"]},
            {"id":"curated.sales_orders","type":"table"},
            {"id":"curated.sales_orders.net_sales","type":"column"}
        ],
        "edges":[
            {"from":"raw.sales_orders","to":"curated.sales_orders","op":"clean+join"},
            {"from":"curated.sales_orders.gross_sales","to":"curated.sales_orders.net_sales","op":"subtract_refunds"}
        ],
        "dashboards":[
            {"id":"bi.rev_daily","tables":["curated.sales_orders"],"teams":["Merch","Finance"]}
        ]
        }

        ```
- External API (single, lightweight):

    - Tavily Search (or SERP API)

    - Purpose: optional fresh lookups for error messages/root causes or library changes; enrich responses with a link or two.

- Why these sources?

    They map 1:1 to the triage questions: 
    - docs = â€œwhat is correct?â€
    - code = â€œwhat changed?â€
    - lineage = â€œwhoâ€™s impacted?â€
    - A single web search tool to add reference links for library upgrades or any changes.

### Default Chunking Strategy
- Docs (PDF/MD): semantic sections ~350â€“600 tokens, 10â€“15% overlap.

    - Why: preserves meaning across headings/paragraphs; overlap helps recall for boundary Qs.

- Code (SQL/Python/YAML):

    - SQL: split by top-level statement (CTE/CREATE/INSERT); cap at 400â€“700 tokens.

- Python/YAML: split by function/class or top-level key blocks; cap at 400â€“700 tokens.

    - Why: code answers hinge on local scope; function/statement boundaries improve precision.

- Lineage: no vector chunking; kept in memory/JSON and queried via a graph tool. We still index small textual notes (node/edge annotations) into Qdrant with payload {"kind":"lineage_note"} to make them retrievable when helpful.

- Why not bigger/smaller chunks ? Smaller chunks increase recall but hurt faithfulness; larger chunks increase context precision but waste tokens. The selected window (350â€“700 tokens) is a practical sweet spot for RAG over mixed docs+code.
---


