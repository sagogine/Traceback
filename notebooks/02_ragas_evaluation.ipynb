{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02: RAGAS Evaluation (Standardized Dataset)\n",
    "\n",
    "## üéØ Objective\n",
    "Assess the RAG pipeline using the RAGAS framework with standardized 15-test-case dataset including key metrics faithfulness, response relevance, context precision, and context recall.\n",
    "\n",
    "### **Evaluation Dataset:**\n",
    "- **15 Business Specifications**: Complete domain coverage\n",
    "- **15 SQL Pipelines**: Comprehensive pipeline coverage  \n",
    "- **Comprehensive Lineage**: 40+ nodes, 35+ edges\n",
    "- **15 Standardized Test Cases**: Covering all business domains\n",
    "\n",
    "### **RAGAS Metrics:**\n",
    "- **Faithfulness** - Factual accuracy of responses\n",
    "- **Answer Relevancy** - Relevance of answers to questions\n",
    "- **Context Precision** - Precision of retrieved context\n",
    "- **Context Recall** - Recall of relevant context\n",
    "\n",
    "### **Expected Outcomes:**\n",
    "- **Comprehensive Performance Table**: Detailed metrics across all domains\n",
    "- **Domain-Specific Analysis**: Performance by business area\n",
    "- **Actionable Recommendations**: Specific improvement areas\n",
    "\n",
    "## Test Coverage\n",
    "- **10 Business Domains**: Sales Orders, Customer Analytics, Inventory Management, Financial Reporting, Marketing Attribution, Supply Chain, HR Analytics, Product Analytics, Risk Management, Compliance Monitoring\n",
    "- **5 Operational Guides**: Incident Playbook, Data Quality Standards, Troubleshooting Guide, SLA Definitions, Escalation Procedures\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Environment setup complete\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Verify API keys\n",
    "if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "    raise RuntimeError(\"OPENAI_API_KEY is not set. Create a .env file or export it in your shell.\")\n",
    "\n",
    "print(\"‚úÖ Environment setup complete\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Imports complete\n"
     ]
    }
   ],
   "source": [
    "# Add src to path for imports\n",
    "sys.path.insert(0, str(Path.cwd().parent / \"src\"))\n",
    "\n",
    "# Import RAGAS components\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import (\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    "    context_precision,\n",
    "    context_recall\n",
    ")\n",
    "from datasets import Dataset\n",
    "\n",
    "# Import our Traceback system\n",
    "from tracebackcore.core import traceback_graph, lineage_retriever, AgentState, initialize_system\n",
    "\n",
    "print(\"‚úÖ Imports complete\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded standardized golden test dataset with 15 comprehensive test cases\n",
      "ÔøΩÔøΩ Test cases cover all 15 business domains:\n",
      "   1. What should I do if the sales orders pipeline fails?...\n",
      "   2. How does the customer analytics pipeline segment customers?...\n",
      "   3. What are the stock level management rules for inventory?...\n",
      "   4. What financial controls are implemented in the reporting pip...\n",
      "   5. What attribution models are used for marketing campaigns?...\n",
      "   6. How is supplier performance measured in the supply chain?...\n",
      "   7. What employee metrics are tracked in the HR analytics pipeli...\n",
      "   8. What product usage metrics are monitored in real-time?...\n",
      "   9. What risk management controls are implemented?...\n",
      "  10. What compliance monitoring procedures are in place?...\n",
      "  11. What are the escalation procedures for data pipeline inciden...\n",
      "  12. What are the data quality standards and monitoring?...\n",
      "  13. What are the SLA definitions and monitoring procedures?...\n",
      "  14. What are the troubleshooting procedures for data pipeline is...\n",
      "  15. What are the common failure patterns in data pipelines?...\n"
     ]
    }
   ],
   "source": [
    "# Load standardized golden test data\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Load the standardized golden test data (from project root)\n",
    "golden_test_data_path = Path(\"../data/golden_test_data.json\")\n",
    "with open(golden_test_data_path, \"r\") as f:\n",
    "    golden_test_data = json.load(f)\n",
    "\n",
    "print(f\"‚úÖ Loaded standardized golden test dataset with {len(golden_test_data)} comprehensive test cases\")\n",
    "print(\"ÔøΩÔøΩ Test cases cover all 15 business domains:\")\n",
    "for i, test_case in enumerate(golden_test_data, 1):\n",
    "    print(f\"  {i:2d}. {test_case[\"question\"][:60]}...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Initialize Traceback System\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Initializing Traceback system...\n",
      "üöÄ Initializing Traceback system...\n",
      "üìö Loading all specifications and SQL pipelines...\n",
      "‚úÖ Loaded 30 documents (15 specs, 15 SQL files)\n",
      "‚úÖ Loaded comprehensive lineage data: 13 nodes, 13 edges\n",
      "‚úÖ Traceback system initialized successfully\n",
      "‚úÖ Traceback system initialized successfully\n"
     ]
    }
   ],
   "source": [
    "# Initialize the Traceback system\n",
    "print(\"üöÄ Initializing Traceback system...\")\n",
    "initialize_system()\n",
    "print(\"‚úÖ Traceback system initialized successfully\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create Golden Test Dataset\n",
    "\n",
    "We'll create a comprehensive test dataset covering various incident scenarios with ground truth answers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Using standardized golden test data from data/golden_test_data.json\n",
      "üìä Test cases loaded: 15\n",
      "üéØ Both evaluations now use identical test data for fair comparison\n"
     ]
    }
   ],
   "source": [
    "# Note: Golden test data is now loaded from standardized JSON file in cell above\n",
    "# This ensures consistency between RAGAS evaluation and Advanced Retrieval evaluation\n",
    "\n",
    "print(\"‚úÖ Using standardized golden test data from data/golden_test_data.json\")\n",
    "print(f\"üìä Test cases loaded: {len(golden_test_data)}\")\n",
    "print(\"üéØ Both evaluations now use identical test data for fair comparison\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Generate Responses Using Traceback System\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Response generation function defined\n"
     ]
    }
   ],
   "source": [
    "def generate_traceback_response(question: str) -> Dict[str, Any]:\n",
    "    \"\"\"Generate response using our Traceback system.\"\"\"\n",
    "    try:\n",
    "        # Create initial state\n",
    "        initial_state = AgentState(\n",
    "            question=question,\n",
    "            context=[],\n",
    "            impact_assessment=None,\n",
    "            blast_radius=None,\n",
    "            recommended_actions=None,\n",
    "            incident_brief=None,\n",
    "            current_step=\"supervisor\",\n",
    "            error=None\n",
    "        )\n",
    "        \n",
    "        # Run the workflow\n",
    "        result = traceback_graph.invoke(initial_state)\n",
    "        \n",
    "        # Get retrieved context from the lineage retriever\n",
    "        retrieved_docs = lineage_retriever.search_with_lineage(question, k=5)\n",
    "        retrieved_contexts = [doc.page_content for doc in retrieved_docs]\n",
    "        \n",
    "        # Extract relevant information\n",
    "        return {\n",
    "            \"answer\": result.get(\"incident_brief\", \"No response generated\"),\n",
    "            \"context\": retrieved_contexts,  # Use actual retrieved context\n",
    "            \"blast_radius\": result.get(\"blast_radius\", []),\n",
    "            \"impact_assessment\": result.get(\"impact_assessment\", {})\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"answer\": f\"Error generating response: {str(e)}\",\n",
    "            \"context\": [],\n",
    "            \"blast_radius\": [],\n",
    "            \"impact_assessment\": {}\n",
    "        }\n",
    "\n",
    "print(\"‚úÖ Response generation function defined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Generating responses using Traceback system...\n",
      "Processing test case 1/15: What should I do if the sales orders pipeline fail...\n",
      "Processing test case 2/15: How does the customer analytics pipeline segment c...\n",
      "Processing test case 3/15: What are the stock level management rules for inve...\n",
      "Processing test case 4/15: What financial controls are implemented in the rep...\n",
      "Processing test case 5/15: What attribution models are used for marketing cam...\n",
      "Processing test case 6/15: How is supplier performance measured in the supply...\n",
      "Processing test case 7/15: What employee metrics are tracked in the HR analyt...\n",
      "Processing test case 8/15: What product usage metrics are monitored in real-t...\n",
      "Processing test case 9/15: What risk management controls are implemented?...\n",
      "Processing test case 10/15: What compliance monitoring procedures are in place...\n",
      "Processing test case 11/15: What are the escalation procedures for data pipeli...\n",
      "Processing test case 12/15: What are the data quality standards and monitoring...\n",
      "Processing test case 13/15: What are the SLA definitions and monitoring proced...\n",
      "Processing test case 14/15: What are the troubleshooting procedures for data p...\n",
      "Processing test case 15/15: What are the common failure patterns in data pipel...\n",
      "‚úÖ Generated responses for 15 test cases\n"
     ]
    }
   ],
   "source": [
    "# Generate responses for all test cases\n",
    "print(\"üîÑ Generating responses using Traceback system...\")\n",
    "\n",
    "evaluation_data = []\n",
    "for i, test_case in enumerate(golden_test_data):\n",
    "    print(f\"Processing test case {i+1}/{len(golden_test_data)}: {test_case['question'][:50]}...\")\n",
    "    \n",
    "    # Generate response\n",
    "    response = generate_traceback_response(test_case[\"question\"])\n",
    "    \n",
    "    # Prepare data for RAGAS evaluation\n",
    "    # Use actual retrieved context from our system, not predefined context\n",
    "    evaluation_data.append({\n",
    "        \"question\": test_case[\"question\"],\n",
    "        \"answer\": response[\"answer\"],\n",
    "        \"contexts\": response[\"context\"],  # Use actual retrieved context\n",
    "        \"ground_truth\": test_case[\"ground_truth\"]\n",
    "    })\n",
    "\n",
    "print(f\"‚úÖ Generated responses for {len(evaluation_data)} test cases\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. RAGAS Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä RAGAS dataset created with 15 samples\n",
      "Dataset columns: ['question', 'answer', 'contexts', 'ground_truth']\n",
      "\n",
      "üîç Sample data format verification:\n",
      "Question: What should I do if the sales orders pipeline fail...\n",
      "Answer length: 3775 characters\n",
      "Contexts count: 5\n",
      "Contexts type: <class 'list'>\n",
      "First context: -- Sales Orders Pipeline\n",
      "-- Purpose: Transform raw...\n",
      "Ground truth length: 432 characters\n"
     ]
    }
   ],
   "source": [
    "# Convert to RAGAS Dataset format\n",
    "ragas_dataset = Dataset.from_list(evaluation_data)\n",
    "\n",
    "print(f\"üìä RAGAS dataset created with {len(ragas_dataset)} samples\")\n",
    "print(f\"Dataset columns: {ragas_dataset.column_names}\")\n",
    "\n",
    "# Verify the data format\n",
    "print(\"\\nüîç Sample data format verification:\")\n",
    "sample = ragas_dataset[0]\n",
    "print(f\"Question: {sample['question'][:50]}...\")\n",
    "print(f\"Answer length: {len(sample['answer'])} characters\")\n",
    "print(f\"Contexts count: {len(sample['contexts'])}\")\n",
    "print(f\"Contexts type: {type(sample['contexts'])}\")\n",
    "print(f\"First context: {sample['contexts'][0][:50]}...\")\n",
    "print(f\"Ground truth length: {len(sample['ground_truth'])} characters\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note: RAGAS EvaluationResult Object\n",
    "RAGAS returns an `EvaluationResult` object, not a dictionary. To access the results, use:\n",
    "- `result.to_pandas()` to get a DataFrame\n",
    "- `result.samples` to get individual sample results\n",
    "- `result.metrics` to get metric names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Testing RAGAS evaluation with first 2 samples...\n",
      "üîÑ Running RAGAS evaluation on test subset...\n",
      "This may take a few minutes...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28e1383272c34875bedaa98b274f84a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ RAGAS test evaluation completed!\n",
      "Test result type: <class 'ragas.dataset_schema.EvaluationResult'>\n",
      "Test results shape: (2, 8)\n",
      "Test results columns: ['user_input', 'retrieved_contexts', 'response', 'reference', 'faithfulness', 'answer_relevancy', 'context_precision', 'context_recall']\n",
      "\n",
      "üìä Test Results Summary:\n",
      "faithfulness: 0.381\n",
      "answer_relevancy: 0.923\n",
      "context_precision: 1.000\n",
      "context_recall: 0.524\n"
     ]
    }
   ],
   "source": [
    "# Test with a smaller subset first to verify everything works\n",
    "print(\"üß™ Testing RAGAS evaluation with first 2 samples...\")\n",
    "\n",
    "# Create a small test dataset\n",
    "test_dataset = Dataset.from_list(evaluation_data[:2])\n",
    "\n",
    "# Define metrics to evaluate\n",
    "metrics = [\n",
    "    faithfulness,      # How factually accurate are the responses?\n",
    "    answer_relevancy, # How relevant are the responses to the questions?\n",
    "    context_precision, # How precise is the retrieved context?\n",
    "    context_recall     # How well does the context cover the ground truth?\n",
    "]\n",
    "\n",
    "print(\"üîÑ Running RAGAS evaluation on test subset...\")\n",
    "print(\"This may take a few minutes...\")\n",
    "\n",
    "# Run evaluation on test subset\n",
    "test_result = evaluate(\n",
    "    test_dataset,\n",
    "    metrics=metrics\n",
    ")\n",
    "\n",
    "print(\"‚úÖ RAGAS test evaluation completed!\")\n",
    "print(f\"Test result type: {type(test_result)}\")\n",
    "\n",
    "# Convert to pandas DataFrame to see the results\n",
    "test_df = test_result.to_pandas()\n",
    "print(f\"Test results shape: {test_df.shape}\")\n",
    "print(f\"Test results columns: {list(test_df.columns)}\")\n",
    "print(\"\\nüìä Test Results Summary:\")\n",
    "for metric in ['faithfulness', 'answer_relevancy', 'context_precision', 'context_recall']:\n",
    "    if metric in test_df.columns:\n",
    "        print(f\"{metric}: {test_df[metric].mean():.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìã Detailed Test Results:\n",
      "==================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_input</th>\n",
       "      <th>retrieved_contexts</th>\n",
       "      <th>response</th>\n",
       "      <th>reference</th>\n",
       "      <th>faithfulness</th>\n",
       "      <th>answer_relevancy</th>\n",
       "      <th>context_precision</th>\n",
       "      <th>context_recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What should I do if the sales orders pipeline ...</td>\n",
       "      <td>[-- Sales Orders Pipeline\\n-- Purpose: Transfo...</td>\n",
       "      <td># Incident Brief: Sales Orders Pipeline Failur...</td>\n",
       "      <td>If the sales orders pipeline fails: 1) Check p...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.910627</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How does the customer analytics pipeline segme...</td>\n",
       "      <td>[# Customer Analytics Pipeline Specification\\n...</td>\n",
       "      <td># Incident Brief: Customer Analytics Pipeline ...</td>\n",
       "      <td>The customer analytics pipeline segments custo...</td>\n",
       "      <td>0.761905</td>\n",
       "      <td>0.935875</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          user_input  \\\n",
       "0  What should I do if the sales orders pipeline ...   \n",
       "1  How does the customer analytics pipeline segme...   \n",
       "\n",
       "                                  retrieved_contexts  \\\n",
       "0  [-- Sales Orders Pipeline\\n-- Purpose: Transfo...   \n",
       "1  [# Customer Analytics Pipeline Specification\\n...   \n",
       "\n",
       "                                            response  \\\n",
       "0  # Incident Brief: Sales Orders Pipeline Failur...   \n",
       "1  # Incident Brief: Customer Analytics Pipeline ...   \n",
       "\n",
       "                                           reference  faithfulness  \\\n",
       "0  If the sales orders pipeline fails: 1) Check p...      0.000000   \n",
       "1  The customer analytics pipeline segments custo...      0.761905   \n",
       "\n",
       "   answer_relevancy  context_precision  context_recall  \n",
       "0          0.910627                1.0        0.714286  \n",
       "1          0.935875                1.0        0.333333  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Test PASSED: Ready for full evaluation\n"
     ]
    }
   ],
   "source": [
    "# Display detailed test results\n",
    "print(\"üìã Detailed Test Results:\")\n",
    "print(\"=\" * 50)\n",
    "display(test_df)\n",
    "\n",
    "# Check if test passed (all metrics > 0)\n",
    "test_passed = all(test_df[metric].mean() > 0 for metric in ['faithfulness', 'answer_relevancy'] if metric in test_df.columns)\n",
    "print(f\"\\n‚úÖ Test {'PASSED' if test_passed else 'FAILED'}: Ready for full evaluation\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Running full RAGAS evaluation on all samples...\n",
      "This may take several minutes...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16a3023e817f45689e92517025034037",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ RAGAS evaluation completed!\n"
     ]
    }
   ],
   "source": [
    "# Run full evaluation on all samples\n",
    "print(\"üöÄ Running full RAGAS evaluation on all samples...\")\n",
    "print(\"This may take several minutes...\")\n",
    "\n",
    "# Run evaluation on full dataset\n",
    "result = evaluate(\n",
    "    ragas_dataset,\n",
    "    metrics=metrics\n",
    ")\n",
    "\n",
    "print(\"‚úÖ RAGAS evaluation completed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Results Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä RAGAS Evaluation Results:\n",
      "==================================================\n",
      "\n",
      "üéØ Overall Performance Metrics:\n",
      "Faithfulness        : 0.494\n",
      "Answer Relevancy    : 0.889\n",
      "Context Precision   : 0.717\n",
      "Context Recall      : 0.436\n",
      "\n",
      "üìã Detailed Results:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_input</th>\n",
       "      <th>retrieved_contexts</th>\n",
       "      <th>response</th>\n",
       "      <th>reference</th>\n",
       "      <th>faithfulness</th>\n",
       "      <th>answer_relevancy</th>\n",
       "      <th>context_precision</th>\n",
       "      <th>context_recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What should I do if the sales orders pipeline ...</td>\n",
       "      <td>[-- Sales Orders Pipeline\\n-- Purpose: Transfo...</td>\n",
       "      <td># Incident Brief: Sales Orders Pipeline Failur...</td>\n",
       "      <td>If the sales orders pipeline fails: 1) Check p...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.910627</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How does the customer analytics pipeline segme...</td>\n",
       "      <td>[# Customer Analytics Pipeline Specification\\n...</td>\n",
       "      <td># Incident Brief: Customer Analytics Pipeline ...</td>\n",
       "      <td>The customer analytics pipeline segments custo...</td>\n",
       "      <td>0.893617</td>\n",
       "      <td>0.935875</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What are the stock level management rules for ...</td>\n",
       "      <td>[-- Inventory Management Pipeline\\n-- Purpose:...</td>\n",
       "      <td># Incident Brief: Inventory Stock Level Manage...</td>\n",
       "      <td>Inventory stock level management rules include...</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>0.902031</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What financial controls are implemented in the...</td>\n",
       "      <td>[# Financial Reporting Pipeline Specification\\...</td>\n",
       "      <td># Incident Brief: Financial Controls in Report...</td>\n",
       "      <td>Financial reporting pipeline controls include:...</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.946052</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What attribution models are used for marketing...</td>\n",
       "      <td>[# Marketing Attribution Pipeline Specificatio...</td>\n",
       "      <td># Incident Brief: Marketing Attribution Models...</td>\n",
       "      <td>Marketing attribution models include: 1) First...</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.896943</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>How is supplier performance measured in the su...</td>\n",
       "      <td>[-- Supply Chain Analytics Pipeline\\n-- Purpos...</td>\n",
       "      <td># Incident Brief: Supplier Performance Measure...</td>\n",
       "      <td>Supplier performance measurement includes: 1) ...</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.887601</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>What employee metrics are tracked in the HR an...</td>\n",
       "      <td>[-- HR Analytics Pipeline\\n-- Purpose: Employe...</td>\n",
       "      <td># Incident Brief: HR Analytics Pipeline Disrup...</td>\n",
       "      <td>HR analytics tracks employee metrics including...</td>\n",
       "      <td>0.295455</td>\n",
       "      <td>0.888966</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>What product usage metrics are monitored in re...</td>\n",
       "      <td>[-- Product Analytics Pipeline\\n-- Purpose: Co...</td>\n",
       "      <td># Incident Brief: Product Usage Metrics Monito...</td>\n",
       "      <td>Real-time product usage metrics include: 1) DA...</td>\n",
       "      <td>0.448276</td>\n",
       "      <td>0.890654</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>What risk management controls are implemented?</td>\n",
       "      <td>[# Risk Management Pipeline Specification\\n\\n#...</td>\n",
       "      <td># Incident Brief: Traceback Incident Triage\\n\\...</td>\n",
       "      <td>Risk management controls include: 1) Credit ri...</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.855866</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>What compliance monitoring procedures are in p...</td>\n",
       "      <td>[# Compliance Monitoring Pipeline Specificatio...</td>\n",
       "      <td># Incident Brief: Compliance Monitoring Proced...</td>\n",
       "      <td>Compliance monitoring procedures include: 1) R...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.893557</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>What are the escalation procedures for data pi...</td>\n",
       "      <td>[# Incident Escalation Procedures\\n\\n## Escala...</td>\n",
       "      <td># Incident Brief\\n\\n## 1. Incident Summary\\nOn...</td>\n",
       "      <td>Data pipeline incident escalation procedures: ...</td>\n",
       "      <td>0.128205</td>\n",
       "      <td>0.921669</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>What are the data quality standards and monito...</td>\n",
       "      <td>[# Data Quality Standards and Monitoring\\n\\n##...</td>\n",
       "      <td># Incident Brief: Data Quality and Compliance ...</td>\n",
       "      <td>Data quality standards include: 1) Completenes...</td>\n",
       "      <td>0.718750</td>\n",
       "      <td>0.893607</td>\n",
       "      <td>0.755556</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>What are the SLA definitions and monitoring pr...</td>\n",
       "      <td>[# Service Level Agreement Definitions\\n\\n## D...</td>\n",
       "      <td># Incident Brief\\n\\n## 1. Incident Summary\\nOn...</td>\n",
       "      <td>SLA definitions and monitoring: 1) Availabilit...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.775065</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>What are the troubleshooting procedures for da...</td>\n",
       "      <td>[# Data Pipeline Troubleshooting Guide\\n\\n## C...</td>\n",
       "      <td># Incident Brief\\n\\n## 1. Incident Summary\\nOn...</td>\n",
       "      <td>Data pipeline troubleshooting procedures: 1) C...</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.887266</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>What are the common failure patterns in data p...</td>\n",
       "      <td>[# Data Pipeline Troubleshooting Guide\\n\\n## C...</td>\n",
       "      <td># Incident Brief\\n\\n## 1. Incident Summary\\nOn...</td>\n",
       "      <td>Common data pipeline failure patterns include:...</td>\n",
       "      <td>0.696970</td>\n",
       "      <td>0.852711</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.700000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           user_input  \\\n",
       "0   What should I do if the sales orders pipeline ...   \n",
       "1   How does the customer analytics pipeline segme...   \n",
       "2   What are the stock level management rules for ...   \n",
       "3   What financial controls are implemented in the...   \n",
       "4   What attribution models are used for marketing...   \n",
       "5   How is supplier performance measured in the su...   \n",
       "6   What employee metrics are tracked in the HR an...   \n",
       "7   What product usage metrics are monitored in re...   \n",
       "8      What risk management controls are implemented?   \n",
       "9   What compliance monitoring procedures are in p...   \n",
       "10  What are the escalation procedures for data pi...   \n",
       "11  What are the data quality standards and monito...   \n",
       "12  What are the SLA definitions and monitoring pr...   \n",
       "13  What are the troubleshooting procedures for da...   \n",
       "14  What are the common failure patterns in data p...   \n",
       "\n",
       "                                   retrieved_contexts  \\\n",
       "0   [-- Sales Orders Pipeline\\n-- Purpose: Transfo...   \n",
       "1   [# Customer Analytics Pipeline Specification\\n...   \n",
       "2   [-- Inventory Management Pipeline\\n-- Purpose:...   \n",
       "3   [# Financial Reporting Pipeline Specification\\...   \n",
       "4   [# Marketing Attribution Pipeline Specificatio...   \n",
       "5   [-- Supply Chain Analytics Pipeline\\n-- Purpos...   \n",
       "6   [-- HR Analytics Pipeline\\n-- Purpose: Employe...   \n",
       "7   [-- Product Analytics Pipeline\\n-- Purpose: Co...   \n",
       "8   [# Risk Management Pipeline Specification\\n\\n#...   \n",
       "9   [# Compliance Monitoring Pipeline Specificatio...   \n",
       "10  [# Incident Escalation Procedures\\n\\n## Escala...   \n",
       "11  [# Data Quality Standards and Monitoring\\n\\n##...   \n",
       "12  [# Service Level Agreement Definitions\\n\\n## D...   \n",
       "13  [# Data Pipeline Troubleshooting Guide\\n\\n## C...   \n",
       "14  [# Data Pipeline Troubleshooting Guide\\n\\n## C...   \n",
       "\n",
       "                                             response  \\\n",
       "0   # Incident Brief: Sales Orders Pipeline Failur...   \n",
       "1   # Incident Brief: Customer Analytics Pipeline ...   \n",
       "2   # Incident Brief: Inventory Stock Level Manage...   \n",
       "3   # Incident Brief: Financial Controls in Report...   \n",
       "4   # Incident Brief: Marketing Attribution Models...   \n",
       "5   # Incident Brief: Supplier Performance Measure...   \n",
       "6   # Incident Brief: HR Analytics Pipeline Disrup...   \n",
       "7   # Incident Brief: Product Usage Metrics Monito...   \n",
       "8   # Incident Brief: Traceback Incident Triage\\n\\...   \n",
       "9   # Incident Brief: Compliance Monitoring Proced...   \n",
       "10  # Incident Brief\\n\\n## 1. Incident Summary\\nOn...   \n",
       "11  # Incident Brief: Data Quality and Compliance ...   \n",
       "12  # Incident Brief\\n\\n## 1. Incident Summary\\nOn...   \n",
       "13  # Incident Brief\\n\\n## 1. Incident Summary\\nOn...   \n",
       "14  # Incident Brief\\n\\n## 1. Incident Summary\\nOn...   \n",
       "\n",
       "                                            reference  faithfulness  \\\n",
       "0   If the sales orders pipeline fails: 1) Check p...      0.000000   \n",
       "1   The customer analytics pipeline segments custo...      0.893617   \n",
       "2   Inventory stock level management rules include...      0.947368   \n",
       "3   Financial reporting pipeline controls include:...      0.928571   \n",
       "4   Marketing attribution models include: 1) First...      0.928571   \n",
       "5   Supplier performance measurement includes: 1) ...      0.250000   \n",
       "6   HR analytics tracks employee metrics including...      0.295455   \n",
       "7   Real-time product usage metrics include: 1) DA...      0.448276   \n",
       "8   Risk management controls include: 1) Credit ri...      0.571429   \n",
       "9   Compliance monitoring procedures include: 1) R...      0.000000   \n",
       "10  Data pipeline incident escalation procedures: ...      0.128205   \n",
       "11  Data quality standards include: 1) Completenes...      0.718750   \n",
       "12  SLA definitions and monitoring: 1) Availabilit...      0.000000   \n",
       "13  Data pipeline troubleshooting procedures: 1) C...      0.600000   \n",
       "14  Common data pipeline failure patterns include:...      0.696970   \n",
       "\n",
       "    answer_relevancy  context_precision  context_recall  \n",
       "0           0.910627           1.000000        0.714286  \n",
       "1           0.935875           1.000000        0.333333  \n",
       "2           0.902031           0.000000        0.000000  \n",
       "3           0.946052           1.000000        0.300000  \n",
       "4           0.896943           1.000000        0.200000  \n",
       "5           0.887601           1.000000        0.142857  \n",
       "6           0.888966           0.000000        0.142857  \n",
       "7           0.890654           0.500000        0.800000  \n",
       "8           0.855866           0.000000        0.300000  \n",
       "9           0.893557           1.000000        0.700000  \n",
       "10          0.921669           0.916667        0.200000  \n",
       "11          0.893607           0.755556        1.000000  \n",
       "12          0.775065           0.583333        0.400000  \n",
       "13          0.887266           1.000000        0.600000  \n",
       "14          0.852711           1.000000        0.700000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Extract results\n",
    "results_df = result.to_pandas()\n",
    "\n",
    "print(\"üìä RAGAS Evaluation Results:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Display overall metrics\n",
    "overall_metrics = {\n",
    "    \"Faithfulness\": results_df['faithfulness'].mean(),\n",
    "    \"Answer Relevancy\": results_df['answer_relevancy'].mean(),\n",
    "    \"Context Precision\": results_df['context_precision'].mean(),\n",
    "    \"Context Recall\": results_df['context_recall'].mean()\n",
    "}\n",
    "\n",
    "print(\"\\nüéØ Overall Performance Metrics:\")\n",
    "for metric, score in overall_metrics.items():\n",
    "    print(f\"{metric:20}: {score:.3f}\")\n",
    "\n",
    "print(\"\\nüìã Detailed Results:\")\n",
    "display(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Debugging Results DataFrame:\n",
      "========================================\n",
      "DataFrame shape: (15, 8)\n",
      "Available columns: ['user_input', 'retrieved_contexts', 'response', 'reference', 'faithfulness', 'answer_relevancy', 'context_precision', 'context_recall']\n",
      "DataFrame head:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_input</th>\n",
       "      <th>retrieved_contexts</th>\n",
       "      <th>response</th>\n",
       "      <th>reference</th>\n",
       "      <th>faithfulness</th>\n",
       "      <th>answer_relevancy</th>\n",
       "      <th>context_precision</th>\n",
       "      <th>context_recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What should I do if the sales orders pipeline ...</td>\n",
       "      <td>[-- Sales Orders Pipeline\\n-- Purpose: Transfo...</td>\n",
       "      <td># Incident Brief: Sales Orders Pipeline Failur...</td>\n",
       "      <td>If the sales orders pipeline fails: 1) Check p...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.910627</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How does the customer analytics pipeline segme...</td>\n",
       "      <td>[# Customer Analytics Pipeline Specification\\n...</td>\n",
       "      <td># Incident Brief: Customer Analytics Pipeline ...</td>\n",
       "      <td>The customer analytics pipeline segments custo...</td>\n",
       "      <td>0.893617</td>\n",
       "      <td>0.935875</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What are the stock level management rules for ...</td>\n",
       "      <td>[-- Inventory Management Pipeline\\n-- Purpose:...</td>\n",
       "      <td># Incident Brief: Inventory Stock Level Manage...</td>\n",
       "      <td>Inventory stock level management rules include...</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>0.902031</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What financial controls are implemented in the...</td>\n",
       "      <td>[# Financial Reporting Pipeline Specification\\...</td>\n",
       "      <td># Incident Brief: Financial Controls in Report...</td>\n",
       "      <td>Financial reporting pipeline controls include:...</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.946052</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What attribution models are used for marketing...</td>\n",
       "      <td>[# Marketing Attribution Pipeline Specificatio...</td>\n",
       "      <td># Incident Brief: Marketing Attribution Models...</td>\n",
       "      <td>Marketing attribution models include: 1) First...</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.896943</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          user_input  \\\n",
       "0  What should I do if the sales orders pipeline ...   \n",
       "1  How does the customer analytics pipeline segme...   \n",
       "2  What are the stock level management rules for ...   \n",
       "3  What financial controls are implemented in the...   \n",
       "4  What attribution models are used for marketing...   \n",
       "\n",
       "                                  retrieved_contexts  \\\n",
       "0  [-- Sales Orders Pipeline\\n-- Purpose: Transfo...   \n",
       "1  [# Customer Analytics Pipeline Specification\\n...   \n",
       "2  [-- Inventory Management Pipeline\\n-- Purpose:...   \n",
       "3  [# Financial Reporting Pipeline Specification\\...   \n",
       "4  [# Marketing Attribution Pipeline Specificatio...   \n",
       "\n",
       "                                            response  \\\n",
       "0  # Incident Brief: Sales Orders Pipeline Failur...   \n",
       "1  # Incident Brief: Customer Analytics Pipeline ...   \n",
       "2  # Incident Brief: Inventory Stock Level Manage...   \n",
       "3  # Incident Brief: Financial Controls in Report...   \n",
       "4  # Incident Brief: Marketing Attribution Models...   \n",
       "\n",
       "                                           reference  faithfulness  \\\n",
       "0  If the sales orders pipeline fails: 1) Check p...      0.000000   \n",
       "1  The customer analytics pipeline segments custo...      0.893617   \n",
       "2  Inventory stock level management rules include...      0.947368   \n",
       "3  Financial reporting pipeline controls include:...      0.928571   \n",
       "4  Marketing attribution models include: 1) First...      0.928571   \n",
       "\n",
       "   answer_relevancy  context_precision  context_recall  \n",
       "0          0.910627                1.0        0.714286  \n",
       "1          0.935875                1.0        0.333333  \n",
       "2          0.902031                0.0        0.000000  \n",
       "3          0.946052                1.0        0.300000  \n",
       "4          0.896943                1.0        0.200000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Debug: Check what columns are available in results_df\n",
    "print(\"üîç Debugging Results DataFrame:\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"DataFrame shape: {results_df.shape}\")\n",
    "print(f\"Available columns: {list(results_df.columns)}\")\n",
    "print(f\"DataFrame head:\")\n",
    "display(results_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä RAGAS Evaluation Summary Table:\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Metric</th>\n",
       "      <th>Score</th>\n",
       "      <th>Interpretation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Faithfulness</td>\n",
       "      <td>0.493814</td>\n",
       "      <td>How factually accurate are the responses?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Answer Relevancy</td>\n",
       "      <td>0.889233</td>\n",
       "      <td>How relevant are the responses to the questions?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Context Precision</td>\n",
       "      <td>0.717037</td>\n",
       "      <td>How precise is the retrieved context?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Context Recall</td>\n",
       "      <td>0.435556</td>\n",
       "      <td>How well does the context cover the ground truth?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Metric     Score  \\\n",
       "0       Faithfulness  0.493814   \n",
       "1   Answer Relevancy  0.889233   \n",
       "2  Context Precision  0.717037   \n",
       "3     Context Recall  0.435556   \n",
       "\n",
       "                                      Interpretation  \n",
       "0          How factually accurate are the responses?  \n",
       "1   How relevant are the responses to the questions?  \n",
       "2              How precise is the retrieved context?  \n",
       "3  How well does the context cover the ground truth?  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a summary table\n",
    "summary_table = pd.DataFrame({\n",
    "    \"Metric\": [\"Faithfulness\", \"Answer Relevancy\", \"Context Precision\", \"Context Recall\"],\n",
    "    \"Score\": [overall_metrics[\"Faithfulness\"], overall_metrics[\"Answer Relevancy\"], \n",
    "              overall_metrics[\"Context Precision\"], overall_metrics[\"Context Recall\"]],\n",
    "    \"Interpretation\": [\n",
    "        \"How factually accurate are the responses?\",\n",
    "        \"How relevant are the responses to the questions?\",\n",
    "        \"How precise is the retrieved context?\",\n",
    "        \"How well does the context cover the ground truth?\"\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"\\nüìä RAGAS Evaluation Summary Table:\")\n",
    "print(\"=\" * 80)\n",
    "display(summary_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Safe Performance Analysis:\n",
      "==================================================\n",
      "Available metrics: ['faithfulness', 'answer_relevancy', 'context_precision', 'context_recall']\n",
      "\n",
      "üìä Overall Performance Metrics:\n",
      "faithfulness        : 0.494\n",
      "answer_relevancy    : 0.889\n",
      "context_precision   : 0.717\n",
      "context_recall      : 0.436\n",
      "\n",
      "üìà Performance Statistics:\n",
      "\n",
      "faithfulness:\n",
      "  Mean: 0.494\n",
      "  Std:  0.360\n",
      "  Min:  0.000\n",
      "  Max:  0.947\n",
      "\n",
      "answer_relevancy:\n",
      "  Mean: 0.889\n",
      "  Std:  0.040\n",
      "  Min:  0.775\n",
      "  Max:  0.946\n",
      "\n",
      "context_precision:\n",
      "  Mean: 0.717\n",
      "  Std:  0.404\n",
      "  Min:  0.000\n",
      "  Max:  1.000\n",
      "\n",
      "context_recall:\n",
      "  Mean: 0.436\n",
      "  Std:  0.295\n",
      "  Min:  0.000\n",
      "  Max:  1.000\n"
     ]
    }
   ],
   "source": [
    "# Safe performance analysis (handles missing columns)\n",
    "print(\"üîç Safe Performance Analysis:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Check what metrics are available\n",
    "available_metrics = [col for col in ['faithfulness', 'answer_relevancy', 'context_precision', 'context_recall'] \n",
    "                    if col in results_df.columns]\n",
    "\n",
    "print(f\"Available metrics: {available_metrics}\")\n",
    "\n",
    "if available_metrics:\n",
    "    # Calculate overall performance\n",
    "    overall_performance = {}\n",
    "    for metric in available_metrics:\n",
    "        overall_performance[metric] = results_df[metric].mean()\n",
    "    \n",
    "    print(\"\\nüìä Overall Performance Metrics:\")\n",
    "    for metric, score in overall_performance.items():\n",
    "        print(f\"{metric:20}: {score:.3f}\")\n",
    "    \n",
    "    # Calculate performance statistics\n",
    "    print(\"\\nüìà Performance Statistics:\")\n",
    "    for metric in available_metrics:\n",
    "        print(f\"\\n{metric}:\")\n",
    "        print(f\"  Mean: {results_df[metric].mean():.3f}\")\n",
    "        print(f\"  Std:  {results_df[metric].std():.3f}\")\n",
    "        print(f\"  Min:  {results_df[metric].min():.3f}\")\n",
    "        print(f\"  Max:  {results_df[metric].max():.3f}\")\n",
    "else:\n",
    "    print(\"‚ùå No metrics found in results DataFrame\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç SAFE Performance Analysis:\n",
      "==================================================\n",
      "Available columns: ['user_input', 'retrieved_contexts', 'response', 'reference', 'faithfulness', 'answer_relevancy', 'context_precision', 'context_recall']\n",
      "‚ö†Ô∏è Question column not found - showing overall performance only\n",
      "\n",
      "üìä Overall Performance Summary:\n",
      "faithfulness: 0.494\n",
      "answer_relevancy: 0.889\n",
      "context_precision: 0.717\n",
      "context_recall: 0.436\n"
     ]
    }
   ],
   "source": [
    "# SAFE ALTERNATIVE: Skip the problematic cell above and use this instead\n",
    "print(\"üîç SAFE Performance Analysis:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Check what columns are actually available\n",
    "print(f\"Available columns: {list(results_df.columns)}\")\n",
    "\n",
    "# Check if question column exists\n",
    "if 'question' in results_df.columns:\n",
    "    print(\"‚úÖ Question column found - can do question type analysis\")\n",
    "    # Add question categories safely\n",
    "    results_df['question_type'] = results_df['question'].apply(lambda x: \n",
    "        'Impact Analysis' if 'impacted' in x.lower() else\n",
    "        'Troubleshooting' if 'troubleshoot' in x.lower() or 'should i do' in x.lower() else\n",
    "        'Dependency Analysis' if 'depend' in x.lower() else\n",
    "        'SLA Query' if 'sla' in x.lower() else\n",
    "        'General'\n",
    "    )\n",
    "    \n",
    "    # Group by question type\n",
    "    available_metrics = [col for col in ['faithfulness', 'answer_relevancy', 'context_precision', 'context_recall'] \n",
    "                        if col in results_df.columns]\n",
    "    \n",
    "    if available_metrics:\n",
    "        performance_by_type = results_df.groupby('question_type')[available_metrics].mean().round(3)\n",
    "        print(\"\\nüìä Performance by Question Type:\")\n",
    "        display(performance_by_type)\n",
    "    else:\n",
    "        print(\"‚ùå No metrics found for grouping\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Question column not found - showing overall performance only\")\n",
    "    \n",
    "    # Show overall performance\n",
    "    available_metrics = [col for col in ['faithfulness', 'answer_relevancy', 'context_precision', 'context_recall'] \n",
    "                        if col in results_df.columns]\n",
    "    \n",
    "    if available_metrics:\n",
    "        print(\"\\nüìä Overall Performance Summary:\")\n",
    "        for metric in available_metrics:\n",
    "            print(f\"{metric}: {results_df[metric].mean():.3f}\")\n",
    "    else:\n",
    "        print(\"‚ùå No performance metrics found\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Performance Analysis and Conclusions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üéØ Performance Strengths and Weaknesses:\n",
      "==================================================\n",
      "\n",
      "‚úÖ Strongest Performance: answer_relevancy (0.889)\n",
      "‚ùå Weakest Performance: context_recall (0.436)\n",
      "\n",
      "üìä Overall Pipeline Score: 0.634\n",
      "üéØ Performance Level: Fair\n"
     ]
    }
   ],
   "source": [
    "# Identify strengths and weaknesses (with safety checks)\n",
    "print(\"\\nüéØ Performance Strengths and Weaknesses:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Use the safe overall_performance from previous cell\n",
    "if 'overall_performance' in locals() and overall_performance:\n",
    "    # Find best and worst performing metrics\n",
    "    best_metric = max(overall_performance.items(), key=lambda x: x[1])\n",
    "    worst_metric = min(overall_performance.items(), key=lambda x: x[1])\n",
    "\n",
    "    print(f\"\\n‚úÖ Strongest Performance: {best_metric[0]} ({best_metric[1]:.3f})\")\n",
    "    print(f\"‚ùå Weakest Performance: {worst_metric[0]} ({worst_metric[1]:.3f})\")\n",
    "\n",
    "    # Calculate overall score\n",
    "    overall_score = np.mean(list(overall_performance.values()))\n",
    "    print(f\"\\nüìä Overall Pipeline Score: {overall_score:.3f}\")\n",
    "\n",
    "    # Performance interpretation\n",
    "    if overall_score >= 0.8:\n",
    "        performance_level = \"Excellent\"\n",
    "    elif overall_score >= 0.7:\n",
    "        performance_level = \"Good\"\n",
    "    elif overall_score >= 0.6:\n",
    "        performance_level = \"Fair\"\n",
    "    else:\n",
    "        performance_level = \"Needs Improvement\"\n",
    "\n",
    "    print(f\"üéØ Performance Level: {performance_level}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No performance data available for analysis\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Detailed Conclusions and Recommendations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìã Detailed Conclusions and Recommendations:\n",
      "============================================================\n",
      "\n",
      "üîç Key Findings:\n",
      "--------------------\n",
      "1. Faithfulness (0.494): The system has significant factual accuracy issues that need attention.\n",
      "2. Answer Relevancy (0.889): Responses are highly relevant to the questions asked.\n",
      "3. Context Precision (0.717): The system retrieves reasonably precise context with some noise.\n",
      "4. Context Recall (0.436): The system often misses important context needed for accurate responses.\n"
     ]
    }
   ],
   "source": [
    "print(\"üìã Detailed Conclusions and Recommendations:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nüîç Key Findings:\")\n",
    "print(\"-\" * 20)\n",
    "\n",
    "# Faithfulness analysis\n",
    "faithfulness_score = overall_metrics[\"Faithfulness\"]\n",
    "if faithfulness_score >= 0.8:\n",
    "    faithfulness_conclusion = \"The system generates highly factual and accurate responses.\"\n",
    "elif faithfulness_score >= 0.6:\n",
    "    faithfulness_conclusion = \"The system generates mostly accurate responses with some factual inconsistencies.\"\n",
    "else:\n",
    "    faithfulness_conclusion = \"The system has significant factual accuracy issues that need attention.\"\n",
    "\n",
    "print(f\"1. Faithfulness ({faithfulness_score:.3f}): {faithfulness_conclusion}\")\n",
    "\n",
    "# Answer relevancy analysis\n",
    "relevancy_score = overall_metrics[\"Answer Relevancy\"]\n",
    "if relevancy_score >= 0.8:\n",
    "    relevancy_conclusion = \"Responses are highly relevant to the questions asked.\"\n",
    "elif relevancy_score >= 0.6:\n",
    "    relevancy_conclusion = \"Responses are generally relevant but may sometimes miss the mark.\"\n",
    "else:\n",
    "    relevancy_conclusion = \"Responses often lack relevance to the specific questions asked.\"\n",
    "\n",
    "print(f\"2. Answer Relevancy ({relevancy_score:.3f}): {relevancy_conclusion}\")\n",
    "\n",
    "# Context precision analysis\n",
    "precision_score = overall_metrics[\"Context Precision\"]\n",
    "if precision_score >= 0.8:\n",
    "    precision_conclusion = \"The system retrieves highly precise and relevant context.\"\n",
    "elif precision_score >= 0.6:\n",
    "    precision_conclusion = \"The system retrieves reasonably precise context with some noise.\"\n",
    "else:\n",
    "    precision_conclusion = \"The system retrieves context with significant noise and irrelevance.\"\n",
    "\n",
    "print(f\"3. Context Precision ({precision_score:.3f}): {precision_conclusion}\")\n",
    "\n",
    "# Context recall analysis\n",
    "recall_score = overall_metrics[\"Context Recall\"]\n",
    "if recall_score >= 0.8:\n",
    "    recall_conclusion = \"The system retrieves comprehensive context that covers ground truth well.\"\n",
    "elif recall_score >= 0.6:\n",
    "    recall_conclusion = \"The system retrieves adequate context but may miss some important information.\"\n",
    "else:\n",
    "    recall_conclusion = \"The system often misses important context needed for accurate responses.\"\n",
    "\n",
    "print(f\"4. Context Recall ({recall_score:.3f}): {recall_conclusion}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üí° Recommendations for Improvement:\n",
      "----------------------------------------\n",
      "‚Ä¢ Improve factual accuracy by enhancing the knowledge base and fact-checking mechanisms\n",
      "‚Ä¢ Improve context precision by refining retrieval algorithms and filtering mechanisms\n",
      "‚Ä¢ Enhance context recall by expanding the knowledge base and improving retrieval coverage\n",
      "‚Ä¢ Consider fine-tuning the LLM on domain-specific data\n",
      "‚Ä¢ Implement feedback loops to continuously improve performance\n",
      "‚Ä¢ Add more diverse test cases to the evaluation dataset\n",
      "‚Ä¢ Consider ensemble methods for better response quality\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nüí° Recommendations for Improvement:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "recommendations = []\n",
    "\n",
    "# Faithfulness recommendations\n",
    "if faithfulness_score < 0.8:\n",
    "    recommendations.append(\"‚Ä¢ Improve factual accuracy by enhancing the knowledge base and fact-checking mechanisms\")\n",
    "\n",
    "# Relevancy recommendations\n",
    "if relevancy_score < 0.8:\n",
    "    recommendations.append(\"‚Ä¢ Enhance response relevance by improving question understanding and response generation\")\n",
    "\n",
    "# Precision recommendations\n",
    "if precision_score < 0.8:\n",
    "    recommendations.append(\"‚Ä¢ Improve context precision by refining retrieval algorithms and filtering mechanisms\")\n",
    "\n",
    "# Recall recommendations\n",
    "if recall_score < 0.8:\n",
    "    recommendations.append(\"‚Ä¢ Enhance context recall by expanding the knowledge base and improving retrieval coverage\")\n",
    "\n",
    "# General recommendations\n",
    "if overall_score < 0.8:\n",
    "    recommendations.extend([\n",
    "        \"‚Ä¢ Consider fine-tuning the LLM on domain-specific data\",\n",
    "        \"‚Ä¢ Implement feedback loops to continuously improve performance\",\n",
    "        \"‚Ä¢ Add more diverse test cases to the evaluation dataset\",\n",
    "        \"‚Ä¢ Consider ensemble methods for better response quality\"\n",
    "    ])\n",
    "\n",
    "if recommendations:\n",
    "    for rec in recommendations:\n",
    "        print(rec)\n",
    "else:\n",
    "    print(\"‚Ä¢ System performance is excellent - consider monitoring for consistency\")\n",
    "    print(\"‚Ä¢ Expand the test dataset to cover more edge cases\")\n",
    "    print(\"‚Ä¢ Implement A/B testing for continuous improvement\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üéØ Overall Pipeline Effectiveness Assessment:\n",
      "==================================================\n",
      "\n",
      "üìä Summary Statistics:\n",
      "‚Ä¢ Total Test Cases: 15\n",
      "‚Ä¢ Overall Score: 0.634\n",
      "‚Ä¢ Performance Level: Fair\n",
      "‚Ä¢ Best Metric: answer_relevancy (0.889)\n",
      "‚Ä¢ Worst Metric: context_recall (0.436)\n",
      "\n",
      "üîç Key Insights:\n",
      "‚Ä¢ The Traceback system demonstrates moderate performance across all RAGAS metrics\n",
      "‚Ä¢ The system excels at answer_relevancy\n",
      "‚Ä¢ Significant improvement needed in context_recall\n",
      "\n",
      "‚úÖ Conclusion:\n",
      "The Traceback pipeline demonstrates fair effectiveness but requires significant improvements.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nüéØ Overall Pipeline Effectiveness Assessment:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(f\"\\nüìä Summary Statistics:\")\n",
    "print(f\"‚Ä¢ Total Test Cases: {len(evaluation_data)}\")\n",
    "print(f\"‚Ä¢ Overall Score: {overall_score:.3f}\")\n",
    "print(f\"‚Ä¢ Performance Level: {performance_level}\")\n",
    "print(f\"‚Ä¢ Best Metric: {best_metric[0]} ({best_metric[1]:.3f})\")\n",
    "print(f\"‚Ä¢ Worst Metric: {worst_metric[0]} ({worst_metric[1]:.3f})\")\n",
    "\n",
    "print(f\"\\nüîç Key Insights:\")\n",
    "print(f\"‚Ä¢ The Traceback system demonstrates {'strong' if overall_score >= 0.7 else 'moderate' if overall_score >= 0.6 else 'weak'} performance across all RAGAS metrics\")\n",
    "print(f\"‚Ä¢ {'The system excels at' if best_metric[1] >= 0.8 else 'The system shows good performance in'} {best_metric[0].lower()}\")\n",
    "print(f\"‚Ä¢ {'Significant improvement needed in' if worst_metric[1] < 0.6 else 'Some improvement possible in'} {worst_metric[0].lower()}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Conclusion:\")\n",
    "if overall_score >= 0.8:\n",
    "    conclusion = \"The Traceback pipeline is highly effective and ready for production deployment.\"\n",
    "elif overall_score >= 0.7:\n",
    "    conclusion = \"The Traceback pipeline shows good effectiveness with room for targeted improvements.\"\n",
    "elif overall_score >= 0.6:\n",
    "    conclusion = \"The Traceback pipeline demonstrates fair effectiveness but requires significant improvements.\"\n",
    "else:\n",
    "    conclusion = \"The Traceback pipeline needs substantial improvements before production deployment.\"\n",
    "\n",
    "print(conclusion)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Save Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Results saved to: /Users/sandeepgogineni/ai-engineering/bootcamp/Traceback/data/ragas_evaluation_results.json\n",
      "üìä Evaluation completed successfully!\n",
      "üéØ Overall Pipeline Score: 0.634 (Fair)\n"
     ]
    }
   ],
   "source": [
    "# Save detailed results\n",
    "results_output_path = Path.cwd().parent / \"data\" / \"ragas_evaluation_results.json\"\n",
    "\n",
    "evaluation_summary = {\n",
    "    \"evaluation_date\": pd.Timestamp.now().isoformat(),\n",
    "    \"total_test_cases\": len(evaluation_data),\n",
    "    \"overall_metrics\": overall_metrics,\n",
    "    \"overall_score\": float(overall_score),\n",
    "    \"performance_level\": performance_level,\n",
    "    \"detailed_results\": results_df.to_dict('records'),\n",
    "    \"recommendations\": recommendations if recommendations else [\"System performance is excellent\"]\n",
    "}\n",
    "\n",
    "with open(results_output_path, 'w') as f:\n",
    "    json.dump(evaluation_summary, f, indent=2)\n",
    "\n",
    "print(f\"‚úÖ Results saved to: {results_output_path}\")\n",
    "print(f\"üìä Evaluation completed successfully!\")\n",
    "print(f\"üéØ Overall Pipeline Score: {overall_score:.3f} ({performance_level})\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
