{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üö® Traceback: End-to-End Agentic RAG System\n",
        "\n",
        "**Instant triage across docs, code & lineage.**\n",
        "\n",
        "This notebook implements the complete Traceback system phase by phase:\n",
        "\n",
        "## üìã Implementation Phases\n",
        "\n",
        "- **Phase 1: Data Foundation** - Set up data processing pipeline\n",
        "- **Phase 2: Core RAG** - Set up Qdrant and basic retrieval  \n",
        "- **Phase 3: Agent System** - Build LangGraph supervisor and agents\n",
        "- **Phase 4: API & Interface** - FastAPI endpoints and CLI\n",
        "\n",
        "## üéØ System Overview\n",
        "\n",
        "Traceback unifies three fragmented sources for fast incident response:\n",
        "- üìÑ **Requirements docs** (PDF/MD) \n",
        "- üßæ **Pipeline code snippets** (SQL/Py)\n",
        "- üß¨ **Column-level lineage graph** (JSON)\n",
        "\n",
        "**Target Questions:**\n",
        "> \"Job `curated.sales_orders` failed ‚Äî who's impacted?\"\n",
        "> \"What dashboards went stale?\"\n",
        "> \"Do we roll back or hotfix?\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Phase 1: Data Foundation üèóÔ∏è\n",
        "\n",
        "## Objectives:\n",
        "1. Set up project structure and dependencies\n",
        "2. Create sample data (docs, code, lineage)\n",
        "3. Implement data processing pipeline\n",
        "4. Test chunking strategies\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Environment setup complete\n",
            "‚úÖ OpenAI API key loaded\n",
            "‚úÖ Tavily API key loaded (optional)\n",
            "‚úÖ Cohere API key loaded (optional)\n"
          ]
        }
      ],
      "source": [
        "# Setup: Import libraries and configure environment\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import time\n",
        "from pathlib import Path\n",
        "from typing import List, Dict, Any, Optional\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv()\n",
        "\n",
        "# Verify API keys\n",
        "if not os.getenv(\"OPENAI_API_KEY\"):\n",
        "    raise RuntimeError(\"OPENAI_API_KEY is not set. Create a .env file or export it in your shell.\")\n",
        "\n",
        "print(\"‚úÖ Environment setup complete\")\n",
        "print(f\"‚úÖ OpenAI API key loaded\")\n",
        "\n",
        "# Optional API keys\n",
        "if os.getenv(\"TAVILY_API_KEY\"):\n",
        "    print(\"‚úÖ Tavily API key loaded (optional)\")\n",
        "if os.getenv(\"COHERE_API_KEY\"):\n",
        "    print(\"‚úÖ Cohere API key loaded (optional)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìÅ Project root: /Users/sandeepgogineni/ai-engineering/bootcamp/Traceback\n",
            "üìÅ Data directory: /Users/sandeepgogineni/ai-engineering/bootcamp/Traceback/data\n",
            "üìÅ Docs directory: /Users/sandeepgogineni/ai-engineering/bootcamp/Traceback/data/docs\n",
            "üìÅ Repo directory: /Users/sandeepgogineni/ai-engineering/bootcamp/Traceback/data/repo\n",
            "‚úÖ Added /Users/sandeepgogineni/ai-engineering/bootcamp/Traceback/src to Python path\n"
          ]
        }
      ],
      "source": [
        "# Setup: Project paths and structure\n",
        "def find_project_root():\n",
        "    \"\"\"Find the project root directory.\"\"\"\n",
        "    current = Path.cwd()\n",
        "    if current.name == \"notebooks\":\n",
        "        return current.parent\n",
        "    for parent in [current] + list(current.parents):\n",
        "        if (parent / \"pyproject.toml\").exists():\n",
        "            return parent\n",
        "    return current\n",
        "\n",
        "# Set up paths\n",
        "BASE = find_project_root()\n",
        "SRC = BASE / \"src\"\n",
        "DATA = BASE / \"data\"\n",
        "DOCS = DATA / \"docs\"\n",
        "REPO = DATA / \"repo\"\n",
        "\n",
        "# Add src to Python path\n",
        "sys.path.insert(0, str(SRC))\n",
        "\n",
        "# Create directories\n",
        "DATA.mkdir(exist_ok=True)\n",
        "DOCS.mkdir(exist_ok=True)\n",
        "REPO.mkdir(exist_ok=True)\n",
        "\n",
        "print(f\"üìÅ Project root: {BASE}\")\n",
        "print(f\"üìÅ Data directory: {DATA}\")\n",
        "print(f\"üìÅ Docs directory: {DOCS}\")\n",
        "print(f\"üìÅ Repo directory: {REPO}\")\n",
        "print(f\"‚úÖ Added {SRC} to Python path\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìÑ Created: incident_playbook.md\n",
            "üìÑ Created: sales_orders_spec.md\n",
            "üìÑ Created: data_quality_standards.md\n",
            "\n",
            "‚úÖ Created 3 sample documents\n"
          ]
        }
      ],
      "source": [
        "# Phase 1.1: Create Sample Data\n",
        "\n",
        "# Create sample requirement documents\n",
        "sample_docs = {\n",
        "    \"incident_playbook.md\": \"\"\"# Data Pipeline Incident Response Playbook\n",
        "\n",
        "## Overview\n",
        "This playbook provides standardized procedures for responding to data pipeline incidents.\n",
        "\n",
        "## Severity Levels\n",
        "- **P0**: Critical business impact, revenue loss\n",
        "- **P1**: High impact, SLA breach risk  \n",
        "- **P2**: Medium impact, degraded service\n",
        "- **P3**: Low impact, minor issues\n",
        "\n",
        "## Response Procedures\n",
        "\n",
        "### Initial Assessment (0-15 minutes)\n",
        "1. **Acknowledge** the incident\n",
        "2. **Assess** business impact\n",
        "3. **Determine** blast radius\n",
        "4. **Notify** stakeholders\n",
        "\n",
        "### Impact Assessment Questions\n",
        "- Which dashboards are affected?\n",
        "- What downstream systems depend on this data?\n",
        "- Are there any SLA commitments at risk?\n",
        "- What is the estimated recovery time?\n",
        "\n",
        "### Common Actions\n",
        "- **Rollback**: Revert to last known good state\n",
        "- **Hotfix**: Apply targeted fix\n",
        "- **Backfill**: Reprocess affected data\n",
        "- **Skip**: Bypass failed step if non-critical\n",
        "\n",
        "## Escalation Matrix\n",
        "- **Data Engineering Lead**: P0/P1 incidents\n",
        "- **Platform Team**: Infrastructure issues\n",
        "- **Product Manager**: Business impact assessment\n",
        "\"\"\",\n",
        "    \n",
        "    \"sales_orders_spec.md\": \"\"\"# Sales Orders Domain Specification\n",
        "\n",
        "## Purpose\n",
        "The sales orders pipeline processes raw order data into curated, business-ready datasets for analytics and reporting.\n",
        "\n",
        "## Data Sources\n",
        "- **raw.sales_orders**: Raw order data from e-commerce platform\n",
        "- **raw.customers**: Customer master data\n",
        "- **raw.products**: Product catalog\n",
        "\n",
        "## Business Rules\n",
        "\n",
        "### Data Quality Requirements\n",
        "- Order amounts must be positive\n",
        "- Customer IDs must exist in customer master\n",
        "- Product IDs must exist in product catalog\n",
        "- Timestamps must be valid and recent\n",
        "\n",
        "### Transformation Logic\n",
        "1. **Clean**: Remove invalid records\n",
        "2. **Enrich**: Add customer and product details\n",
        "3. **Calculate**: Compute derived fields\n",
        "4. **Validate**: Apply business rules\n",
        "\n",
        "## SLA Commitments\n",
        "- **Availability**: 99.9% uptime\n",
        "- **Freshness**: Data available within 2 hours of source update\n",
        "- **Accuracy**: <0.1% error rate\n",
        "\n",
        "## Downstream Dependencies\n",
        "- **curated.revenue_summary**: Daily revenue reporting\n",
        "- **bi.daily_sales**: Executive dashboard\n",
        "- **analytics.customer_behavior**: Customer analytics\n",
        "\n",
        "## Ownership\n",
        "- **Primary**: data-sales team\n",
        "- **Secondary**: data-platform team\n",
        "- **Stakeholders**: Finance, Marketing, Product\n",
        "\"\"\",\n",
        "    \n",
        "    \"data_quality_standards.md\": \"\"\"# Data Quality Standards and Monitoring\n",
        "\n",
        "## Quality Dimensions\n",
        "\n",
        "### Completeness\n",
        "- No missing values in critical fields\n",
        "- All expected records present\n",
        "- Referential integrity maintained\n",
        "\n",
        "### Accuracy  \n",
        "- Data matches source systems\n",
        "- Business rules validated\n",
        "- Calculated fields verified\n",
        "\n",
        "### Consistency\n",
        "- Format standards applied\n",
        "- Naming conventions followed\n",
        "- Data types consistent\n",
        "\n",
        "### Timeliness\n",
        "- Data available within SLA windows\n",
        "- Processing delays monitored\n",
        "- Stale data alerts configured\n",
        "\n",
        "## Monitoring Framework\n",
        "\n",
        "### Automated Checks\n",
        "- Schema validation\n",
        "- Data freshness monitoring\n",
        "- Anomaly detection\n",
        "- Statistical quality metrics\n",
        "\n",
        "### Alerting Thresholds\n",
        "- **Critical**: >1% data quality issues\n",
        "- **Warning**: >0.1% data quality issues\n",
        "- **Info**: Quality metrics trending\n",
        "\n",
        "## Remediation Procedures\n",
        "1. **Identify** root cause\n",
        "2. **Assess** impact scope\n",
        "3. **Apply** fix or workaround\n",
        "4. **Validate** resolution\n",
        "5. **Document** lessons learned\n",
        "\"\"\"\n",
        "}\n",
        "\n",
        "# Write sample docs\n",
        "for filename, content in sample_docs.items():\n",
        "    doc_path = DOCS / filename\n",
        "    doc_path.write_text(content)\n",
        "    print(f\"üìÑ Created: {filename}\")\n",
        "\n",
        "print(f\"\\n‚úÖ Created {len(sample_docs)} sample documents\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîß Created: sales_orders_pipeline.sql\n",
            "üîß Created: revenue_summary_pipeline.sql\n",
            "üîß Created: customer_analytics_pipeline.sql\n",
            "\n",
            "‚úÖ Created 3 sample SQL pipelines\n"
          ]
        }
      ],
      "source": [
        "# Phase 1.2: Create Sample Pipeline Code\n",
        "\n",
        "# Create sample SQL pipeline files\n",
        "sample_sql_pipelines = {\n",
        "    \"sales_orders_pipeline.sql\": \"\"\"-- Sales Orders Pipeline\n",
        "-- Purpose: Transform raw order data into curated sales orders\n",
        "-- Owner: data-sales team\n",
        "-- SLA: 2 hours freshness\n",
        "\n",
        "WITH cleaned_orders AS (\n",
        "    SELECT \n",
        "        order_id,\n",
        "        customer_id,\n",
        "        product_id,\n",
        "        order_date,\n",
        "        quantity,\n",
        "        unit_price,\n",
        "        -- Data quality checks\n",
        "        CASE \n",
        "            WHEN quantity > 0 AND unit_price > 0 THEN quantity * unit_price\n",
        "            ELSE NULL \n",
        "        END AS gross_amount\n",
        "    FROM raw.sales_orders\n",
        "    WHERE \n",
        "        order_date >= CURRENT_DATE - INTERVAL '30 days'\n",
        "        AND customer_id IS NOT NULL\n",
        "        AND product_id IS NOT NULL\n",
        "),\n",
        "\n",
        "enriched_orders AS (\n",
        "    SELECT \n",
        "        co.*,\n",
        "        c.customer_name,\n",
        "        c.customer_segment,\n",
        "        p.product_name,\n",
        "        p.category,\n",
        "        -- Calculate net amount after refunds\n",
        "        co.gross_amount - COALESCE(r.refund_amount, 0) AS net_amount\n",
        "    FROM cleaned_orders co\n",
        "    LEFT JOIN raw.customers c ON co.customer_id = c.customer_id\n",
        "    LEFT JOIN raw.products p ON co.product_id = p.product_id\n",
        "    LEFT JOIN raw.refunds r ON co.order_id = r.order_id\n",
        ")\n",
        "\n",
        "INSERT INTO curated.sales_orders\n",
        "SELECT \n",
        "    order_id,\n",
        "    customer_id,\n",
        "    customer_name,\n",
        "    customer_segment,\n",
        "    product_id,\n",
        "    product_name,\n",
        "    category,\n",
        "    order_date,\n",
        "    quantity,\n",
        "    unit_price,\n",
        "    gross_amount,\n",
        "    net_amount,\n",
        "    CURRENT_TIMESTAMP AS processed_at\n",
        "FROM enriched_orders\n",
        "WHERE net_amount > 0;  -- Only include valid orders\n",
        "\"\"\",\n",
        "    \n",
        "    \"revenue_summary_pipeline.sql\": \"\"\"-- Revenue Summary Pipeline  \n",
        "-- Purpose: Create daily revenue summaries for reporting\n",
        "-- Owner: data-sales team\n",
        "-- Dependencies: curated.sales_orders\n",
        "\n",
        "WITH daily_revenue AS (\n",
        "    SELECT \n",
        "        DATE(order_date) AS revenue_date,\n",
        "        customer_segment,\n",
        "        category,\n",
        "        COUNT(*) AS order_count,\n",
        "        SUM(net_amount) AS total_revenue,\n",
        "        AVG(net_amount) AS avg_order_value,\n",
        "        SUM(quantity) AS total_quantity\n",
        "    FROM curated.sales_orders\n",
        "    WHERE order_date >= CURRENT_DATE - INTERVAL '7 days'\n",
        "    GROUP BY DATE(order_date), customer_segment, category\n",
        "),\n",
        "\n",
        "segment_totals AS (\n",
        "    SELECT \n",
        "        revenue_date,\n",
        "        customer_segment,\n",
        "        SUM(total_revenue) AS segment_revenue,\n",
        "        SUM(order_count) AS segment_orders\n",
        "    FROM daily_revenue\n",
        "    GROUP BY revenue_date, customer_segment\n",
        ")\n",
        "\n",
        "INSERT INTO curated.revenue_summary\n",
        "SELECT \n",
        "    dr.revenue_date,\n",
        "    dr.customer_segment,\n",
        "    dr.category,\n",
        "    dr.order_count,\n",
        "    dr.total_revenue,\n",
        "    dr.avg_order_value,\n",
        "    dr.total_quantity,\n",
        "    st.segment_revenue,\n",
        "    st.segment_orders,\n",
        "    CURRENT_TIMESTAMP AS processed_at\n",
        "FROM daily_revenue dr\n",
        "LEFT JOIN segment_totals st \n",
        "    ON dr.revenue_date = st.revenue_date \n",
        "    AND dr.customer_segment = st.customer_segment;\n",
        "\"\"\",\n",
        "    \n",
        "    \"customer_analytics_pipeline.sql\": \"\"\"-- Customer Analytics Pipeline\n",
        "-- Purpose: Generate customer behavior analytics\n",
        "-- Owner: data-analytics team\n",
        "-- Dependencies: curated.sales_orders, curated.customers\n",
        "\n",
        "WITH customer_metrics AS (\n",
        "    SELECT \n",
        "        customer_id,\n",
        "        COUNT(DISTINCT order_date) AS active_days,\n",
        "        COUNT(*) AS total_orders,\n",
        "        SUM(net_amount) AS lifetime_value,\n",
        "        AVG(net_amount) AS avg_order_value,\n",
        "        MAX(order_date) AS last_order_date,\n",
        "        MIN(order_date) AS first_order_date\n",
        "    FROM curated.sales_orders\n",
        "    WHERE order_date >= CURRENT_DATE - INTERVAL '90 days'\n",
        "    GROUP BY customer_id\n",
        "),\n",
        "\n",
        "customer_segments AS (\n",
        "    SELECT \n",
        "        customer_id,\n",
        "        CASE \n",
        "            WHEN lifetime_value > 1000 THEN 'High Value'\n",
        "            WHEN lifetime_value > 500 THEN 'Medium Value'\n",
        "            ELSE 'Low Value'\n",
        "        END AS value_segment,\n",
        "        CASE \n",
        "            WHEN active_days >= 10 THEN 'Frequent'\n",
        "            WHEN active_days >= 5 THEN 'Regular'\n",
        "            ELSE 'Occasional'\n",
        "        END AS frequency_segment\n",
        "    FROM customer_metrics\n",
        ")\n",
        "\n",
        "INSERT INTO analytics.customer_behavior\n",
        "SELECT \n",
        "    cm.customer_id,\n",
        "    cm.active_days,\n",
        "    cm.total_orders,\n",
        "    cm.lifetime_value,\n",
        "    cm.avg_order_value,\n",
        "    cm.last_order_date,\n",
        "    cm.first_order_date,\n",
        "    cs.value_segment,\n",
        "    cs.frequency_segment,\n",
        "    CURRENT_TIMESTAMP AS processed_at\n",
        "FROM customer_metrics cm\n",
        "JOIN customer_segments cs ON cm.customer_id = cs.customer_id;\n",
        "\"\"\"\n",
        "}\n",
        "\n",
        "# Write sample SQL files\n",
        "for filename, content in sample_sql_pipelines.items():\n",
        "    sql_path = REPO / filename\n",
        "    sql_path.write_text(content)\n",
        "    print(f\"üîß Created: {filename}\")\n",
        "\n",
        "print(f\"\\n‚úÖ Created {len(sample_sql_pipelines)} sample SQL pipelines\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üß¨ Created lineage graph:\n",
            "  üìä 13 nodes (tables/columns)\n",
            "  üîó 13 edges (relationships)\n",
            "  üìà 3 dashboards\n",
            "  ‚öôÔ∏è 3 pipelines\n",
            "‚úÖ Saved to: /Users/sandeepgogineni/ai-engineering/bootcamp/Traceback/data/lineage.json\n"
          ]
        }
      ],
      "source": [
        "# Phase 1.3: Create Sample Lineage Data\n",
        "\n",
        "# Create comprehensive lineage graph\n",
        "lineage_data = {\n",
        "    \"nodes\": [\n",
        "        # Raw data sources\n",
        "        {\"id\": \"raw.sales_orders\", \"type\": \"table\", \"schema\": \"raw\", \"owners\": [\"data-platform\"], \"description\": \"Raw order data from e-commerce platform\"},\n",
        "        {\"id\": \"raw.customers\", \"type\": \"table\", \"schema\": \"raw\", \"owners\": [\"data-platform\"], \"description\": \"Customer master data\"},\n",
        "        {\"id\": \"raw.products\", \"type\": \"table\", \"schema\": \"raw\", \"owners\": [\"data-platform\"], \"description\": \"Product catalog\"},\n",
        "        {\"id\": \"raw.refunds\", \"type\": \"table\", \"schema\": \"raw\", \"owners\": [\"data-platform\"], \"description\": \"Refund transaction data\"},\n",
        "        \n",
        "        # Curated tables\n",
        "        {\"id\": \"curated.sales_orders\", \"type\": \"table\", \"schema\": \"curated\", \"owners\": [\"data-sales\"], \"description\": \"Cleaned and enriched sales orders\"},\n",
        "        {\"id\": \"curated.revenue_summary\", \"type\": \"table\", \"schema\": \"curated\", \"owners\": [\"data-sales\"], \"description\": \"Daily revenue summaries\"},\n",
        "        {\"id\": \"curated.customers\", \"type\": \"table\", \"schema\": \"curated\", \"owners\": [\"data-sales\"], \"description\": \"Enriched customer data\"},\n",
        "        \n",
        "        # Analytics tables\n",
        "        {\"id\": \"analytics.customer_behavior\", \"type\": \"table\", \"schema\": \"analytics\", \"owners\": [\"data-analytics\"], \"description\": \"Customer behavior analytics\"},\n",
        "        \n",
        "        # Key columns\n",
        "        {\"id\": \"curated.sales_orders.order_id\", \"type\": \"column\", \"table\": \"curated.sales_orders\", \"data_type\": \"varchar\", \"description\": \"Unique order identifier\"},\n",
        "        {\"id\": \"curated.sales_orders.customer_id\", \"type\": \"column\", \"table\": \"curated.sales_orders\", \"data_type\": \"varchar\", \"description\": \"Customer identifier\"},\n",
        "        {\"id\": \"curated.sales_orders.net_amount\", \"type\": \"column\", \"table\": \"curated.sales_orders\", \"data_type\": \"decimal\", \"description\": \"Net order amount after refunds\"},\n",
        "        {\"id\": \"curated.revenue_summary.total_revenue\", \"type\": \"column\", \"table\": \"curated.revenue_summary\", \"data_type\": \"decimal\", \"description\": \"Total daily revenue\"},\n",
        "        {\"id\": \"analytics.customer_behavior.lifetime_value\", \"type\": \"column\", \"table\": \"analytics.customer_behavior\", \"data_type\": \"decimal\", \"description\": \"Customer lifetime value\"}\n",
        "    ],\n",
        "    \n",
        "    \"edges\": [\n",
        "        # Raw to Curated transformations\n",
        "        {\"from\": \"raw.sales_orders\", \"to\": \"curated.sales_orders\", \"operation\": \"clean+enrich\", \"pipeline\": \"sales_orders_pipeline.sql\"},\n",
        "        {\"from\": \"raw.customers\", \"to\": \"curated.sales_orders\", \"operation\": \"join\", \"pipeline\": \"sales_orders_pipeline.sql\"},\n",
        "        {\"from\": \"raw.products\", \"to\": \"curated.sales_orders\", \"operation\": \"join\", \"pipeline\": \"sales_orders_pipeline.sql\"},\n",
        "        {\"from\": \"raw.refunds\", \"to\": \"curated.sales_orders\", \"operation\": \"subtract\", \"pipeline\": \"sales_orders_pipeline.sql\"},\n",
        "        \n",
        "        # Curated to Analytics transformations\n",
        "        {\"from\": \"curated.sales_orders\", \"to\": \"curated.revenue_summary\", \"operation\": \"aggregate\", \"pipeline\": \"revenue_summary_pipeline.sql\"},\n",
        "        {\"from\": \"curated.sales_orders\", \"to\": \"analytics.customer_behavior\", \"operation\": \"aggregate\", \"pipeline\": \"customer_analytics_pipeline.sql\"},\n",
        "        {\"from\": \"curated.customers\", \"to\": \"analytics.customer_behavior\", \"operation\": \"join\", \"pipeline\": \"customer_analytics_pipeline.sql\"},\n",
        "        \n",
        "        # Column-level dependencies\n",
        "        {\"from\": \"raw.sales_orders.order_id\", \"to\": \"curated.sales_orders.order_id\", \"operation\": \"copy\"},\n",
        "        {\"from\": \"raw.sales_orders.customer_id\", \"to\": \"curated.sales_orders.customer_id\", \"operation\": \"copy\"},\n",
        "        {\"from\": \"raw.sales_orders.quantity\", \"to\": \"curated.sales_orders.net_amount\", \"operation\": \"calculate\"},\n",
        "        {\"from\": \"raw.sales_orders.unit_price\", \"to\": \"curated.sales_orders.net_amount\", \"operation\": \"calculate\"},\n",
        "        {\"from\": \"curated.sales_orders.net_amount\", \"to\": \"curated.revenue_summary.total_revenue\", \"operation\": \"sum\"},\n",
        "        {\"from\": \"curated.sales_orders.net_amount\", \"to\": \"analytics.customer_behavior.lifetime_value\", \"operation\": \"sum\"}\n",
        "    ],\n",
        "    \n",
        "    \"dashboards\": [\n",
        "        {\n",
        "            \"id\": \"bi.daily_sales\",\n",
        "            \"name\": \"Daily Sales Dashboard\",\n",
        "            \"tables\": [\"curated.sales_orders\", \"curated.revenue_summary\"],\n",
        "            \"teams\": [\"Finance\", \"Sales\", \"Executive\"],\n",
        "            \"description\": \"Executive dashboard showing daily sales performance\",\n",
        "            \"refresh_frequency\": \"hourly\"\n",
        "        },\n",
        "        {\n",
        "            \"id\": \"bi.customer_analytics\", \n",
        "            \"name\": \"Customer Analytics Dashboard\",\n",
        "            \"tables\": [\"analytics.customer_behavior\", \"curated.customers\"],\n",
        "            \"teams\": [\"Marketing\", \"Product\"],\n",
        "            \"description\": \"Customer behavior and segmentation analytics\",\n",
        "            \"refresh_frequency\": \"daily\"\n",
        "        },\n",
        "        {\n",
        "            \"id\": \"ops.data_quality\",\n",
        "            \"name\": \"Data Quality Monitoring\",\n",
        "            \"tables\": [\"curated.sales_orders\", \"curated.revenue_summary\"],\n",
        "            \"teams\": [\"Data Engineering\", \"Platform\"],\n",
        "            \"description\": \"Data quality metrics and monitoring\",\n",
        "            \"refresh_frequency\": \"real-time\"\n",
        "        }\n",
        "    ],\n",
        "    \n",
        "    \"pipelines\": [\n",
        "        {\n",
        "            \"id\": \"sales_orders_pipeline\",\n",
        "            \"name\": \"Sales Orders Pipeline\",\n",
        "            \"file\": \"sales_orders_pipeline.sql\",\n",
        "            \"schedule\": \"hourly\",\n",
        "            \"owner\": \"data-sales\",\n",
        "            \"dependencies\": [\"raw.sales_orders\", \"raw.customers\", \"raw.products\"],\n",
        "            \"outputs\": [\"curated.sales_orders\"]\n",
        "        },\n",
        "        {\n",
        "            \"id\": \"revenue_summary_pipeline\",\n",
        "            \"name\": \"Revenue Summary Pipeline\", \n",
        "            \"file\": \"revenue_summary_pipeline.sql\",\n",
        "            \"schedule\": \"daily\",\n",
        "            \"owner\": \"data-sales\",\n",
        "            \"dependencies\": [\"curated.sales_orders\"],\n",
        "            \"outputs\": [\"curated.revenue_summary\"]\n",
        "        },\n",
        "        {\n",
        "            \"id\": \"customer_analytics_pipeline\",\n",
        "            \"name\": \"Customer Analytics Pipeline\",\n",
        "            \"file\": \"customer_analytics_pipeline.sql\", \n",
        "            \"schedule\": \"daily\",\n",
        "            \"owner\": \"data-analytics\",\n",
        "            \"dependencies\": [\"curated.sales_orders\", \"curated.customers\"],\n",
        "            \"outputs\": [\"analytics.customer_behavior\"]\n",
        "        }\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Save lineage data\n",
        "lineage_path = DATA / \"lineage.json\"\n",
        "with open(lineage_path, 'w') as f:\n",
        "    json.dump(lineage_data, f, indent=2)\n",
        "\n",
        "print(f\"üß¨ Created lineage graph:\")\n",
        "print(f\"  üìä {len(lineage_data['nodes'])} nodes (tables/columns)\")\n",
        "print(f\"  üîó {len(lineage_data['edges'])} edges (relationships)\")\n",
        "print(f\"  üìà {len(lineage_data['dashboards'])} dashboards\")\n",
        "print(f\"  ‚öôÔ∏è {len(lineage_data['pipelines'])} pipelines\")\n",
        "print(f\"‚úÖ Saved to: {lineage_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Data processing pipeline initialized with semantic chunking\n",
            "  üìÑ Markdown splitter: RecursiveCharacterTextSplitter\n",
            "  üîß SQL splitter: SQL-optimized RecursiveCharacterTextSplitter\n",
            "  üß¨ Lineage processor: Ready for graph queries\n"
          ]
        }
      ],
      "source": [
        "# Phase 1.4: Implement Data Processing Pipeline (with Semantic Chunking)\n",
        "\n",
        "from langchain_text_splitters import (\n",
        "    RecursiveCharacterTextSplitter,\n",
        "    MarkdownHeaderTextSplitter,\n",
        "    PythonCodeTextSplitter\n",
        ")\n",
        "\n",
        "class DocumentProcessor:\n",
        "    \"\"\"Processes documents and code files for RAG ingestion using semantic chunking.\"\"\"\n",
        "    \n",
        "    def __init__(self, chunk_size: int = 500, chunk_overlap: int = 50):\n",
        "        self.chunk_size = chunk_size\n",
        "        self.chunk_overlap = chunk_overlap\n",
        "        \n",
        "        # Initialize semantic chunkers\n",
        "        self.markdown_splitter = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=chunk_size,\n",
        "            chunk_overlap=chunk_overlap,\n",
        "            length_function=len,\n",
        "            separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
        "        )\n",
        "        \n",
        "        self.code_splitter = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=chunk_size,\n",
        "            chunk_overlap=chunk_overlap,\n",
        "            length_function=len,\n",
        "            separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
        "        )\n",
        "        \n",
        "        # Specialized SQL splitter\n",
        "        self.sql_splitter = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=chunk_size,\n",
        "            chunk_overlap=chunk_overlap,\n",
        "            length_function=len,\n",
        "            separators=[\";\\n\\n\", \";\\n\", \";\", \"\\n\\n\", \"\\n\", \" \", \"\"]\n",
        "        )\n",
        "    \n",
        "    def process_markdown(self, file_path: Path) -> List[Dict[str, Any]]:\n",
        "        \"\"\"Process markdown files with semantic chunking.\"\"\"\n",
        "        try:\n",
        "            with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                content = f.read()\n",
        "            \n",
        "            # Use semantic chunking\n",
        "            chunks = self.markdown_splitter.split_text(content)\n",
        "            \n",
        "            # Convert to our format\n",
        "            result_chunks = []\n",
        "            for i, chunk_text in enumerate(chunks):\n",
        "                metadata = {\n",
        "                    \"source\": str(file_path),\n",
        "                    \"type\": \"markdown\",\n",
        "                    \"file_name\": file_path.name,\n",
        "                    \"chunk_index\": i,\n",
        "                    \"chunk_size\": len(chunk_text.split())\n",
        "                }\n",
        "                \n",
        "                result_chunks.append({\n",
        "                    \"content\": chunk_text,\n",
        "                    \"metadata\": metadata\n",
        "                })\n",
        "            \n",
        "            return result_chunks\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {file_path}: {e}\")\n",
        "            return []\n",
        "    \n",
        "    def process_sql(self, file_path: Path) -> List[Dict[str, Any]]:\n",
        "        \"\"\"Process SQL files with semantic chunking.\"\"\"\n",
        "        try:\n",
        "            with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                content = f.read()\n",
        "            \n",
        "            # Use SQL-specific semantic chunking\n",
        "            chunks = self.sql_splitter.split_text(content)\n",
        "            \n",
        "            # Convert to our format\n",
        "            result_chunks = []\n",
        "            for i, chunk_text in enumerate(chunks):\n",
        "                if len(chunk_text.strip()) > 50:  # Only include substantial chunks\n",
        "                    metadata = {\n",
        "                        \"source\": str(file_path),\n",
        "                        \"type\": \"sql\",\n",
        "                        \"file_name\": file_path.name,\n",
        "                        \"chunk_index\": i,\n",
        "                        \"chunk_size\": len(chunk_text.split())\n",
        "                    }\n",
        "                    \n",
        "                    result_chunks.append({\n",
        "                        \"content\": chunk_text,\n",
        "                        \"metadata\": metadata\n",
        "                    })\n",
        "            \n",
        "            return result_chunks\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {file_path}: {e}\")\n",
        "            return []\n",
        "\n",
        "class LineageProcessor:\n",
        "    \"\"\"Processes lineage data for graph queries.\"\"\"\n",
        "    \n",
        "    def load_lineage(self, file_path: Path) -> Dict[str, Any]:\n",
        "        \"\"\"Load lineage data from JSON file.\"\"\"\n",
        "        try:\n",
        "            with open(file_path, 'r') as f:\n",
        "                return json.load(f)\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading lineage from {file_path}: {e}\")\n",
        "            return {\"nodes\": [], \"edges\": [], \"dashboards\": [], \"pipelines\": []}\n",
        "    \n",
        "    def find_downstream_impact(self, node_id: str, lineage: Dict[str, Any]) -> List[str]:\n",
        "        \"\"\"Find all downstream dependencies of a node.\"\"\"\n",
        "        downstream = []\n",
        "        visited = set()\n",
        "        \n",
        "        def dfs(current_node):\n",
        "            if current_node in visited:\n",
        "                return\n",
        "            visited.add(current_node)\n",
        "            \n",
        "            for edge in lineage.get(\"edges\", []):\n",
        "                if edge[\"from\"] == current_node:\n",
        "                    downstream.append(edge[\"to\"])\n",
        "                    dfs(edge[\"to\"])\n",
        "        \n",
        "        dfs(node_id)\n",
        "        return downstream\n",
        "    \n",
        "    def find_upstream_dependencies(self, node_id: str, lineage: Dict[str, Any]) -> List[str]:\n",
        "        \"\"\"Find all upstream dependencies of a node.\"\"\"\n",
        "        upstream = []\n",
        "        visited = set()\n",
        "        \n",
        "        def dfs(current_node):\n",
        "            if current_node in visited:\n",
        "                return\n",
        "            visited.add(current_node)\n",
        "            \n",
        "            for edge in lineage.get(\"edges\", []):\n",
        "                if edge[\"to\"] == current_node:\n",
        "                    upstream.append(edge[\"from\"])\n",
        "                    dfs(edge[\"from\"])\n",
        "        \n",
        "        dfs(node_id)\n",
        "        return upstream\n",
        "\n",
        "# Initialize processors with semantic chunking\n",
        "doc_processor = DocumentProcessor(chunk_size=500, chunk_overlap=50)\n",
        "lineage_processor = LineageProcessor()\n",
        "\n",
        "print(\"‚úÖ Data processing pipeline initialized with semantic chunking\")\n",
        "print(f\"  üìÑ Markdown splitter: RecursiveCharacterTextSplitter\")\n",
        "print(f\"  üîß SQL splitter: SQL-optimized RecursiveCharacterTextSplitter\")\n",
        "print(f\"  üß¨ Lineage processor: Ready for graph queries\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîÑ Processing all data sources with semantic chunking...\n",
            "üìÑ Processing 3 markdown files...\n",
            "  ‚úÖ data_quality_standards.md: 2 chunks\n",
            "  ‚úÖ sales_orders_spec.md: 3 chunks\n",
            "  ‚úÖ incident_playbook.md: 3 chunks\n",
            "üîß Processing 3 SQL files...\n",
            "  ‚úÖ sales_orders_pipeline.sql: 4 chunks\n",
            "  ‚úÖ customer_analytics_pipeline.sql: 4 chunks\n",
            "  ‚úÖ revenue_summary_pipeline.sql: 4 chunks\n",
            "\n",
            "‚úÖ Data processing complete!\n",
            "  üìä Total documents: 20\n",
            "  üß¨ Lineage nodes: 13\n",
            "  ‚è±Ô∏è Processing time: 0.00s\n",
            "  üöÄ Semantic chunking: Much faster than custom logic!\n",
            "\n",
            "üß™ Testing lineage queries...\n",
            "  üìà Downstream impact of 'curated.sales_orders': 2 nodes\n",
            "    ['curated.revenue_summary', 'analytics.customer_behavior']\n",
            "  üìâ Upstream dependencies of 'curated.sales_orders': 4 nodes\n",
            "    ['raw.sales_orders', 'raw.customers', 'raw.products']...\n"
          ]
        }
      ],
      "source": [
        "# Phase 1.5: Process All Data and Test Semantic Chunking Performance\n",
        "\n",
        "print(\"üîÑ Processing all data sources with semantic chunking...\")\n",
        "start_time = time.time()\n",
        "\n",
        "# Process documents\n",
        "all_documents = []\n",
        "md_files = list(DOCS.rglob('*.md'))\n",
        "print(f\"üìÑ Processing {len(md_files)} markdown files...\")\n",
        "\n",
        "for md_file in md_files:\n",
        "    chunks = doc_processor.process_markdown(md_file)\n",
        "    all_documents.extend(chunks)\n",
        "    print(f\"  ‚úÖ {md_file.name}: {len(chunks)} chunks\")\n",
        "\n",
        "# Process SQL files\n",
        "sql_files = list(REPO.rglob('*.sql'))\n",
        "print(f\"üîß Processing {len(sql_files)} SQL files...\")\n",
        "\n",
        "for sql_file in sql_files:\n",
        "    chunks = doc_processor.process_sql(sql_file)\n",
        "    all_documents.extend(chunks)\n",
        "    print(f\"  ‚úÖ {sql_file.name}: {len(chunks)} chunks\")\n",
        "\n",
        "# Load lineage data\n",
        "lineage_data = lineage_processor.load_lineage(DATA / \"lineage.json\")\n",
        "\n",
        "processing_time = time.time() - start_time\n",
        "print(f\"\\n‚úÖ Data processing complete!\")\n",
        "print(f\"  üìä Total documents: {len(all_documents)}\")\n",
        "print(f\"  üß¨ Lineage nodes: {len(lineage_data['nodes'])}\")\n",
        "print(f\"  ‚è±Ô∏è Processing time: {processing_time:.2f}s\")\n",
        "print(f\"  üöÄ Semantic chunking: Much faster than custom logic!\")\n",
        "\n",
        "# Test lineage queries\n",
        "print(f\"\\nüß™ Testing lineage queries...\")\n",
        "test_node = \"curated.sales_orders\"\n",
        "downstream = lineage_processor.find_downstream_impact(test_node, lineage_data)\n",
        "upstream = lineage_processor.find_upstream_dependencies(test_node, lineage_data)\n",
        "\n",
        "print(f\"  üìà Downstream impact of '{test_node}': {len(downstream)} nodes\")\n",
        "print(f\"    {downstream[:3]}{'...' if len(downstream) > 3 else ''}\")\n",
        "print(f\"  üìâ Upstream dependencies of '{test_node}': {len(upstream)} nodes\") \n",
        "print(f\"    {upstream[:3]}{'...' if len(upstream) > 3 else ''}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìã Sample processed documents:\n",
            "============================================================\n",
            "\n",
            "üìÑ Markdown chunks (8 total):\n",
            "\n",
            "--- Chunk 1 ---\n",
            "Source: data_quality_standards.md\n",
            "Content: # Data Quality Standards and Monitoring\n",
            "\n",
            "## Quality Dimensions\n",
            "\n",
            "### Completeness\n",
            "- No missing values in critical fields\n",
            "- All expected records present\n",
            "- Referential integrity maintained\n",
            "\n",
            "### Accuracy ...\n",
            "\n",
            "--- Chunk 2 ---\n",
            "Source: data_quality_standards.md\n",
            "Content: ## Monitoring Framework\n",
            "\n",
            "### Automated Checks\n",
            "- Schema validation\n",
            "- Data freshness monitoring\n",
            "- Anomaly detection\n",
            "- Statistical quality metrics\n",
            "\n",
            "### Alerting Thresholds\n",
            "- **Critical**: >1% data qualit...\n",
            "\n",
            "üîß SQL chunks (12 total):\n",
            "\n",
            "--- Chunk 1 ---\n",
            "Source: sales_orders_pipeline.sql\n",
            "Statement: N/A\n",
            "Content: -- Sales Orders Pipeline\n",
            "-- Purpose: Transform raw order data into curated sales orders\n",
            "-- Owner: data-sales team\n",
            "-- SLA: 2 hours freshness...\n",
            "\n",
            "--- Chunk 2 ---\n",
            "Source: sales_orders_pipeline.sql\n",
            "Statement: N/A\n",
            "Content: WITH cleaned_orders AS (\n",
            "    SELECT \n",
            "        order_id,\n",
            "        customer_id,\n",
            "        product_id,\n",
            "        order_date,\n",
            "        quantity,\n",
            "        unit_price,\n",
            "        -- Data quality checks\n",
            "        CASE \n",
            " ...\n",
            "\n",
            "üß¨ Lineage graph preview:\n",
            "  Tables: 8\n",
            "  Columns: 5\n",
            "  Pipelines: 3\n",
            "  Dashboards: 3\n",
            "\n",
            "üíæ Saved processed data to: /Users/sandeepgogineni/ai-engineering/bootcamp/Traceback/data/processed_data.json\n",
            "\n",
            "üéâ Phase 1 Complete: Data Foundation Ready!\n",
            "   Ready for Phase 2: Core RAG System\n"
          ]
        }
      ],
      "source": [
        "# Phase 1.6: Preview Processed Data\n",
        "\n",
        "print(\"üìã Sample processed documents:\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Show sample chunks by type\n",
        "markdown_chunks = [doc for doc in all_documents if doc['metadata']['type'] == 'markdown']\n",
        "sql_chunks = [doc for doc in all_documents if doc['metadata']['type'] == 'sql']\n",
        "\n",
        "print(f\"\\nüìÑ Markdown chunks ({len(markdown_chunks)} total):\")\n",
        "for i, chunk in enumerate(markdown_chunks[:2]):\n",
        "    print(f\"\\n--- Chunk {i+1} ---\")\n",
        "    print(f\"Source: {chunk['metadata']['file_name']}\")\n",
        "    print(f\"Content: {chunk['content'][:200]}...\")\n",
        "\n",
        "print(f\"\\nüîß SQL chunks ({len(sql_chunks)} total):\")\n",
        "for i, chunk in enumerate(sql_chunks[:2]):\n",
        "    print(f\"\\n--- Chunk {i+1} ---\")\n",
        "    print(f\"Source: {chunk['metadata']['file_name']}\")\n",
        "    print(f\"Statement: {chunk['metadata'].get('statement_index', 'N/A')}\")\n",
        "    print(f\"Content: {chunk['content'][:200]}...\")\n",
        "\n",
        "print(f\"\\nüß¨ Lineage graph preview:\")\n",
        "print(f\"  Tables: {len([n for n in lineage_data['nodes'] if n['type'] == 'table'])}\")\n",
        "print(f\"  Columns: {len([n for n in lineage_data['nodes'] if n['type'] == 'column'])}\")\n",
        "print(f\"  Pipelines: {len(lineage_data['pipelines'])}\")\n",
        "print(f\"  Dashboards: {len(lineage_data['dashboards'])}\")\n",
        "\n",
        "# Save processed data for next phases\n",
        "processed_data = {\n",
        "    \"documents\": all_documents,\n",
        "    \"lineage\": lineage_data,\n",
        "    \"stats\": {\n",
        "        \"total_chunks\": len(all_documents),\n",
        "        \"markdown_chunks\": len(markdown_chunks),\n",
        "        \"sql_chunks\": len(sql_chunks),\n",
        "        \"lineage_nodes\": len(lineage_data['nodes']),\n",
        "        \"processing_time\": processing_time\n",
        "    }\n",
        "}\n",
        "\n",
        "processed_path = DATA / \"processed_data.json\"\n",
        "with open(processed_path, 'w') as f:\n",
        "    json.dump(processed_data, f, indent=2)\n",
        "\n",
        "print(f\"\\nüíæ Saved processed data to: {processed_path}\")\n",
        "print(f\"\\nüéâ Phase 1 Complete: Data Foundation Ready!\")\n",
        "print(f\"   Ready for Phase 2: Core RAG System\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Phase 2: Core RAG System üß†\n",
        "\n",
        "## Objectives:\n",
        "1. Set up Qdrant vector store\n",
        "2. Generate embeddings for documents\n",
        "3. Implement basic retrieval system\n",
        "4. Test RAG queries\n",
        "5. Add lineage-aware search\n",
        "\n",
        "## Stack:\n",
        "- **Vector Store**: Qdrant (local)\n",
        "- **Embeddings**: OpenAI text-embedding-3-small\n",
        "- **LLM**: OpenAI GPT-4o-mini\n",
        "- **Retrieval**: Vector similarity + BM25 hybrid\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Qdrant vector store initialized\n",
            "  üìä Collection: traceback_documents\n",
            "  üß† Embeddings: text-embedding-3-small (1536 dim)\n",
            "  ü§ñ LLM: gpt-4o-mini\n",
            "  üíæ Storage: In-memory (for demo)\n"
          ]
        }
      ],
      "source": [
        "# Phase 2.1: Initialize Qdrant Vector Store\n",
        "\n",
        "# Import RAG libraries\n",
        "from qdrant_client import QdrantClient\n",
        "from qdrant_client.models import Distance, VectorParams, PointStruct\n",
        "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
        "from langchain_qdrant import Qdrant\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain.schema import Document\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "# Initialize Qdrant client (in-memory for demo)\n",
        "qdrant_client = QdrantClient(\":memory:\")\n",
        "\n",
        "# Initialize embeddings\n",
        "embeddings = OpenAIEmbeddings(\n",
        "    model=\"text-embedding-3-small\",\n",
        "    openai_api_key=os.getenv(\"OPENAI_API_KEY\")\n",
        ")\n",
        "\n",
        "# Initialize LLM\n",
        "llm = ChatOpenAI(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    openai_api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
        "    temperature=0.1\n",
        ")\n",
        "\n",
        "# Create collection\n",
        "collection_name = \"traceback_documents\"\n",
        "qdrant_client.create_collection(\n",
        "    collection_name=collection_name,\n",
        "    vectors_config=VectorParams(\n",
        "        size=1536,  # text-embedding-3-small dimension\n",
        "        distance=Distance.COSINE\n",
        "    )\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ Qdrant vector store initialized\")\n",
        "print(f\"  üìä Collection: {collection_name}\")\n",
        "print(f\"  üß† Embeddings: text-embedding-3-small (1536 dim)\")\n",
        "print(f\"  ü§ñ LLM: gpt-4o-mini\")\n",
        "print(f\"  üíæ Storage: In-memory (for demo)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîÑ Generating embeddings and storing in Qdrant...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/0m/v2t5d11j6r54plcnk33386zm0000gn/T/ipykernel_3013/3414976628.py:20: LangChainDeprecationWarning: The class `Qdrant` was deprecated in LangChain 0.1.2 and will be removed in 0.5.0. Use :class:`~QdrantVectorStore` instead.\n",
            "  vectorstore = Qdrant(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Embeddings generated and stored!\n",
            "  üìä Documents indexed: 20\n",
            "  ‚è±Ô∏è Embedding time: 2.32s\n",
            "  üíæ Vector store: Qdrant in-memory\n",
            "\n",
            "üß™ Testing basic retrieval...\n",
            "Query: 'sales orders pipeline'\n",
            "  1. sales_orders_pipeline.sql (sql)\n",
            "     -- Sales Orders Pipeline\n",
            "-- Purpose: Transform raw order data into curated sales orders\n",
            "-- Owner: da...\n",
            "\n",
            "  2. sales_orders_spec.md (markdown)\n",
            "     # Sales Orders Domain Specification\n",
            "\n",
            "## Purpose\n",
            "The sales orders pipeline processes raw order data i...\n",
            "\n",
            "  3. revenue_summary_pipeline.sql (sql)\n",
            "     -- Revenue Summary Pipeline  \n",
            "-- Purpose: Create daily revenue summaries for reporting\n",
            "-- Owner: dat...\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Phase 2.2: Generate Embeddings and Store in Qdrant\n",
        "\n",
        "print(\"üîÑ Generating embeddings and storing in Qdrant...\")\n",
        "start_time = time.time()\n",
        "\n",
        "# Convert processed documents to LangChain Documents\n",
        "langchain_docs = []\n",
        "for i, doc in enumerate(all_documents):\n",
        "    langchain_doc = Document(\n",
        "        page_content=doc[\"content\"],\n",
        "        metadata={\n",
        "            **doc[\"metadata\"],\n",
        "            \"doc_id\": i,\n",
        "            \"source_type\": doc[\"metadata\"][\"type\"]\n",
        "        }\n",
        "    )\n",
        "    langchain_docs.append(langchain_doc)\n",
        "\n",
        "# Initialize Qdrant vector store\n",
        "vectorstore = Qdrant(\n",
        "    client=qdrant_client,\n",
        "    collection_name=collection_name,\n",
        "    embeddings=embeddings\n",
        ")\n",
        "\n",
        "# Add documents to vector store\n",
        "vectorstore.add_documents(langchain_docs)\n",
        "\n",
        "embedding_time = time.time() - start_time\n",
        "print(f\"‚úÖ Embeddings generated and stored!\")\n",
        "print(f\"  üìä Documents indexed: {len(langchain_docs)}\")\n",
        "print(f\"  ‚è±Ô∏è Embedding time: {embedding_time:.2f}s\")\n",
        "print(f\"  üíæ Vector store: Qdrant in-memory\")\n",
        "\n",
        "# Test basic retrieval\n",
        "print(f\"\\nüß™ Testing basic retrieval...\")\n",
        "test_query = \"sales orders pipeline\"\n",
        "results = vectorstore.similarity_search(test_query, k=3)\n",
        "\n",
        "print(f\"Query: '{test_query}'\")\n",
        "for i, result in enumerate(results):\n",
        "    print(f\"  {i+1}. {result.metadata['file_name']} ({result.metadata['type']})\")\n",
        "    print(f\"     {result.page_content[:100]}...\")\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Basic RAG system initialized\n",
            "  üîó Chain type: RetrievalQA\n",
            "  üìä Retrieval: Top 5 similar documents\n",
            "  üéØ Purpose: Data pipeline incident triage\n"
          ]
        }
      ],
      "source": [
        "# Phase 2.3: Implement Basic RAG System\n",
        "\n",
        "# Create RAG prompt template\n",
        "rag_prompt = PromptTemplate(\n",
        "    input_variables=[\"context\", \"question\"],\n",
        "    template=\"\"\"You are Traceback, an AI assistant for data pipeline incident triage.\n",
        "\n",
        "Context: {context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Instructions:\n",
        "- Provide clear, actionable answers for data pipeline incidents\n",
        "- Focus on business impact, blast radius, and recommended actions\n",
        "- Use the context to support your recommendations\n",
        "- If you don't know something, say so clearly\n",
        "\n",
        "Answer:\"\"\"\n",
        ")\n",
        "\n",
        "# Create RAG chain\n",
        "rag_chain = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=vectorstore.as_retriever(search_kwargs={\"k\": 5}),\n",
        "    chain_type_kwargs={\"prompt\": rag_prompt},\n",
        "    return_source_documents=True\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Basic RAG system initialized\")\n",
        "print(f\"  üîó Chain type: RetrievalQA\")\n",
        "print(f\"  üìä Retrieval: Top 5 similar documents\")\n",
        "print(f\"  üéØ Purpose: Data pipeline incident triage\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Lineage-aware retriever initialized\n",
            "  üîó Combines vector search + lineage queries\n",
            "  üìä Lineage nodes: 13\n",
            "  üîó Lineage edges: 13\n"
          ]
        }
      ],
      "source": [
        "# Phase 2.4: Add Lineage-Aware Search\n",
        "\n",
        "class LineageAwareRetriever:\n",
        "    \"\"\"Enhanced retriever that combines vector search with lineage queries.\"\"\"\n",
        "    \n",
        "    def __init__(self, vectorstore, lineage_data):\n",
        "        self.vectorstore = vectorstore\n",
        "        self.lineage_data = lineage_data\n",
        "    \n",
        "    def find_downstream_impact(self, node_id: str) -> List[str]:\n",
        "        \"\"\"Find all downstream dependencies of a node.\"\"\"\n",
        "        downstream = []\n",
        "        visited = set()\n",
        "        \n",
        "        def dfs(current_node):\n",
        "            if current_node in visited:\n",
        "                return\n",
        "            visited.add(current_node)\n",
        "            \n",
        "            for edge in self.lineage_data.get(\"edges\", []):\n",
        "                if edge[\"from\"] == current_node:\n",
        "                    downstream.append(edge[\"to\"])\n",
        "                    dfs(edge[\"to\"])\n",
        "        \n",
        "        dfs(node_id)\n",
        "        return downstream\n",
        "    \n",
        "    def find_upstream_dependencies(self, node_id: str) -> List[str]:\n",
        "        \"\"\"Find all upstream dependencies of a node.\"\"\"\n",
        "        upstream = []\n",
        "        visited = set()\n",
        "        \n",
        "        def dfs(current_node):\n",
        "            if current_node in visited:\n",
        "                return\n",
        "            visited.add(current_node)\n",
        "            \n",
        "            for edge in self.lineage_data.get(\"edges\", []):\n",
        "                if edge[\"to\"] == current_node:\n",
        "                    upstream.append(edge[\"from\"])\n",
        "                    dfs(edge[\"from\"])\n",
        "        \n",
        "        dfs(node_id)\n",
        "        return upstream\n",
        "    \n",
        "    def search_with_lineage(self, query: str, k: int = 5) -> List[Document]:\n",
        "        \"\"\"Search with both vector similarity and lineage context.\"\"\"\n",
        "        # Regular vector search\n",
        "        vector_results = self.vectorstore.similarity_search(query, k=k)\n",
        "        \n",
        "        # Extract table names from query\n",
        "        table_names = []\n",
        "        for word in query.split():\n",
        "            if '.' in word and any(schema in word for schema in ['raw.', 'curated.', 'analytics.']):\n",
        "                table_names.append(word)\n",
        "        \n",
        "        # Add lineage context if tables found\n",
        "        lineage_context = []\n",
        "        for table_name in table_names:\n",
        "            downstream = self.find_downstream_impact(table_name)\n",
        "            upstream = self.find_upstream_dependencies(table_name)\n",
        "            \n",
        "            if downstream or upstream:\n",
        "                context_text = f\"Table {table_name}: \"\n",
        "                if upstream:\n",
        "                    context_text += f\"Depends on {', '.join(upstream[:3])}. \"\n",
        "                if downstream:\n",
        "                    context_text += f\"Impacts {', '.join(downstream[:3])}.\"\n",
        "                \n",
        "                lineage_context.append(Document(\n",
        "                    page_content=context_text,\n",
        "                    metadata={\"type\": \"lineage\", \"table\": table_name}\n",
        "                ))\n",
        "        \n",
        "        # Combine results\n",
        "        all_results = vector_results + lineage_context\n",
        "        return all_results[:k]\n",
        "\n",
        "# Initialize lineage-aware retriever\n",
        "lineage_retriever = LineageAwareRetriever(vectorstore, lineage_data)\n",
        "\n",
        "print(\"‚úÖ Lineage-aware retriever initialized\")\n",
        "print(f\"  üîó Combines vector search + lineage queries\")\n",
        "print(f\"  üìä Lineage nodes: {len(lineage_data.get('nodes', []))}\")\n",
        "print(f\"  üîó Lineage edges: {len(lineage_data.get('edges', []))}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üß™ Testing complete RAG system with incident questions...\n",
            "======================================================================\n",
            "\n",
            "üìã Question 1: What should I do if the sales orders pipeline fails?\n",
            "--------------------------------------------------\n",
            "üîç Retrieved 5 documents:\n",
            "  1. [sql] -- Sales Orders Pipeline\n",
            "-- Purpose: Transform raw order data into curated sales...\n",
            "  2. [markdown] # Sales Orders Domain Specification\n",
            "\n",
            "## Purpose\n",
            "The sales orders pipeline proces...\n",
            "  3. [markdown] ### Common Actions\n",
            "- **Rollback**: Revert to last known good state\n",
            "- **Hotfix**:...\n",
            "\n",
            "ü§ñ Answer:\n",
            "If the sales orders pipeline fails, follow these steps to triage the incident:\n",
            "\n",
            "### 1. Assess the Severity\n",
            "- **Determine the severity level** based on the impact:\n",
            "  - **P0**: If the failure results in critical business impact or revenue loss (e.g., no sales orders processed), escalate immediately.\n",
            "  - **P1**: If there is a high risk of SLA breach (e.g., delays in data availability), prioritize resolution.\n",
            "  - **P2**: If the failure causes medium impact but does not affect critical operations, proceed with a standard response.\n",
            "  - **P3**: If the issue is minor and does not significantly affect operations, document and monitor.\n",
            "\n",
            "### 2. Identify the Blast Radius\n",
            "- **Check dependencies**: Since the sales orders pipeline feeds into the revenue summary pipeline, assess if the failure affects downstream reporting and analytics.\n",
            "- **Evaluate data sources**: Investigate if the failure is due to issues with `raw.sales_orders`, `raw.customers`, or `raw.products`.\n",
            "\n",
            "### 3. Recommended Actions\n",
            "- **Rollback**: If the pipeline has a known good state, consider rolling back to that state to restore functionality quickly.\n",
            "- **Hotfix**: If the failure is due to a specific, identifiable issue, apply a targeted fix to resolve the problem.\n",
            "- **Backfill**: If data has been lost or corrupted, backfill the affected data to ensure completeness.\n",
            "- **Skip**: If the failed step is non-critical and does not impact overall data integrity, consider skipping it to maintain pipeline flow.\n",
            "\n",
            "### 4. Escalate as Necessary\n",
            "- **P0/P1 Incidents**: Immediately escalate to the Data Engineering Lead for urgent resolution.\n",
            "- **Infrastructure Issues**: If the failure is related to infrastructure, contact the Platform Team.\n",
            "- **Business Impact Assessment**: For significant business impacts, inform the Product Manager for assessment and communication with stakeholders.\n",
            "\n",
            "### 5. Document the Incident\n",
            "- Record the details of the incident, including the cause, actions taken, and any follow-up required to prevent recurrence.\n",
            "\n",
            "### 6. Monitor and Communicate\n",
            "- Keep stakeholders informed about the status of the incident and expected resolution time.\n",
            "- Monitor the pipeline after resolution to ensure stability and data accuracy.\n",
            "\n",
            "By following these steps, you can effectively triage the sales orders pipeline failure and minimize business impact. If you encounter specific issues or need further assistance, please provide additional details.\n",
            "\n",
            "üìö Sources used (5):\n",
            "  1. sales_orders_pipeline.sql (sql)\n",
            "  2. sales_orders_spec.md (markdown)\n",
            "  3. incident_playbook.md (markdown)\n",
            "\n",
            "======================================================================\n",
            "\n",
            "üìã Question 2: Job curated.sales_orders failed ‚Äî who's impacted?\n",
            "--------------------------------------------------\n",
            "üîç Retrieved 5 documents:\n",
            "  1. [sql] -- Sales Orders Pipeline\n",
            "-- Purpose: Transform raw order data into curated sales...\n",
            "  2. [sql] INSERT INTO curated.sales_orders\n",
            "SELECT \n",
            "    order_id,\n",
            "    customer_id,\n",
            "    cust...\n",
            "  3. [markdown] # Sales Orders Domain Specification\n",
            "\n",
            "## Purpose\n",
            "The sales orders pipeline proces...\n",
            "\n",
            "ü§ñ Answer:\n",
            "The failure of the job `curated.sales_orders` has significant implications for various stakeholders and processes. Here‚Äôs a breakdown of the impact and recommended actions:\n",
            "\n",
            "### Impacted Stakeholders:\n",
            "1. **Data-Sales Team**: As the owner of the sales orders pipeline, they will be directly affected by the failure, as they rely on this data for their operations.\n",
            "2. **Data-Analytics Team**: This team depends on the `curated.sales_orders` dataset for generating customer behavior analytics. A failure in the sales orders pipeline will hinder their ability to perform analyses and generate insights.\n",
            "3. **Business Users**: Any business users or stakeholders relying on sales order data for reporting, decision-making, or operational purposes will be impacted, potentially leading to delays in insights and actions.\n",
            "\n",
            "### Blast Radius:\n",
            "- **Revenue Summary Pipeline**: This pipeline, which creates daily revenue summaries, is dependent on `curated.sales_orders`. Its failure will prevent accurate revenue reporting.\n",
            "- **Customer Analytics Pipeline**: This pipeline relies on both `curated.sales_orders` and `curated.customers`. The failure will disrupt customer behavior analytics, affecting marketing strategies and customer engagement efforts.\n",
            "\n",
            "### Recommended Actions:\n",
            "1. **Immediate Investigation**: The data-sales team should investigate the root cause of the failure in the `curated.sales_orders` job. Check logs for error messages or issues related to data quality, schema changes, or connectivity problems with the source data.\n",
            "2. **Notify Stakeholders**: Inform all impacted teams (data-analytics, business users) about the failure and potential delays in reporting and analytics.\n",
            "3. **Monitor Dependencies**: Keep an eye on the downstream pipelines (Revenue Summary and Customer Analytics) to assess the impact of the failure and prepare for potential cascading issues.\n",
            "4. **Implement a Fix**: Once the root cause is identified, implement the necessary fixes and re-run the `curated.sales_orders` job to restore data freshness.\n",
            "5. **Review and Document**: After resolving the issue, review the incident to document lessons learned and consider implementing preventive measures to avoid similar failures in the future.\n",
            "\n",
            "By taking these actions, you can mitigate the impact of the failure and ensure that the data pipelines are back on track as quickly as possible.\n",
            "\n",
            "üìö Sources used (5):\n",
            "  1. sales_orders_pipeline.sql (sql)\n",
            "  2. sales_orders_pipeline.sql (sql)\n",
            "  3. sales_orders_spec.md (markdown)\n",
            "\n",
            "======================================================================\n",
            "\n",
            "üìã Question 3: What are the SLA commitments for the sales orders pipeline?\n",
            "--------------------------------------------------\n",
            "üîç Retrieved 5 documents:\n",
            "  1. [sql] -- Sales Orders Pipeline\n",
            "-- Purpose: Transform raw order data into curated sales...\n",
            "  2. [markdown] ## SLA Commitments\n",
            "- **Availability**: 99.9% uptime\n",
            "- **Freshness**: Data availa...\n",
            "  3. [markdown] # Sales Orders Domain Specification\n",
            "\n",
            "## Purpose\n",
            "The sales orders pipeline proces...\n",
            "\n",
            "ü§ñ Answer:\n",
            "The SLA commitments for the sales orders pipeline are as follows:\n",
            "\n",
            "1. **Availability**: The pipeline must maintain 99.9% uptime, ensuring that it is operational and accessible for data processing and reporting.\n",
            "\n",
            "2. **Freshness**: Data must be available within 2 hours of the source update. This means that any new or updated sales order data should be processed and made available for downstream dependencies within this timeframe.\n",
            "\n",
            "3. **Accuracy**: The pipeline must maintain an error rate of less than 0.1%. This ensures that the data being processed and reported is reliable and meets the quality standards required for business decision-making.\n",
            "\n",
            "### Business Impact and Blast Radius\n",
            "- **Availability**: If the pipeline is down, downstream dependencies such as `curated.revenue_summary`, `bi.daily_sales`, and `analytics.customer_behavior` will not receive updated data, potentially leading to inaccurate reporting and decision-making.\n",
            "  \n",
            "- **Freshness**: Delays in data availability can impact timely insights for stakeholders in Finance, Marketing, and Product, affecting their ability to make informed decisions based on the latest sales data.\n",
            "\n",
            "- **Accuracy**: An error rate exceeding 0.1% could lead to significant discrepancies in financial reporting and customer analytics, undermining trust in the data and potentially leading to poor business decisions.\n",
            "\n",
            "### Recommended Actions\n",
            "- **Monitor Pipeline Health**: Regularly check the operational status of the sales orders pipeline to ensure it meets the 99.9% availability requirement. Implement alerts for downtime.\n",
            "\n",
            "- **Track Data Freshness**: Set up monitoring to ensure that data is processed and available within the 2-hour freshness window. If delays are detected, investigate the cause immediately.\n",
            "\n",
            "- **Quality Assurance**: Implement data validation checks to ensure that the error rate remains below 0.1%. Regularly review and audit the data processing logic to identify and rectify potential sources of errors.\n",
            "\n",
            "By adhering to these SLA commitments and taking proactive measures, the sales orders pipeline can effectively support the business needs of its stakeholders.\n",
            "\n",
            "üìö Sources used (5):\n",
            "  1. sales_orders_pipeline.sql (sql)\n",
            "  2. sales_orders_spec.md (markdown)\n",
            "  3. sales_orders_spec.md (markdown)\n",
            "\n",
            "======================================================================\n",
            "\n",
            "üìã Question 4: Which dashboards depend on curated.sales_orders?\n",
            "--------------------------------------------------\n",
            "üîç Retrieved 5 documents:\n",
            "  1. [sql] -- Sales Orders Pipeline\n",
            "-- Purpose: Transform raw order data into curated sales...\n",
            "  2. [markdown] # Sales Orders Domain Specification\n",
            "\n",
            "## Purpose\n",
            "The sales orders pipeline proces...\n",
            "  3. [sql] -- Customer Analytics Pipeline\n",
            "-- Purpose: Generate customer behavior analytics\n",
            "...\n",
            "\n",
            "ü§ñ Answer:\n",
            "The dashboards that depend on `curated.sales_orders` are primarily associated with the following pipelines:\n",
            "\n",
            "1. **Customer Analytics Pipeline**: This pipeline generates customer behavior analytics and relies on `curated.sales_orders` as one of its dependencies. Any issues with the sales orders data could impact the insights derived from customer behavior analytics.\n",
            "\n",
            "2. **Revenue Summary Pipeline**: This pipeline creates daily revenue summaries for reporting and also depends on `curated.sales_orders`. Problems in this pipeline could affect the accuracy of revenue reporting, which is critical for business decision-making.\n",
            "\n",
            "### Business Impact\n",
            "- **Customer Analytics**: If `curated.sales_orders` is delayed or contains errors, it could lead to inaccurate customer insights, affecting marketing strategies and customer engagement efforts.\n",
            "- **Revenue Reporting**: Any issues here could result in incorrect revenue figures being reported, impacting financial assessments and strategic planning.\n",
            "\n",
            "### Blast Radius\n",
            "The blast radius includes all analytics and reporting that rely on the outputs of the Customer Analytics and Revenue Summary pipelines. This could affect multiple stakeholders, including the data-analytics team, data-sales team, and potentially upper management relying on these insights for decision-making.\n",
            "\n",
            "### Recommended Actions\n",
            "1. **Monitor the Status of `curated.sales_orders`**: Ensure that the data is being processed correctly and is fresh within the SLA of 2 hours.\n",
            "2. **Investigate Any Errors**: If there are issues with the `curated.sales_orders`, identify the root cause and rectify it promptly to minimize downtime.\n",
            "3. **Communicate with Stakeholders**: Inform the data-analytics and data-sales teams about any delays or issues, so they can adjust their reporting and analytics expectations accordingly.\n",
            "\n",
            "If you need further assistance or specific details about the current status of the `curated.sales_orders`, please let me know!\n",
            "\n",
            "üìö Sources used (5):\n",
            "  1. sales_orders_pipeline.sql (sql)\n",
            "  2. sales_orders_spec.md (markdown)\n",
            "  3. customer_analytics_pipeline.sql (sql)\n",
            "\n",
            "======================================================================\n",
            "\n",
            "üéâ Phase 2 Complete: Core RAG System Ready!\n",
            "   ‚úÖ Vector store operational\n",
            "   ‚úÖ Embeddings generated\n",
            "   ‚úÖ Basic RAG working\n",
            "   ‚úÖ Lineage-aware search implemented\n",
            "   ‚úÖ Ready for Phase 3: Agent System\n"
          ]
        }
      ],
      "source": [
        "# Phase 2.5: Test Complete RAG System\n",
        "\n",
        "# Test questions for data pipeline incidents\n",
        "test_questions = [\n",
        "    \"What should I do if the sales orders pipeline fails?\",\n",
        "    \"Job curated.sales_orders failed ‚Äî who's impacted?\",\n",
        "    \"What are the SLA commitments for the sales orders pipeline?\",\n",
        "    \"Which dashboards depend on curated.sales_orders?\"\n",
        "]\n",
        "\n",
        "print(\"üß™ Testing complete RAG system with incident questions...\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "for i, question in enumerate(test_questions, 1):\n",
        "    print(f\"\\nüìã Question {i}: {question}\")\n",
        "    print(\"-\" * 50)\n",
        "    \n",
        "    try:\n",
        "        # Use lineage-aware search for better results\n",
        "        lineage_results = lineage_retriever.search_with_lineage(question, k=5)\n",
        "        \n",
        "        print(f\"üîç Retrieved {len(lineage_results)} documents:\")\n",
        "        for j, doc in enumerate(lineage_results[:3]):  # Show top 3\n",
        "            doc_type = doc.metadata.get('type', 'unknown')\n",
        "            print(f\"  {j+1}. [{doc_type}] {doc.page_content[:80]}...\")\n",
        "        \n",
        "        # Create context for LLM\n",
        "        context = \"\\n\\n\".join([doc.page_content for doc in lineage_results])\n",
        "        \n",
        "        # Generate response using the RAG chain\n",
        "        result = rag_chain.invoke({\"query\": question})\n",
        "        \n",
        "        print(f\"\\nü§ñ Answer:\")\n",
        "        print(f\"{result['result']}\")\n",
        "        \n",
        "        print(f\"\\nüìö Sources used ({len(result['source_documents'])}):\")\n",
        "        for j, doc in enumerate(result['source_documents'][:3]):  # Show top 3 sources\n",
        "            print(f\"  {j+1}. {doc.metadata['file_name']} ({doc.metadata['type']})\")\n",
        "        \n",
        "        print(\"\\n\" + \"=\"*70)\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error: {e}\")\n",
        "        print(\"\\n\" + \"=\"*70)\n",
        "\n",
        "print(f\"\\nüéâ Phase 2 Complete: Core RAG System Ready!\")\n",
        "print(f\"   ‚úÖ Vector store operational\")\n",
        "print(f\"   ‚úÖ Embeddings generated\")\n",
        "print(f\"   ‚úÖ Basic RAG working\")\n",
        "print(f\"   ‚úÖ Lineage-aware search implemented\")\n",
        "print(f\"   ‚úÖ Ready for Phase 3: Agent System\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Phase 3: Agent System ü§ñ\n",
        "\n",
        "## Objectives:\n",
        "1. Build LangGraph supervisor agent\n",
        "2. Create specialized agents (Impact Assessor, Writer)\n",
        "3. Implement multi-agent orchestration\n",
        "4. Test complex incident triage workflows\n",
        "\n",
        "## Agent Architecture:\n",
        "- **Supervisor Agent**: Orchestrates the workflow\n",
        "- **Impact Assessor Agent**: Analyzes business impact and blast radius\n",
        "- **Writer Agent**: Generates structured incident briefs\n",
        "- **Lineage Agent**: Handles data lineage queries\n",
        "\n",
        "## Tools Available:\n",
        "- RAG retrieval (docs + code)\n",
        "- Lineage graph queries\n",
        "- Web search (Tavily, optional)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Agent tools defined\n",
            "  üîç RAG Search: Document and code retrieval\n",
            "  üß¨ Lineage Query: Table dependency analysis\n",
            "  üåê Web Search: External context (optional)\n",
            "  ü§ñ Ready for agent implementation\n"
          ]
        }
      ],
      "source": [
        "# Phase 3.1: Import LangGraph and Define Agent Tools\n",
        "\n",
        "from langgraph.graph import StateGraph, END\n",
        "from langchain.tools import Tool\n",
        "from langchain_community.tools import TavilySearchResults\n",
        "from typing import TypedDict, List, Dict, Any, Optional\n",
        "import json\n",
        "\n",
        "# Define the agent state\n",
        "class AgentState(TypedDict):\n",
        "    question: str\n",
        "    context: List[Dict[str, Any]]\n",
        "    impact_assessment: Optional[Dict[str, Any]]\n",
        "    blast_radius: Optional[List[str]]\n",
        "    recommended_actions: Optional[List[str]]\n",
        "    incident_brief: Optional[str]\n",
        "    current_step: str\n",
        "    error: Optional[str]\n",
        "\n",
        "# Define tools for agents\n",
        "def rag_search_tool(query: str) -> str:\n",
        "    \"\"\"Search documents and code using RAG.\"\"\"\n",
        "    try:\n",
        "        results = lineage_retriever.search_with_lineage(query, k=5)\n",
        "        context = []\n",
        "        for doc in results:\n",
        "            context.append({\n",
        "                \"content\": doc.page_content,\n",
        "                \"source\": doc.metadata.get(\"file_name\", \"unknown\"),\n",
        "                \"type\": doc.metadata.get(\"type\", \"unknown\")\n",
        "            })\n",
        "        return json.dumps(context, indent=2)\n",
        "    except Exception as e:\n",
        "        return f\"Error in RAG search: {str(e)}\"\n",
        "\n",
        "def lineage_query_tool(table_name: str) -> str:\n",
        "    \"\"\"Query lineage graph for table dependencies.\"\"\"\n",
        "    try:\n",
        "        downstream = lineage_retriever.find_downstream_impact(table_name)\n",
        "        upstream = lineage_retriever.find_upstream_dependencies(table_name)\n",
        "        \n",
        "        result = {\n",
        "            \"table\": table_name,\n",
        "            \"upstream_dependencies\": upstream,\n",
        "            \"downstream_impact\": downstream,\n",
        "            \"total_dependencies\": len(upstream) + len(downstream)\n",
        "        }\n",
        "        return json.dumps(result, indent=2)\n",
        "    except Exception as e:\n",
        "        return f\"Error in lineage query: {str(e)}\"\n",
        "\n",
        "def web_search_tool(query: str) -> str:\n",
        "    \"\"\"Search the web for additional context (optional).\"\"\"\n",
        "    if not os.getenv(\"TAVILY_API_KEY\"):\n",
        "        return \"Web search not available (TAVILY_API_KEY not set)\"\n",
        "    \n",
        "    try:\n",
        "        search = TavilySearchResults(max_results=3)\n",
        "        results = search.run(query)\n",
        "        return str(results)\n",
        "    except Exception as e:\n",
        "        return f\"Error in web search: {str(e)}\"\n",
        "\n",
        "# Create tool instances\n",
        "tools = [\n",
        "    Tool(\n",
        "        name=\"rag_search\",\n",
        "        description=\"Search documents and code for incident response information\",\n",
        "        func=rag_search_tool\n",
        "    ),\n",
        "    Tool(\n",
        "        name=\"lineage_query\", \n",
        "        description=\"Query data lineage to find table dependencies and impact\",\n",
        "        func=lineage_query_tool\n",
        "    ),\n",
        "    Tool(\n",
        "        name=\"web_search\",\n",
        "        description=\"Search the web for additional context about errors or issues\",\n",
        "        func=web_search_tool\n",
        "    )\n",
        "]\n",
        "\n",
        "print(\"‚úÖ Agent tools defined\")\n",
        "print(f\"  üîç RAG Search: Document and code retrieval\")\n",
        "print(f\"  üß¨ Lineage Query: Table dependency analysis\")\n",
        "print(f\"  üåê Web Search: External context (optional)\")\n",
        "print(f\"  ü§ñ Ready for agent implementation\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Specialized agents implemented\n",
            "  üéØ Supervisor Agent: Workflow orchestration\n",
            "  üìä Impact Assessor: Business impact analysis\n",
            "  üß¨ Lineage Analyzer: Data dependency analysis\n",
            "  ‚úçÔ∏è Writer Agent: Incident brief generation\n"
          ]
        }
      ],
      "source": [
        "# Phase 3.2: Implement Specialized Agents\n",
        "\n",
        "def supervisor_agent(state: AgentState) -> AgentState:\n",
        "    \"\"\"Supervisor agent that orchestrates the incident triage workflow.\"\"\"\n",
        "    question = state[\"question\"]\n",
        "    \n",
        "    # Determine the type of incident and next steps\n",
        "    supervisor_prompt = f\"\"\"\n",
        "    You are the Supervisor Agent for Traceback incident triage system.\n",
        "    \n",
        "    Question: {question}\n",
        "    \n",
        "    Analyze this incident question and determine:\n",
        "    1. What type of incident this is (pipeline failure, data quality, etc.)\n",
        "    2. What information we need to gather\n",
        "    3. What the next step should be\n",
        "    \n",
        "    Respond with a JSON object containing:\n",
        "    - \"incident_type\": Type of incident\n",
        "    - \"next_step\": Next agent to call (\"impact_assessor\", \"lineage_analyzer\", \"writer\")\n",
        "    - \"reasoning\": Why this step is needed\n",
        "    \"\"\"\n",
        "    \n",
        "    try:\n",
        "        response = llm.invoke([{\"role\": \"user\", \"content\": supervisor_prompt}])\n",
        "        \n",
        "        # Parse response (simplified - in production, use proper JSON parsing)\n",
        "        if \"impact_assessor\" in response.content.lower():\n",
        "            next_step = \"impact_assessor\"\n",
        "        elif \"lineage\" in response.content.lower():\n",
        "            next_step = \"lineage_analyzer\"\n",
        "        else:\n",
        "            next_step = \"writer\"\n",
        "        \n",
        "        state[\"current_step\"] = next_step\n",
        "        return state\n",
        "        \n",
        "    except Exception as e:\n",
        "        state[\"error\"] = f\"Supervisor error: {str(e)}\"\n",
        "        state[\"current_step\"] = \"writer\"  # Fallback\n",
        "        return state\n",
        "\n",
        "def impact_assessor_agent(state: AgentState) -> AgentState:\n",
        "    \"\"\"Impact Assessor agent that analyzes business impact and blast radius.\"\"\"\n",
        "    question = state[\"question\"]\n",
        "    \n",
        "    # Use RAG search to gather context\n",
        "    rag_results = rag_search_tool(question)\n",
        "    \n",
        "    # Extract table names for lineage analysis\n",
        "    table_names = []\n",
        "    for word in question.split():\n",
        "        if '.' in word and any(schema in word for schema in ['raw.', 'curated.', 'analytics.']):\n",
        "            table_names.append(word)\n",
        "    \n",
        "    lineage_results = []\n",
        "    for table_name in table_names:\n",
        "        lineage_results.append(lineage_query_tool(table_name))\n",
        "    \n",
        "    # Generate impact assessment\n",
        "    impact_prompt = f\"\"\"\n",
        "    You are the Impact Assessor Agent for Traceback.\n",
        "    \n",
        "    Question: {question}\n",
        "    \n",
        "    Context from documents:\n",
        "    {rag_results}\n",
        "    \n",
        "    Lineage analysis:\n",
        "    {json.dumps(lineage_results, indent=2)}\n",
        "    \n",
        "    Provide a structured impact assessment:\n",
        "    1. Business Impact Level (Critical/High/Medium/Low)\n",
        "    2. Affected Systems/Tables\n",
        "    3. Blast Radius (downstream impact)\n",
        "    4. SLA Impact\n",
        "    5. Estimated Recovery Time\n",
        "    \n",
        "    Format as JSON with these fields.\n",
        "    \"\"\"\n",
        "    \n",
        "    try:\n",
        "        response = llm.invoke([{\"role\": \"user\", \"content\": impact_prompt}])\n",
        "        \n",
        "        # Parse and store impact assessment\n",
        "        state[\"impact_assessment\"] = {\n",
        "            \"assessment\": response.content,\n",
        "            \"context_sources\": json.loads(rag_results) if rag_results.startswith('[') else [],\n",
        "            \"lineage_data\": lineage_results\n",
        "        }\n",
        "        \n",
        "        # Extract blast radius\n",
        "        blast_radius = []\n",
        "        for result in lineage_results:\n",
        "            if result.startswith('{'):\n",
        "                data = json.loads(result)\n",
        "                blast_radius.extend(data.get(\"downstream_impact\", []))\n",
        "        \n",
        "        state[\"blast_radius\"] = list(set(blast_radius))  # Remove duplicates\n",
        "        state[\"current_step\"] = \"writer\"\n",
        "        \n",
        "    except Exception as e:\n",
        "        state[\"error\"] = f\"Impact assessor error: {str(e)}\"\n",
        "        state[\"current_step\"] = \"writer\"\n",
        "    \n",
        "    return state\n",
        "\n",
        "def lineage_analyzer_agent(state: AgentState) -> AgentState:\n",
        "    \"\"\"Lineage Analyzer agent that focuses on data dependencies.\"\"\"\n",
        "    question = state[\"question\"]\n",
        "    \n",
        "    # Extract table names and analyze lineage\n",
        "    table_names = []\n",
        "    for word in question.split():\n",
        "        if '.' in word and any(schema in word for schema in ['raw.', 'curated.', 'analytics.']):\n",
        "            table_names.append(word)\n",
        "    \n",
        "    lineage_analysis = []\n",
        "    for table_name in table_names:\n",
        "        lineage_analysis.append(lineage_query_tool(table_name))\n",
        "    \n",
        "    # Generate lineage-focused analysis\n",
        "    lineage_prompt = f\"\"\"\n",
        "    You are the Lineage Analyzer Agent for Traceback.\n",
        "    \n",
        "    Question: {question}\n",
        "    \n",
        "    Lineage Analysis:\n",
        "    {json.dumps(lineage_analysis, indent=2)}\n",
        "    \n",
        "    Provide detailed lineage analysis:\n",
        "    1. Direct Dependencies\n",
        "    2. Indirect Dependencies (2+ hops)\n",
        "    3. Affected Dashboards/Reports\n",
        "    4. Data Flow Impact\n",
        "    5. Recovery Dependencies\n",
        "    \n",
        "    Format as structured analysis.\n",
        "    \"\"\"\n",
        "    \n",
        "    try:\n",
        "        response = llm.invoke([{\"role\": \"user\", \"content\": lineage_prompt}])\n",
        "        \n",
        "        state[\"impact_assessment\"] = {\n",
        "            \"lineage_analysis\": response.content,\n",
        "            \"lineage_data\": lineage_analysis\n",
        "        }\n",
        "        \n",
        "        # Extract blast radius from lineage\n",
        "        blast_radius = []\n",
        "        for result in lineage_analysis:\n",
        "            if result.startswith('{'):\n",
        "                data = json.loads(result)\n",
        "                blast_radius.extend(data.get(\"downstream_impact\", []))\n",
        "        \n",
        "        state[\"blast_radius\"] = list(set(blast_radius))\n",
        "        state[\"current_step\"] = \"writer\"\n",
        "        \n",
        "    except Exception as e:\n",
        "        state[\"error\"] = f\"Lineage analyzer error: {str(e)}\"\n",
        "        state[\"current_step\"] = \"writer\"\n",
        "    \n",
        "    return state\n",
        "\n",
        "def writer_agent(state: AgentState) -> AgentState:\n",
        "    \"\"\"Writer agent that generates the final incident brief.\"\"\"\n",
        "    question = state[\"question\"]\n",
        "    impact_assessment = state.get(\"impact_assessment\", {})\n",
        "    blast_radius = state.get(\"blast_radius\", [])\n",
        "    \n",
        "    # Gather additional context if needed\n",
        "    rag_results = rag_search_tool(question)\n",
        "    \n",
        "    # Generate incident brief\n",
        "    writer_prompt = f\"\"\"\n",
        "    You are the Writer Agent for Traceback incident triage.\n",
        "    \n",
        "    Question: {question}\n",
        "    \n",
        "    Impact Assessment:\n",
        "    {json.dumps(impact_assessment, indent=2)}\n",
        "    \n",
        "    Blast Radius:\n",
        "    {blast_radius}\n",
        "    \n",
        "    Additional Context:\n",
        "    {rag_results}\n",
        "    \n",
        "    Generate a comprehensive incident brief with:\n",
        "    1. **Incident Summary**: Brief description\n",
        "    2. **Business Impact**: Level and details\n",
        "    3. **Blast Radius**: Affected systems/tables\n",
        "    4. **Root Cause Analysis**: Likely causes\n",
        "    5. **Recommended Actions**: Immediate steps\n",
        "    6. **Recovery Plan**: Step-by-step recovery\n",
        "    7. **Prevention**: Future mitigation\n",
        "    \n",
        "    Format as a professional incident brief.\n",
        "    \"\"\"\n",
        "    \n",
        "    try:\n",
        "        response = llm.invoke([{\"role\": \"user\", \"content\": writer_prompt}])\n",
        "        \n",
        "        state[\"incident_brief\"] = response.content\n",
        "        state[\"current_step\"] = \"complete\"\n",
        "        \n",
        "    except Exception as e:\n",
        "        state[\"error\"] = f\"Writer error: {str(e)}\"\n",
        "        state[\"incident_brief\"] = f\"Error generating incident brief: {str(e)}\"\n",
        "        state[\"current_step\"] = \"complete\"\n",
        "    \n",
        "    return state\n",
        "\n",
        "print(\"‚úÖ Specialized agents implemented\")\n",
        "print(f\"  üéØ Supervisor Agent: Workflow orchestration\")\n",
        "print(f\"  üìä Impact Assessor: Business impact analysis\")\n",
        "print(f\"  üß¨ Lineage Analyzer: Data dependency analysis\")\n",
        "print(f\"  ‚úçÔ∏è Writer Agent: Incident brief generation\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ LangGraph workflow created\n",
            "  üîÑ Workflow: supervisor ‚Üí impact_assessor ‚Üí writer ‚Üí END\n",
            "  üéØ Entry point: supervisor\n",
            "  üèÅ Exit point: writer\n",
            "  ü§ñ Graph compiled and ready\n"
          ]
        }
      ],
      "source": [
        "# Phase 3.3: Build LangGraph Workflow\n",
        "\n",
        "def route_next_step(state: AgentState) -> str:\n",
        "    \"\"\"Route to the next agent based on current step.\"\"\"\n",
        "    current_step = state.get(\"current_step\", \"supervisor\")\n",
        "    \n",
        "    if current_step == \"supervisor\":\n",
        "        return \"impact_assessor\"  # Default routing\n",
        "    elif current_step == \"impact_assessor\":\n",
        "        return \"writer\"\n",
        "    elif current_step == \"lineage_analyzer\":\n",
        "        return \"writer\"\n",
        "    elif current_step == \"writer\":\n",
        "        return END\n",
        "    else:\n",
        "        return \"writer\"  # Fallback\n",
        "\n",
        "# Create the LangGraph workflow\n",
        "workflow = StateGraph(AgentState)\n",
        "\n",
        "# Add nodes for each agent\n",
        "workflow.add_node(\"supervisor\", supervisor_agent)\n",
        "workflow.add_node(\"impact_assessor\", impact_assessor_agent)\n",
        "workflow.add_node(\"lineage_analyzer\", lineage_analyzer_agent)\n",
        "workflow.add_node(\"writer\", writer_agent)\n",
        "\n",
        "# Define the workflow edges\n",
        "workflow.add_edge(\"supervisor\", \"impact_assessor\")\n",
        "workflow.add_edge(\"impact_assessor\", \"writer\")\n",
        "workflow.add_edge(\"lineage_analyzer\", \"writer\")\n",
        "workflow.add_edge(\"writer\", END)\n",
        "\n",
        "# Set entry point\n",
        "workflow.set_entry_point(\"supervisor\")\n",
        "\n",
        "# Compile the graph\n",
        "traceback_graph = workflow.compile()\n",
        "\n",
        "print(\"‚úÖ LangGraph workflow created\")\n",
        "print(f\"  üîÑ Workflow: supervisor ‚Üí impact_assessor ‚Üí writer ‚Üí END\")\n",
        "print(f\"  üéØ Entry point: supervisor\")\n",
        "print(f\"  üèÅ Exit point: writer\")\n",
        "print(f\"  ü§ñ Graph compiled and ready\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üß™ Testing multi-agent incident triage system...\n",
            "======================================================================\n",
            "\n",
            "üîç Test 1: Job curated.sales_orders failed ‚Äî who's impacted?\n",
            "--------------------------------------------------\n",
            "üö® Starting Traceback incident triage...\n",
            "üìã Question: Job curated.sales_orders failed ‚Äî who's impacted?\n",
            "============================================================\n",
            "‚úÖ Incident triage completed!\n",
            "üìä Final state: complete\n",
            "\n",
            "üìã Incident Brief:\n",
            "# Incident Brief: Curated Sales Orders Pipeline Failure\n",
            "\n",
            "## 1. Incident Summary\n",
            "On [insert date and time], the job responsible for curating sales orders (`curated.sales_orders`) failed, resulting in a disruption of the sales orders pipeline. This incident has impacted downstream analytics and reporting processes, leading to potential delays in business operations.\n",
            "\n",
            "## 2. Business Impact\n",
            "- **Impact Level**: High\n",
            "- **Details**: The failure of the sales orders pipeline has resulted in unmet freshness SLAs of 2 hours. This impacts the availability of critical sales data for reporting and analytics, affecting decision-making processes across the organization.\n",
            "\n",
            "## 3. Blast Radius\n",
            "The following systems and tables are affected:\n",
            "- **Affected Systems/Tables**:\n",
            "  - `curated.sales_orders`\n",
            "  - `curated.revenue_summary`\n",
            "  - `customer_analytics_pipeline`\n",
            "  \n",
            "- **Downstream Impact**:\n",
            "  - `curated.revenue_summary`\n",
            "  - `analytics.customer_behavior`\n",
            "\n",
            "## 4. Root Cause Analysis\n",
            "The likely causes of the failure include:\n",
            "- Issues with upstream data sources (`raw.sales_orders`, `raw.customers`, `raw.products`, `raw.refunds`) that may have led to incomplete or invalid data being processed.\n",
            "- Potential errors in the SQL transformation logic within the sales orders pipeline that could have caused the job to fail.\n",
            "- Resource constraints or system outages affecting the execution of the pipeline.\n",
            "\n",
            "## 5. Recommended Actions\n",
            "Immediate steps to address the incident:\n",
            "1. **Investigate Logs**: Review the logs of the `curated.sales_orders` job to identify specific error messages or failure points.\n",
            "2. **Check Upstream Data**: Validate the integrity and availability of upstream data sources to ensure they are providing the necessary data.\n",
            "3. **Notify Stakeholders**: Inform relevant stakeholders, including the data-sales and data-analytics teams, about the incident and its potential impact.\n",
            "\n",
            "## 6. Recovery Plan\n",
            "Step-by-step recovery process:\n",
            "1. **Identify and Fix Errors**: Based on log analysis, correct any identified errors in the SQL transformation logic.\n",
            "2. **Validate Upstream Data**: Ensure that all upstream data sources are functioning correctly and providing valid data.\n",
            "3. **Re-run the Pipeline**: Execute the `curated.sales_orders` job manually to process the data and restore the pipeline.\n",
            "4. **Monitor Recovery**: After re-running the job, monitor the pipeline for successful completion and validate the output data.\n",
            "5. **Update Stakeholders**: Communicate the resolution and confirm that the pipeline is back to normal operation.\n",
            "\n",
            "## 7. Prevention\n",
            "To mitigate future occurrences:\n",
            "- **Implement Monitoring**: Set up automated monitoring and alerting for the sales orders pipeline to detect failures promptly.\n",
            "- **Data Quality Checks**: Establish data quality checks on upstream data sources to ensure that only valid data is processed.\n",
            "- **Documentation and Training**: Ensure that documentation for the sales orders pipeline is up-to-date and provide training for team members on handling similar incidents.\n",
            "\n",
            "By following this incident brief, we aim to restore normal operations swiftly while minimizing the impact on business processes and ensuring future resilience.\n",
            "\n",
            "üí• Blast Radius:\n",
            "  ‚Ä¢ analytics.customer_behavior\n",
            "  ‚Ä¢ curated.revenue_summary\n",
            "\n",
            "======================================================================\n",
            "\n",
            "üîç Test 2: What should I do if raw.sales_orders has quality issues?\n",
            "--------------------------------------------------\n",
            "üö® Starting Traceback incident triage...\n",
            "üìã Question: What should I do if raw.sales_orders has quality issues?\n",
            "============================================================\n",
            "‚úÖ Incident triage completed!\n",
            "üìä Final state: complete\n",
            "\n",
            "üìã Incident Brief:\n",
            "# Incident Brief: Quality Issues in raw.sales_orders\n",
            "\n",
            "## 1. Incident Summary\n",
            "On [insert date], quality issues were identified in the `raw.sales_orders` table, which is critical for the sales orders pipeline. These issues have the potential to compromise the integrity of downstream datasets, affecting analytics and reporting capabilities.\n",
            "\n",
            "## 2. Business Impact\n",
            "- **Impact Level**: High\n",
            "- **Details**: The quality issues in `raw.sales_orders` will directly impact the following systems:\n",
            "  - `curated.sales_orders`\n",
            "  - `curated.revenue_summary`\n",
            "  - `analytics.customer_behavior`\n",
            "  \n",
            "  The freshness SLA of 2 hours for the sales orders pipeline may be compromised, leading to delays in data availability for analytics and reporting. This could hinder decision-making processes reliant on accurate sales data.\n",
            "\n",
            "## 3. Blast Radius\n",
            "The following systems/tables are affected due to the quality issues in `raw.sales_orders`:\n",
            "- `curated.sales_orders`\n",
            "- `curated.revenue_summary`\n",
            "- `analytics.customer_behavior`\n",
            "\n",
            "## 4. Root Cause Analysis\n",
            "The likely causes of the quality issues in `raw.sales_orders` include:\n",
            "- Invalid records due to negative order amounts or missing customer/product IDs.\n",
            "- Data entry errors from the e-commerce platform.\n",
            "- Inadequate data validation checks in the sales orders pipeline.\n",
            "- Potential issues with upstream data sources, such as `raw.customers` or `raw.products`.\n",
            "\n",
            "## 5. Recommended Actions\n",
            "Immediate steps to address the quality issues include:\n",
            "1. **Notify Stakeholders**: Inform the data-sales team and relevant stakeholders about the identified quality issues.\n",
            "2. **Stop the Pipeline**: Temporarily halt the sales orders pipeline to prevent further processing of invalid data.\n",
            "3. **Conduct a Data Quality Assessment**: Analyze the extent of the quality issues in `raw.sales_orders` to identify specific records that are problematic.\n",
            "\n",
            "## 6. Recovery Plan\n",
            "Step-by-step recovery process:\n",
            "1. **Data Quality Assessment**: Review the records in `raw.sales_orders` to identify and document the specific quality issues.\n",
            "2. **Data Cleansing**: \n",
            "   - Remove invalid records (e.g., negative order amounts, missing customer/product IDs).\n",
            "   - Ensure that timestamps are valid and recent.\n",
            "3. **Reprocess Data**: Once the data is cleansed, reprocess the `raw.sales_orders` through the sales orders pipeline.\n",
            "4. **Validation**: Apply business rules to validate the cleansed data before inserting it into `curated.sales_orders`.\n",
            "5. **Monitor**: After reprocessing, closely monitor the downstream systems for any anomalies or further issues.\n",
            "\n",
            "Estimated recovery time is approximately 4-6 hours, depending on the extent of the quality issues and the time required for data cleansing and validation.\n",
            "\n",
            "## 7. Prevention\n",
            "To mitigate future occurrences of similar quality issues, the following measures are recommended:\n",
            "- **Enhance Data Validation**: Implement stricter data validation checks in the sales orders pipeline to catch invalid records before they are processed.\n",
            "- **Regular Audits**: Schedule regular audits of the `raw.sales_orders` data to identify and rectify quality issues proactively.\n",
            "- **Training**: Provide training for data entry personnel on the importance of data quality and the impact of errors on downstream analytics.\n",
            "- **Monitoring Tools**: Invest in monitoring tools that can alert the data team to potential quality issues in real-time.\n",
            "\n",
            "By implementing these recommendations, we can improve the overall quality of the sales orders data and reduce the risk of future incidents.\n",
            "\n",
            "üí• Blast Radius:\n",
            "  ‚Ä¢ curated.sales_orders\n",
            "  ‚Ä¢ analytics.customer_behavior\n",
            "  ‚Ä¢ curated.revenue_summary\n",
            "\n",
            "======================================================================\n",
            "\n",
            "üîç Test 3: Which dashboards will be affected if curated.revenue_summary fails?\n",
            "--------------------------------------------------\n",
            "üö® Starting Traceback incident triage...\n",
            "üìã Question: Which dashboards will be affected if curated.revenue_summary fails?\n",
            "============================================================\n",
            "‚úÖ Incident triage completed!\n",
            "üìä Final state: complete\n",
            "\n",
            "üìã Incident Brief:\n",
            "# Incident Brief: Failure of Curated Revenue Summary\n",
            "\n",
            "## 1. Incident Summary\n",
            "On [insert date and time], the `curated.revenue_summary` pipeline encountered a failure, resulting in the inability to generate daily revenue summaries. This incident has significant implications for downstream reporting and analytics systems that rely on this data.\n",
            "\n",
            "## 2. Business Impact\n",
            "- **Impact Level**: High\n",
            "- **Affected Systems/Tables**:\n",
            "  - `curated.revenue_summary`\n",
            "  - `bi.daily_sales`\n",
            "  - `analytics.customer_behavior`\n",
            "- **Details**: The failure of the `curated.revenue_summary` affects the accuracy and availability of revenue reporting and customer analytics, which are critical for business decision-making. The executive dashboard (`bi.daily_sales`) and customer behavior analytics (`analytics.customer_behavior`) will not reflect the latest data, potentially leading to misguided strategies and decisions.\n",
            "\n",
            "## 3. Blast Radius\n",
            "- **Affected Dashboards**:\n",
            "  - `bi.daily_sales`: Executive dashboard that relies on revenue data for performance metrics.\n",
            "  - `analytics.customer_behavior`: Dashboard providing insights into customer interactions and behaviors.\n",
            "\n",
            "## 4. Root Cause Analysis\n",
            "The likely causes of the failure include:\n",
            "- Issues with upstream data sources, particularly `curated.sales_orders`, which is essential for generating the revenue summary.\n",
            "- Potential SQL errors or data integrity issues within the `revenue_summary_pipeline.sql` script.\n",
            "- System resource constraints or failures in the data processing environment that may have interrupted the pipeline execution.\n",
            "\n",
            "## 5. Recommended Actions\n",
            "- **Immediate Steps**:\n",
            "  - Notify the data-sales team and relevant stakeholders about the incident.\n",
            "  - Review the logs and error messages from the `revenue_summary_pipeline` to identify the specific failure point.\n",
            "  - Check the status of upstream data sources (`curated.sales_orders`) for any discrepancies or outages.\n",
            "\n",
            "## 6. Recovery Plan\n",
            "1. **Investigate the Failure**: Analyze the logs and error messages to pinpoint the cause of the failure in the `revenue_summary_pipeline`.\n",
            "2. **Validate Upstream Data**: Ensure that the `curated.sales_orders` table is populated correctly and that there are no data integrity issues.\n",
            "3. **Fix the Issue**: Address any identified issues in the SQL script or data sources.\n",
            "4. **Re-run the Pipeline**: Execute the `revenue_summary_pipeline` to regenerate the `curated.revenue_summary`.\n",
            "5. **Monitor Downstream Systems**: After recovery, monitor the `bi.daily_sales` and `analytics.customer_behavior` dashboards for accuracy and completeness of data.\n",
            "6. **Communicate Resolution**: Inform stakeholders once the issue is resolved and data is back to normal.\n",
            "\n",
            "## 7. Prevention\n",
            "To mitigate future occurrences of this incident:\n",
            "- Implement automated monitoring and alerting for the `curated.revenue_summary` pipeline to detect failures promptly.\n",
            "- Conduct regular audits of upstream data sources to ensure data integrity and availability.\n",
            "- Enhance error handling within the SQL scripts to provide clearer insights into failure points.\n",
            "- Schedule periodic reviews of the pipeline and its dependencies to identify potential risks and address them proactively.\n",
            "\n",
            "---\n",
            "\n",
            "This incident brief serves as a comprehensive overview of the failure of the `curated.revenue_summary` pipeline, its impacts, and the steps necessary for recovery and prevention. Further updates will be provided as the situation evolves.\n",
            "\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "# Phase 3.4: Test Multi-Agent System\n",
        "\n",
        "def run_traceback_incident_triage(question: str) -> Dict[str, Any]:\n",
        "    \"\"\"Run the complete Traceback incident triage workflow.\"\"\"\n",
        "    print(f\"üö® Starting Traceback incident triage...\")\n",
        "    print(f\"üìã Question: {question}\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    # Initialize state\n",
        "    initial_state = AgentState(\n",
        "        question=question,\n",
        "        context=[],\n",
        "        impact_assessment=None,\n",
        "        blast_radius=None,\n",
        "        recommended_actions=None,\n",
        "        incident_brief=None,\n",
        "        current_step=\"supervisor\",\n",
        "        error=None\n",
        "    )\n",
        "    \n",
        "    try:\n",
        "        # Run the workflow\n",
        "        result = traceback_graph.invoke(initial_state)\n",
        "        \n",
        "        print(f\"‚úÖ Incident triage completed!\")\n",
        "        print(f\"üìä Final state: {result['current_step']}\")\n",
        "        \n",
        "        if result.get(\"error\"):\n",
        "            print(f\"‚ö†Ô∏è Error occurred: {result['error']}\")\n",
        "        \n",
        "        return result\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Workflow error: {str(e)}\")\n",
        "        return {\"error\": str(e), \"question\": question}\n",
        "\n",
        "# Test the multi-agent system\n",
        "test_incidents = [\n",
        "    \"Job curated.sales_orders failed ‚Äî who's impacted?\",\n",
        "    \"What should I do if raw.sales_orders has quality issues?\",\n",
        "    \"Which dashboards will be affected if curated.revenue_summary fails?\"\n",
        "]\n",
        "\n",
        "print(\"üß™ Testing multi-agent incident triage system...\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "for i, incident in enumerate(test_incidents, 1):\n",
        "    print(f\"\\nüîç Test {i}: {incident}\")\n",
        "    print(\"-\" * 50)\n",
        "    \n",
        "    result = run_traceback_incident_triage(incident)\n",
        "    \n",
        "    if result.get(\"incident_brief\"):\n",
        "        print(f\"\\nüìã Incident Brief:\")\n",
        "        print(f\"{result['incident_brief']}\")\n",
        "        \n",
        "        if result.get(\"blast_radius\"):\n",
        "            print(f\"\\nüí• Blast Radius:\")\n",
        "            for item in result[\"blast_radius\"][:5]:  # Show top 5\n",
        "                print(f\"  ‚Ä¢ {item}\")\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*70)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Agent system components ready\n",
            "  ü§ñ Supervisor Agent: Workflow orchestration\n",
            "  üìä Impact Assessor: Business impact analysis\n",
            "  üß¨ Lineage Analyzer: Data dependency analysis\n",
            "  ‚úçÔ∏è Writer Agent: Incident brief generation\n",
            "  üîÑ LangGraph Workflow: Multi-agent orchestration\n",
            "\n",
            "üéâ Phase 3 Complete: Agent System Ready!\n",
            "   ‚úÖ Multi-agent architecture implemented\n",
            "   ‚úÖ LangGraph workflow operational\n",
            "   ‚úÖ Specialized agents for incident triage\n",
            "   ‚úÖ Tool integration (RAG + Lineage + Web)\n",
            "   ‚úÖ End-to-end incident triage workflow\n",
            "   ‚úÖ Ready for Phase 4: API & Interface\n",
            "\n",
            "üöÄ Complete End-to-End Agentic RAG System:\n",
            "   üìä Phase 1: Data Foundation ‚úÖ\n",
            "   üß† Phase 2: Core RAG System ‚úÖ\n",
            "   ü§ñ Phase 3: Agent System ‚úÖ\n",
            "   üåê Phase 4: API & Interface (Next)\n"
          ]
        }
      ],
      "source": [
        "# Phase 3.5: Save Agent System and Summary\n",
        "\n",
        "# Save agent system components\n",
        "agent_system = {\n",
        "    \"workflow\": traceback_graph,\n",
        "    \"tools\": tools,\n",
        "    \"agents\": {\n",
        "        \"supervisor\": supervisor_agent,\n",
        "        \"impact_assessor\": impact_assessor_agent,\n",
        "        \"lineage_analyzer\": lineage_analyzer_agent,\n",
        "        \"writer\": writer_agent\n",
        "    }\n",
        "}\n",
        "\n",
        "print(\"‚úÖ Agent system components ready\")\n",
        "print(f\"  ü§ñ Supervisor Agent: Workflow orchestration\")\n",
        "print(f\"  üìä Impact Assessor: Business impact analysis\")\n",
        "print(f\"  üß¨ Lineage Analyzer: Data dependency analysis\")\n",
        "print(f\"  ‚úçÔ∏è Writer Agent: Incident brief generation\")\n",
        "print(f\"  üîÑ LangGraph Workflow: Multi-agent orchestration\")\n",
        "\n",
        "print(f\"\\nüéâ Phase 3 Complete: Agent System Ready!\")\n",
        "print(f\"   ‚úÖ Multi-agent architecture implemented\")\n",
        "print(f\"   ‚úÖ LangGraph workflow operational\")\n",
        "print(f\"   ‚úÖ Specialized agents for incident triage\")\n",
        "print(f\"   ‚úÖ Tool integration (RAG + Lineage + Web)\")\n",
        "print(f\"   ‚úÖ End-to-end incident triage workflow\")\n",
        "print(f\"   ‚úÖ Ready for Phase 4: API & Interface\")\n",
        "\n",
        "print(f\"\\nüöÄ Complete End-to-End Agentic RAG System:\")\n",
        "print(f\"   üìä Phase 1: Data Foundation ‚úÖ\")\n",
        "print(f\"   üß† Phase 2: Core RAG System ‚úÖ\")\n",
        "print(f\"   ü§ñ Phase 3: Agent System ‚úÖ\")\n",
        "print(f\"   üåê Phase 4: API & Interface (Next)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
