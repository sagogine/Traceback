{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Task 5: RAGAS Evaluation - Comprehensive Golden Test Data Set\n",
        "\n",
        "## ✅ Updated for 15-Spec System\n",
        "This notebook has been updated to evaluate the comprehensive 15-spec Traceback system covering all business domains and operational guides.\n",
        "\n",
        "## Objective\n",
        "Assess the Traceback pipeline using the RAGAS framework with key metrics:\n",
        "- **Faithfulness**: How factually accurate are the generated responses?\n",
        "- **Response Relevance**: How relevant are the responses to the questions?\n",
        "- **Context Precision**: How precise is the retrieved context?\n",
        "- **Context Recall**: How well does the context cover the ground truth?\n",
        "\n",
        "## Methodology\n",
        "1. Create a comprehensive golden test dataset covering all 15 business domains\n",
        "2. Generate responses using our enhanced Traceback system\n",
        "3. Evaluate using RAGAS metrics\n",
        "4. Analyze results and draw conclusions about pipeline effectiveness\n",
        "\n",
        "## Test Coverage\n",
        "- **10 Business Domains**: Sales Orders, Customer Analytics, Inventory Management, Financial Reporting, Marketing Attribution, Supply Chain, HR Analytics, Product Analytics, Risk Management, Compliance Monitoring\n",
        "- **5 Operational Guides**: Incident Playbook, Data Quality Standards, Troubleshooting Guide, SLA Definitions, Escalation Procedures\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Environment setup complete\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from typing import List, Dict, Any\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv()\n",
        "\n",
        "# Verify API keys\n",
        "if not os.getenv(\"OPENAI_API_KEY\"):\n",
        "    raise RuntimeError(\"OPENAI_API_KEY is not set. Create a .env file or export it in your shell.\")\n",
        "\n",
        "print(\"✅ Environment setup complete\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Imports complete\n"
          ]
        }
      ],
      "source": [
        "# Add src to path for imports\n",
        "sys.path.insert(0, str(Path.cwd().parent / \"src\"))\n",
        "\n",
        "# Import RAGAS components\n",
        "from ragas import evaluate\n",
        "from ragas.metrics import (\n",
        "    faithfulness,\n",
        "    answer_relevancy,\n",
        "    context_precision,\n",
        "    context_recall\n",
        ")\n",
        "from datasets import Dataset\n",
        "\n",
        "# Import our Traceback system\n",
        "from tracebackcore.core import traceback_graph, lineage_retriever, AgentState, initialize_system\n",
        "\n",
        "print(\"✅ Imports complete\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Initialize Traceback System\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🚀 Initializing Traceback system...\n",
            "🚀 Initializing Traceback system...\n",
            "✅ Traceback system initialized successfully\n",
            "✅ Traceback system initialized successfully\n"
          ]
        }
      ],
      "source": [
        "# Initialize the Traceback system\n",
        "print(\"🚀 Initializing Traceback system...\")\n",
        "initialize_system()\n",
        "print(\"✅ Traceback system initialized successfully\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Create Golden Test Dataset\n",
        "\n",
        "We'll create a comprehensive test dataset covering various incident scenarios with ground truth answers.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Updated golden test dataset with 15 comprehensive test cases\n",
            "📊 Coverage: 10 business domains + 5 operational guides\n",
            "🎯 RAGAS Improvement: Enhanced test coverage for better evaluation\n",
            "🚀 Ready for comprehensive RAGAS evaluation!\n"
          ]
        }
      ],
      "source": [
        "# Updated: Comprehensive Golden Test Dataset (15 Business Domains)\n",
        "\n",
        "# Replace the old test data with comprehensive coverage\n",
        "golden_test_data = [\n",
        "    # Sales Orders Domain\n",
        "    {\n",
        "        \"question\": \"What should I do if the sales orders pipeline fails?\",\n",
        "        \"ground_truth\": \"If the sales orders pipeline fails, follow these steps: 1) Assess business impact and determine blast radius affecting curated.sales_orders, curated.revenue_summary, and analytics.customer_behavior, 2) Check pipeline logs for error messages, 3) Verify data source availability (raw.sales_orders, raw.customers, raw.products), 4) Validate data quality metrics, 5) Test individual pipeline components, 6) Review recent changes or deployments. The pipeline has 99.9% uptime SLA and 2-hour freshness requirement. Escalate to data-sales team for P0/P1 incidents.\",\n",
        "        \"context\": [\n",
        "            \"Sales orders pipeline processes raw order data into curated datasets for analytics and reporting.\",\n",
        "            \"SLA commitments: 99.9% uptime, 2-hour freshness, <0.1% error rate\",\n",
        "            \"Downstream dependencies: curated.revenue_summary, bi.daily_sales, analytics.customer_behavior\",\n",
        "            \"Common failure patterns: data quality issues, dependency failures, performance degradation\"\n",
        "        ]\n",
        "    },\n",
        "    \n",
        "    # Customer Analytics Domain\n",
        "    {\n",
        "        \"question\": \"How does the customer analytics pipeline segment customers?\",\n",
        "        \"ground_truth\": \"The customer analytics pipeline segments customers based on lifetime value: VIP (>$10,000), High Value ($5,000-$10,000), Medium Value ($1,000-$5,000), and Low Value (<$1,000). It also calculates behavioral metrics including engagement scores based on interaction frequency, churn risk using ML models, and purchase propensity for next 30-day purchases. The pipeline processes curated.sales_orders, raw.customer_interactions, raw.marketing_campaigns, and raw.support_tickets with 99.5% uptime SLA and daily updates by 6 AM.\",\n",
        "        \"context\": [\n",
        "            \"Customer analytics pipeline for segmentation, lifetime value calculation, and behavioral analysis\",\n",
        "            \"Customer segmentation: VIP, High Value, Medium Value, Low Value based on lifetime value\",\n",
        "            \"Behavioral metrics: engagement score, churn risk, purchase propensity\",\n",
        "            \"Data sources: curated.sales_orders, raw.customer_interactions, raw.marketing_campaigns, raw.support_tickets\"\n",
        "        ]\n",
        "    },\n",
        "    \n",
        "    # Inventory Management Domain\n",
        "    {\n",
        "        \"question\": \"What are the stock level management rules for inventory?\",\n",
        "        \"ground_truth\": \"Inventory stock levels are managed with these rules: Critical (<10 units remaining), Low (10-50 units remaining), Normal (50-200 units remaining), and High (>200 units remaining). The reorder logic includes auto-reorder when stock falls below reorder point, manual approval for high-value items, and seasonal adjustments based on historical patterns. The system uses real-time updates (<5 minutes) with 99.9% uptime SLA and <0.01% error rate, processing raw.inventory_transactions, raw.warehouse_locations, raw.supplier_data, and raw.demand_forecasts.\",\n",
        "        \"context\": [\n",
        "            \"Real-time inventory tracking and management for warehouse operations and demand forecasting\",\n",
        "            \"Stock level management: Critical, Low, Normal, High based on remaining units\",\n",
        "            \"Reorder logic: auto-reorder, manual approval, seasonal adjustments\",\n",
        "            \"Real-time updates with 99.9% uptime SLA and <0.01% error rate\"\n",
        "        ]\n",
        "    },\n",
        "    \n",
        "    # Financial Reporting Domain\n",
        "    {\n",
        "        \"question\": \"What financial controls are implemented in the reporting pipeline?\",\n",
        "        \"ground_truth\": \"The financial reporting pipeline implements comprehensive financial controls including daily bank reconciliation, month-end accrual processing, asset depreciation calculations, and automated tax computations. It ensures SOX compliance with segregation of duties, follows GAAP standards, and maintains complete audit trails for all transactions. The pipeline processes raw.general_ledger, raw.accounts_payable, raw.accounts_receivable, and raw.budget_data with 99.95% uptime SLA, daily processing by 8 AM, and <0.001% error rate for financial precision.\",\n",
        "        \"context\": [\n",
        "            \"Comprehensive financial data processing for regulatory compliance and management reporting\",\n",
        "            \"Financial controls: reconciliation, accruals, depreciation, tax calculations\",\n",
        "            \"Compliance requirements: SOX compliance, GAAP standards, audit trails\",\n",
        "            \"High precision requirements: 99.95% uptime, <0.001% error rate\"\n",
        "        ]\n",
        "    },\n",
        "    \n",
        "    # Marketing Attribution Domain\n",
        "    {\n",
        "        \"question\": \"What attribution models are used for marketing campaigns?\",\n",
        "        \"ground_truth\": \"The marketing attribution pipeline uses multiple attribution models: First Touch (credit to first interaction), Last Touch (credit to final interaction), Linear (equal credit to all touchpoints), and Time Decay (more credit to recent interactions). It calculates ROI metrics including Campaign ROI (Revenue/Campaign Cost), Channel ROI (Revenue/Channel Investment), and Customer LTV (Lifetime Value). The pipeline processes raw.marketing_touchpoints, raw.campaign_performance, raw.conversion_events, and raw.customer_journey with 99.0% uptime SLA and weekly updates by Monday 9 AM.\",\n",
        "        \"context\": [\n",
        "            \"Multi-touch attribution modeling for marketing campaign effectiveness and ROI analysis\",\n",
        "            \"Attribution models: First Touch, Last Touch, Linear, Time Decay\",\n",
        "            \"ROI calculations: Campaign ROI, Channel ROI, Customer LTV\",\n",
        "            \"Weekly processing with 99.0% uptime SLA\"\n",
        "        ]\n",
        "    },\n",
        "    \n",
        "    # Supply Chain Domain\n",
        "    {\n",
        "        \"question\": \"How is supplier performance measured in the supply chain?\",\n",
        "        \"ground_truth\": \"Supplier performance is measured using key metrics: on-time delivery (>95% target), quality score (>98% target), cost efficiency with budget variance tracking, and risk assessment for supplier stability. The system also optimizes logistics with route optimization for cost and time minimization, strategic inventory positioning, and ML-based demand forecasting. The pipeline processes raw.supplier_performance, raw.logistics_data, raw.procurement_data, and raw.quality_metrics with 99.5% uptime SLA and daily updates by 7 AM.\",\n",
        "        \"context\": [\n",
        "            \"End-to-end supply chain visibility and optimization for cost reduction and efficiency\",\n",
        "            \"Supplier performance metrics: on-time delivery, quality score, cost efficiency, risk assessment\",\n",
        "            \"Logistics optimization: route optimization, inventory positioning, demand forecasting\",\n",
        "            \"Daily processing with 99.5% uptime SLA\"\n",
        "        ]\n",
        "    },\n",
        "    \n",
        "    # HR Analytics Domain\n",
        "    {\n",
        "        \"question\": \"What employee metrics are tracked in the HR analytics pipeline?\",\n",
        "        \"ground_truth\": \"The HR analytics pipeline tracks comprehensive employee metrics including retention rate (annual turnover calculations), performance scores (quarterly evaluations), engagement metrics (survey-based indicators), and career progression (promotion and growth tracking). It also includes predictive analytics with ML-based churn prediction for retention modeling, performance forecasting for future performance prediction, and skill gap analysis for training needs identification. The pipeline processes raw.employee_data, raw.performance_reviews, raw.attendance_data, and raw.learning_records with 99.0% uptime SLA and monthly updates by 5th of month.\",\n",
        "        \"context\": [\n",
        "            \"Employee lifecycle analytics for talent management, retention, and performance optimization\",\n",
        "            \"Employee metrics: retention rate, performance scores, engagement metrics, career progression\",\n",
        "            \"Predictive analytics: churn prediction, performance forecasting, skill gap analysis\",\n",
        "            \"Monthly processing with 99.0% uptime SLA\"\n",
        "        ]\n",
        "    },\n",
        "    \n",
        "    # Product Analytics Domain\n",
        "    {\n",
        "        \"question\": \"What product usage metrics are monitored in real-time?\",\n",
        "        \"ground_truth\": \"The product analytics pipeline monitors real-time usage metrics including DAU/MAU (Daily and Monthly Active Users), feature adoption rates for new features, session analytics for user journey analysis, and conversion funnels for step-by-step conversion tracking. It also tracks product KPIs including engagement scores (user activity level), retention rates (user return behavior), feature stickiness (feature retention metrics), and NPS tracking (Net Promoter Score monitoring). The pipeline processes raw.user_interactions, raw.feature_usage, raw.user_feedback, and raw.performance_metrics with 99.5% uptime SLA and real-time updates (<1 minute).\",\n",
        "        \"context\": [\n",
        "            \"Comprehensive product usage analytics for feature optimization and user experience\",\n",
        "            \"Usage metrics: DAU/MAU, feature adoption, session analytics, conversion funnels\",\n",
        "            \"Product KPIs: engagement score, retention rate, feature stickiness, NPS tracking\",\n",
        "            \"Real-time processing with <1 minute updates\"\n",
        "        ]\n",
        "    },\n",
        "    \n",
        "    # Risk Management Domain\n",
        "    {\n",
        "        \"question\": \"How is risk scoring calculated in the risk management system?\",\n",
        "        \"ground_truth\": \"Risk scoring is calculated across multiple dimensions: credit risk (customer creditworthiness), operational risk (process failure probability), market risk (external market volatility), and compliance risk (regulatory violation probability). The system provides real-time monitoring with threshold-based notifications, executive reporting dashboards, historical risk pattern analysis, and risk reduction measure tracking. The pipeline processes raw.transaction_data, raw.customer_data, raw.market_data, and raw.compliance_data with 99.95% uptime SLA and real-time updates (<30 seconds) for critical compliance requirements.\",\n",
        "        \"context\": [\n",
        "            \"Comprehensive risk assessment and monitoring for operational, financial, and compliance risks\",\n",
        "            \"Risk scoring: credit risk, operational risk, market risk, compliance risk\",\n",
        "            \"Risk monitoring: real-time alerts, dashboards, trend analysis, mitigation tracking\",\n",
        "            \"Critical compliance requirements with <30 second updates\"\n",
        "        ]\n",
        "    },\n",
        "    \n",
        "    # Compliance Monitoring Domain\n",
        "    {\n",
        "        \"question\": \"What regulatory compliance standards are monitored?\",\n",
        "        \"ground_truth\": \"The compliance monitoring system monitors multiple regulatory standards including GDPR (Data privacy and protection), SOX (Financial controls and reporting), PCI DSS (Payment card data security), and HIPAA (Healthcare data protection). It implements monitoring rules for data access (unauthorized access detection), data retention (compliance with retention policies), data quality (accuracy and completeness checks), and audit trails (complete activity logging). The pipeline processes raw.audit_logs, raw.transaction_data, raw.customer_data, and raw.employee_data with 99.99% uptime SLA and real-time monitoring (<10 seconds) for critical compliance requirements.\",\n",
        "        \"context\": [\n",
        "            \"Automated compliance monitoring and reporting for regulatory requirements and internal policies\",\n",
        "            \"Regulatory compliance: GDPR, SOX, PCI DSS, HIPAA\",\n",
        "            \"Monitoring rules: data access, data retention, data quality, audit trails\",\n",
        "            \"Critical compliance with <10 second monitoring\"\n",
        "        ]\n",
        "    },\n",
        "    \n",
        "    # Operational Guides\n",
        "    {\n",
        "        \"question\": \"What are the escalation procedures for data pipeline incidents?\",\n",
        "        \"ground_truth\": \"Escalation procedures follow a structured path: Level 1 (Data Engineer, 0-30 min), Level 2 (Senior Data Engineer, 30-60 min), Level 3 (Data Engineering Lead, 60-120 min), Level 4 (Engineering Manager, 120+ min). Escalation triggers include SLA breach imminent or occurred, multiple downstream systems affected, business-critical functionality impacted, and no resolution within expected timeframe. Communication protocols include immediate Slack alerts to #data-incidents, 15-minute email to stakeholders, 30-minute status page update, and 60-minute executive notification for P0/P1 incidents.\",\n",
        "        \"context\": [\n",
        "            \"Incident escalation procedures for data pipeline incidents\",\n",
        "            \"Escalation paths: Level 1-4 with specific timeframes and roles\",\n",
        "            \"Escalation triggers: SLA breach, multiple systems affected, critical functionality\",\n",
        "            \"Communication protocols: Slack, email, status page, executive notification\"\n",
        "        ]\n",
        "    },\n",
        "    \n",
        "    # Data Quality Standards\n",
        "    {\n",
        "        \"question\": \"What are the data quality monitoring thresholds?\",\n",
        "        \"ground_truth\": \"Data quality monitoring uses automated checks including schema validation, data freshness monitoring, anomaly detection, and statistical quality metrics. Alerting thresholds are set at Critical (>1% data quality issues), Warning (>0.1% data quality issues), and Info (quality metrics trending). The monitoring framework covers completeness (no missing values in critical fields, all expected records present, referential integrity maintained), accuracy (data matches source systems, business rules validated, calculated fields verified), consistency (format standards applied, naming conventions followed, data types consistent), and timeliness (data available within SLA windows, processing delays monitored, stale data alerts configured).\",\n",
        "        \"context\": [\n",
        "            \"Data quality standards and monitoring framework\",\n",
        "            \"Quality dimensions: completeness, accuracy, consistency, timeliness\",\n",
        "            \"Automated checks: schema validation, freshness monitoring, anomaly detection\",\n",
        "            \"Alerting thresholds: Critical, Warning, Info levels\"\n",
        "        ]\n",
        "    },\n",
        "    \n",
        "    # SLA Definitions\n",
        "    {\n",
        "        \"question\": \"What are the different SLA tiers for data freshness?\",\n",
        "        \"ground_truth\": \"Data freshness SLAs are tiered as follows: Real-time (<5 minutes delay), Near real-time (<1 hour delay), Batch (<4 hours delay), and Historical (<24 hours delay). Availability SLAs include Critical Systems (99.9% uptime), Important Systems (99.5% uptime), and Standard Systems (99.0% uptime). Recovery Time Objectives (RTO) are P0 Incidents (<1 hour), P1 Incidents (<4 hours), P2 Incidents (<24 hours), and P3 Incidents (<72 hours). Data accuracy requirements vary by type: Financial Data (<0.01% error rate), Operational Data (<0.1% error rate), and Analytical Data (<1% error rate).\",\n",
        "        \"context\": [\n",
        "            \"Service Level Agreement definitions for data pipeline operations\",\n",
        "            \"Data freshness SLAs: Real-time, Near real-time, Batch, Historical\",\n",
        "            \"Availability SLAs: Critical, Important, Standard systems\",\n",
        "            \"Recovery Time Objectives: P0-P3 incident classifications\"\n",
        "        ]\n",
        "    },\n",
        "    \n",
        "    # Troubleshooting Guide\n",
        "    {\n",
        "        \"question\": \"What are the common failure patterns in data pipelines?\",\n",
        "        \"ground_truth\": \"Common failure patterns include Data Quality Issues (symptoms: null values, invalid formats, constraint violations; root causes: source system changes, data corruption, schema drift; solutions: data validation, schema enforcement, source monitoring), Performance Degradation (symptoms: slow queries, timeouts, resource exhaustion; root causes: data volume growth, inefficient queries, resource constraints; solutions: query optimization, resource scaling, partitioning), and Dependency Failures (symptoms: missing upstream data, broken references; root causes: upstream pipeline failures, API outages, network issues; solutions: dependency monitoring, fallback mechanisms, retry logic). Diagnostic procedures include checking pipeline logs, verifying data source availability, validating data quality metrics, testing individual components, and reviewing recent changes.\",\n",
        "        \"context\": [\n",
        "            \"Data pipeline troubleshooting guide for common failure patterns\",\n",
        "            \"Failure patterns: data quality issues, performance degradation, dependency failures\",\n",
        "            \"Diagnostic procedures: logs, data sources, quality metrics, components, changes\",\n",
        "            \"Solutions: validation, optimization, monitoring, fallback mechanisms\"\n",
        "        ]\n",
        "    },\n",
        "    \n",
        "    # Incident Playbook\n",
        "    {\n",
        "        \"question\": \"What are the severity levels for data pipeline incidents?\",\n",
        "        \"ground_truth\": \"Data pipeline incidents are classified into four severity levels: P0 (Critical business impact, revenue loss), P1 (High impact, SLA breach risk), P2 (Medium impact, degraded service), and P3 (Low impact, minor issues). Response procedures include initial assessment (0-15 minutes) with incident acknowledgment, business impact assessment, blast radius determination, and stakeholder notification. Common actions include rollback (revert to last known good state), hotfix (apply targeted fix), backfill (reprocess affected data), and skip (bypass failed step if non-critical). The escalation matrix involves Data Engineering Lead for P0/P1 incidents, Platform Team for infrastructure issues, and Product Manager for business impact assessment.\",\n",
        "        \"context\": [\n",
        "            \"Data pipeline incident response playbook with severity classifications\",\n",
        "            \"Severity levels: P0 (Critical), P1 (High), P2 (Medium), P3 (Low)\",\n",
        "            \"Response procedures: initial assessment, common actions, escalation matrix\",\n",
        "            \"Timeline: 0-15 minutes for initial assessment\"\n",
        "        ]\n",
        "    }\n",
        "]\n",
        "\n",
        "print(f\"✅ Updated golden test dataset with {len(golden_test_data)} comprehensive test cases\")\n",
        "print(f\"📊 Coverage: 10 business domains + 5 operational guides\")\n",
        "print(f\"🎯 RAGAS Improvement: Enhanced test coverage for better evaluation\")\n",
        "print(f\"🚀 Ready for comprehensive RAGAS evaluation!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Generate Responses Using Traceback System\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Response generation function defined\n"
          ]
        }
      ],
      "source": [
        "def generate_traceback_response(question: str) -> Dict[str, Any]:\n",
        "    \"\"\"Generate response using our Traceback system.\"\"\"\n",
        "    try:\n",
        "        # Create initial state\n",
        "        initial_state = AgentState(\n",
        "            question=question,\n",
        "            context=[],\n",
        "            impact_assessment=None,\n",
        "            blast_radius=None,\n",
        "            recommended_actions=None,\n",
        "            incident_brief=None,\n",
        "            current_step=\"supervisor\",\n",
        "            error=None\n",
        "        )\n",
        "        \n",
        "        # Run the workflow\n",
        "        result = traceback_graph.invoke(initial_state)\n",
        "        \n",
        "        # Get retrieved context from the lineage retriever\n",
        "        retrieved_docs = lineage_retriever.search_with_lineage(question, k=5)\n",
        "        retrieved_contexts = [doc.page_content for doc in retrieved_docs]\n",
        "        \n",
        "        # Extract relevant information\n",
        "        return {\n",
        "            \"answer\": result.get(\"incident_brief\", \"No response generated\"),\n",
        "            \"context\": retrieved_contexts,  # Use actual retrieved context\n",
        "            \"blast_radius\": result.get(\"blast_radius\", []),\n",
        "            \"impact_assessment\": result.get(\"impact_assessment\", {})\n",
        "        }\n",
        "    except Exception as e:\n",
        "        return {\n",
        "            \"answer\": f\"Error generating response: {str(e)}\",\n",
        "            \"context\": [],\n",
        "            \"blast_radius\": [],\n",
        "            \"impact_assessment\": {}\n",
        "        }\n",
        "\n",
        "print(\"✅ Response generation function defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🔄 Generating responses using Traceback system...\n",
            "Processing test case 1/15: What should I do if the sales orders pipeline fail...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing test case 2/15: How does the customer analytics pipeline segment c...\n",
            "Processing test case 3/15: What are the stock level management rules for inve...\n",
            "Processing test case 4/15: What financial controls are implemented in the rep...\n",
            "Processing test case 5/15: What attribution models are used for marketing cam...\n",
            "Processing test case 6/15: How is supplier performance measured in the supply...\n",
            "Processing test case 7/15: What employee metrics are tracked in the HR analyt...\n",
            "Processing test case 8/15: What product usage metrics are monitored in real-t...\n",
            "Processing test case 9/15: How is risk scoring calculated in the risk managem...\n",
            "Processing test case 10/15: What regulatory compliance standards are monitored...\n",
            "Processing test case 11/15: What are the escalation procedures for data pipeli...\n",
            "Processing test case 12/15: What are the data quality monitoring thresholds?...\n",
            "Processing test case 13/15: What are the different SLA tiers for data freshnes...\n",
            "Processing test case 14/15: What are the common failure patterns in data pipel...\n",
            "Processing test case 15/15: What are the severity levels for data pipeline inc...\n",
            "✅ Generated responses for 15 test cases\n"
          ]
        }
      ],
      "source": [
        "# Generate responses for all test cases\n",
        "print(\"🔄 Generating responses using Traceback system...\")\n",
        "\n",
        "evaluation_data = []\n",
        "for i, test_case in enumerate(golden_test_data):\n",
        "    print(f\"Processing test case {i+1}/{len(golden_test_data)}: {test_case['question'][:50]}...\")\n",
        "    \n",
        "    # Generate response\n",
        "    response = generate_traceback_response(test_case[\"question\"])\n",
        "    \n",
        "    # Prepare data for RAGAS evaluation\n",
        "    # Use actual retrieved context from our system, not predefined context\n",
        "    evaluation_data.append({\n",
        "        \"question\": test_case[\"question\"],\n",
        "        \"answer\": response[\"answer\"],\n",
        "        \"contexts\": response[\"context\"],  # Use actual retrieved context\n",
        "        \"ground_truth\": test_case[\"ground_truth\"]\n",
        "    })\n",
        "\n",
        "print(f\"✅ Generated responses for {len(evaluation_data)} test cases\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. RAGAS Evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "📊 RAGAS dataset created with 15 samples\n",
            "Dataset columns: ['question', 'answer', 'contexts', 'ground_truth']\n",
            "\n",
            "🔍 Sample data format verification:\n",
            "Question: What should I do if the sales orders pipeline fail...\n",
            "Answer length: 3512 characters\n",
            "Contexts count: 3\n",
            "Contexts type: <class 'list'>\n",
            "First context: Sales orders pipeline processes raw order data int...\n",
            "Ground truth length: 557 characters\n"
          ]
        }
      ],
      "source": [
        "# Convert to RAGAS Dataset format\n",
        "ragas_dataset = Dataset.from_list(evaluation_data)\n",
        "\n",
        "print(f\"📊 RAGAS dataset created with {len(ragas_dataset)} samples\")\n",
        "print(f\"Dataset columns: {ragas_dataset.column_names}\")\n",
        "\n",
        "# Verify the data format\n",
        "print(\"\\n🔍 Sample data format verification:\")\n",
        "sample = ragas_dataset[0]\n",
        "print(f\"Question: {sample['question'][:50]}...\")\n",
        "print(f\"Answer length: {len(sample['answer'])} characters\")\n",
        "print(f\"Contexts count: {len(sample['contexts'])}\")\n",
        "print(f\"Contexts type: {type(sample['contexts'])}\")\n",
        "print(f\"First context: {sample['contexts'][0][:50]}...\")\n",
        "print(f\"Ground truth length: {len(sample['ground_truth'])} characters\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Note: RAGAS EvaluationResult Object\n",
        "RAGAS returns an `EvaluationResult` object, not a dictionary. To access the results, use:\n",
        "- `result.to_pandas()` to get a DataFrame\n",
        "- `result.samples` to get individual sample results\n",
        "- `result.metrics` to get metric names\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🧪 Testing RAGAS evaluation with first 2 samples...\n",
            "🔄 Running RAGAS evaluation on test subset...\n",
            "This may take a few minutes...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6b9ed8d10622496caf9920e80c121070",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluating:   0%|          | 0/8 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ RAGAS test evaluation completed!\n",
            "Test result type: <class 'ragas.dataset_schema.EvaluationResult'>\n",
            "Test results shape: (2, 8)\n",
            "Test results columns: ['user_input', 'retrieved_contexts', 'response', 'reference', 'faithfulness', 'answer_relevancy', 'context_precision', 'context_recall']\n",
            "\n",
            "📊 Test Results Summary:\n",
            "faithfulness: 0.408\n",
            "answer_relevancy: 0.444\n",
            "context_precision: 0.250\n",
            "context_recall: 0.062\n"
          ]
        }
      ],
      "source": [
        "# Test with a smaller subset first to verify everything works\n",
        "print(\"🧪 Testing RAGAS evaluation with first 2 samples...\")\n",
        "\n",
        "# Create a small test dataset\n",
        "test_dataset = Dataset.from_list(evaluation_data[:2])\n",
        "\n",
        "# Define metrics to evaluate\n",
        "metrics = [\n",
        "    faithfulness,      # How factually accurate are the responses?\n",
        "    answer_relevancy, # How relevant are the responses to the questions?\n",
        "    context_precision, # How precise is the retrieved context?\n",
        "    context_recall     # How well does the context cover the ground truth?\n",
        "]\n",
        "\n",
        "print(\"🔄 Running RAGAS evaluation on test subset...\")\n",
        "print(\"This may take a few minutes...\")\n",
        "\n",
        "# Run evaluation on test subset\n",
        "test_result = evaluate(\n",
        "    test_dataset,\n",
        "    metrics=metrics\n",
        ")\n",
        "\n",
        "print(\"✅ RAGAS test evaluation completed!\")\n",
        "print(f\"Test result type: {type(test_result)}\")\n",
        "\n",
        "# Convert to pandas DataFrame to see the results\n",
        "test_df = test_result.to_pandas()\n",
        "print(f\"Test results shape: {test_df.shape}\")\n",
        "print(f\"Test results columns: {list(test_df.columns)}\")\n",
        "print(\"\\n📊 Test Results Summary:\")\n",
        "for metric in ['faithfulness', 'answer_relevancy', 'context_precision', 'context_recall']:\n",
        "    if metric in test_df.columns:\n",
        "        print(f\"{metric}: {test_df[metric].mean():.3f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "📋 Detailed Test Results:\n",
            "==================================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user_input</th>\n",
              "      <th>retrieved_contexts</th>\n",
              "      <th>response</th>\n",
              "      <th>reference</th>\n",
              "      <th>faithfulness</th>\n",
              "      <th>answer_relevancy</th>\n",
              "      <th>context_precision</th>\n",
              "      <th>context_recall</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>What should I do if the sales orders pipeline ...</td>\n",
              "      <td>[Sales orders pipeline processes raw order dat...</td>\n",
              "      <td># Incident Brief: Sales Orders Pipeline Failur...</td>\n",
              "      <td>If the sales orders pipeline fails, follow the...</td>\n",
              "      <td>0.763158</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.125</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>How does the customer analytics pipeline segme...</td>\n",
              "      <td>[Sales orders pipeline processes raw order dat...</td>\n",
              "      <td># Incident Brief: Customer Analytics Pipeline ...</td>\n",
              "      <td>The customer analytics pipeline segments custo...</td>\n",
              "      <td>0.052632</td>\n",
              "      <td>0.88719</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                          user_input  \\\n",
              "0  What should I do if the sales orders pipeline ...   \n",
              "1  How does the customer analytics pipeline segme...   \n",
              "\n",
              "                                  retrieved_contexts  \\\n",
              "0  [Sales orders pipeline processes raw order dat...   \n",
              "1  [Sales orders pipeline processes raw order dat...   \n",
              "\n",
              "                                            response  \\\n",
              "0  # Incident Brief: Sales Orders Pipeline Failur...   \n",
              "1  # Incident Brief: Customer Analytics Pipeline ...   \n",
              "\n",
              "                                           reference  faithfulness  \\\n",
              "0  If the sales orders pipeline fails, follow the...      0.763158   \n",
              "1  The customer analytics pipeline segments custo...      0.052632   \n",
              "\n",
              "   answer_relevancy  context_precision  context_recall  \n",
              "0           0.00000                0.5           0.125  \n",
              "1           0.88719                0.0           0.000  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "✅ Test PASSED: Ready for full evaluation\n"
          ]
        }
      ],
      "source": [
        "# Display detailed test results\n",
        "print(\"📋 Detailed Test Results:\")\n",
        "print(\"=\" * 50)\n",
        "display(test_df)\n",
        "\n",
        "# Check if test passed (all metrics > 0)\n",
        "test_passed = all(test_df[metric].mean() > 0 for metric in ['faithfulness', 'answer_relevancy'] if metric in test_df.columns)\n",
        "print(f\"\\n✅ Test {'PASSED' if test_passed else 'FAILED'}: Ready for full evaluation\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🚀 Running full RAGAS evaluation on all samples...\n",
            "This may take several minutes...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a71183f0b9fe46359de172eb549b4505",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluating:   0%|          | 0/60 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ RAGAS evaluation completed!\n"
          ]
        }
      ],
      "source": [
        "# Run full evaluation on all samples\n",
        "print(\"🚀 Running full RAGAS evaluation on all samples...\")\n",
        "print(\"This may take several minutes...\")\n",
        "\n",
        "# Run evaluation on full dataset\n",
        "result = evaluate(\n",
        "    ragas_dataset,\n",
        "    metrics=metrics\n",
        ")\n",
        "\n",
        "print(\"✅ RAGAS evaluation completed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Results Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "📊 RAGAS Evaluation Results:\n",
            "==================================================\n",
            "\n",
            "🎯 Overall Performance Metrics:\n",
            "Faithfulness        : 0.469\n",
            "Answer Relevancy    : 0.787\n",
            "Context Precision   : 0.033\n",
            "Context Recall      : 0.017\n",
            "\n",
            "📋 Detailed Results:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user_input</th>\n",
              "      <th>retrieved_contexts</th>\n",
              "      <th>response</th>\n",
              "      <th>reference</th>\n",
              "      <th>faithfulness</th>\n",
              "      <th>answer_relevancy</th>\n",
              "      <th>context_precision</th>\n",
              "      <th>context_recall</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>What should I do if the sales orders pipeline ...</td>\n",
              "      <td>[Sales orders pipeline processes raw order dat...</td>\n",
              "      <td># Incident Brief: Sales Orders Pipeline Failur...</td>\n",
              "      <td>If the sales orders pipeline fails, follow the...</td>\n",
              "      <td>0.571429</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>How does the customer analytics pipeline segme...</td>\n",
              "      <td>[Sales orders pipeline processes raw order dat...</td>\n",
              "      <td># Incident Brief: Customer Analytics Pipeline ...</td>\n",
              "      <td>The customer analytics pipeline segments custo...</td>\n",
              "      <td>0.078947</td>\n",
              "      <td>0.892550</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>What are the stock level management rules for ...</td>\n",
              "      <td>[Sales orders pipeline processes raw order dat...</td>\n",
              "      <td># Incident Brief: Inventory Stock Level Manage...</td>\n",
              "      <td>Inventory stock levels are managed with these ...</td>\n",
              "      <td>0.088235</td>\n",
              "      <td>0.894840</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>What financial controls are implemented in the...</td>\n",
              "      <td>[Sales orders pipeline processes raw order dat...</td>\n",
              "      <td># Incident Brief: Reporting Pipeline Disruptio...</td>\n",
              "      <td>The financial reporting pipeline implements co...</td>\n",
              "      <td>0.631579</td>\n",
              "      <td>0.845754</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>What attribution models are used for marketing...</td>\n",
              "      <td>[Sales orders pipeline processes raw order dat...</td>\n",
              "      <td># Incident Brief: Marketing Campaign Attributi...</td>\n",
              "      <td>The marketing attribution pipeline uses multip...</td>\n",
              "      <td>0.482759</td>\n",
              "      <td>0.893890</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>How is supplier performance measured in the su...</td>\n",
              "      <td>[Sales orders pipeline processes raw order dat...</td>\n",
              "      <td># Incident Brief: Supplier Performance Measure...</td>\n",
              "      <td>Supplier performance is measured using key met...</td>\n",
              "      <td>0.111111</td>\n",
              "      <td>0.879565</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>What employee metrics are tracked in the HR an...</td>\n",
              "      <td>[Sales orders pipeline processes raw order dat...</td>\n",
              "      <td># Incident Brief: HR Analytics Pipeline Disrup...</td>\n",
              "      <td>The HR analytics pipeline tracks comprehensive...</td>\n",
              "      <td>0.027778</td>\n",
              "      <td>0.898012</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>What product usage metrics are monitored in re...</td>\n",
              "      <td>[Sales orders pipeline processes raw order dat...</td>\n",
              "      <td># Incident Brief: Sales Orders Pipeline Disrup...</td>\n",
              "      <td>The product analytics pipeline monitors real-t...</td>\n",
              "      <td>0.951220</td>\n",
              "      <td>0.786488</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>How is risk scoring calculated in the risk man...</td>\n",
              "      <td>[Data pipeline incident response procedures: 1...</td>\n",
              "      <td># Incident Brief\\n\\n## 1. Incident Summary\\nOn...</td>\n",
              "      <td>Risk scoring is calculated across multiple dim...</td>\n",
              "      <td>0.705882</td>\n",
              "      <td>0.755445</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>What regulatory compliance standards are monit...</td>\n",
              "      <td>[Data pipeline incident response procedures: 1...</td>\n",
              "      <td># Incident Brief: Sales Orders Pipeline Disrup...</td>\n",
              "      <td>The compliance monitoring system monitors mult...</td>\n",
              "      <td>0.277778</td>\n",
              "      <td>0.748935</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>What are the escalation procedures for data pi...</td>\n",
              "      <td>[Data pipeline incident response procedures: 1...</td>\n",
              "      <td># Incident Brief: Data Pipeline Incident\\n\\n##...</td>\n",
              "      <td>Escalation procedures follow a structured path...</td>\n",
              "      <td>0.757576</td>\n",
              "      <td>0.872640</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>What are the data quality monitoring thresholds?</td>\n",
              "      <td>[Data pipeline incident response procedures: 1...</td>\n",
              "      <td># Incident Brief: Sales Orders Pipeline Incide...</td>\n",
              "      <td>Data quality monitoring uses automated checks ...</td>\n",
              "      <td>0.705882</td>\n",
              "      <td>0.769921</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>What are the different SLA tiers for data fres...</td>\n",
              "      <td>[Sales orders pipeline processes raw order dat...</td>\n",
              "      <td># Incident Brief: Data Freshness SLA Violation...</td>\n",
              "      <td>Data freshness SLAs are tiered as follows: Rea...</td>\n",
              "      <td>0.909091</td>\n",
              "      <td>0.855317</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>What are the common failure patterns in data p...</td>\n",
              "      <td>[Data pipeline incident response procedures: 1...</td>\n",
              "      <td># Incident Brief: Sales Orders Data Pipeline F...</td>\n",
              "      <td>Common failure patterns include Data Quality I...</td>\n",
              "      <td>0.707317</td>\n",
              "      <td>0.830415</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>What are the severity levels for data pipeline...</td>\n",
              "      <td>[Data pipeline incident response procedures: 1...</td>\n",
              "      <td># Incident Brief: Data Pipeline Incident\\n\\n##...</td>\n",
              "      <td>Data pipeline incidents are classified into fo...</td>\n",
              "      <td>0.031250</td>\n",
              "      <td>0.874996</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.25</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                           user_input  \\\n",
              "0   What should I do if the sales orders pipeline ...   \n",
              "1   How does the customer analytics pipeline segme...   \n",
              "2   What are the stock level management rules for ...   \n",
              "3   What financial controls are implemented in the...   \n",
              "4   What attribution models are used for marketing...   \n",
              "5   How is supplier performance measured in the su...   \n",
              "6   What employee metrics are tracked in the HR an...   \n",
              "7   What product usage metrics are monitored in re...   \n",
              "8   How is risk scoring calculated in the risk man...   \n",
              "9   What regulatory compliance standards are monit...   \n",
              "10  What are the escalation procedures for data pi...   \n",
              "11   What are the data quality monitoring thresholds?   \n",
              "12  What are the different SLA tiers for data fres...   \n",
              "13  What are the common failure patterns in data p...   \n",
              "14  What are the severity levels for data pipeline...   \n",
              "\n",
              "                                   retrieved_contexts  \\\n",
              "0   [Sales orders pipeline processes raw order dat...   \n",
              "1   [Sales orders pipeline processes raw order dat...   \n",
              "2   [Sales orders pipeline processes raw order dat...   \n",
              "3   [Sales orders pipeline processes raw order dat...   \n",
              "4   [Sales orders pipeline processes raw order dat...   \n",
              "5   [Sales orders pipeline processes raw order dat...   \n",
              "6   [Sales orders pipeline processes raw order dat...   \n",
              "7   [Sales orders pipeline processes raw order dat...   \n",
              "8   [Data pipeline incident response procedures: 1...   \n",
              "9   [Data pipeline incident response procedures: 1...   \n",
              "10  [Data pipeline incident response procedures: 1...   \n",
              "11  [Data pipeline incident response procedures: 1...   \n",
              "12  [Sales orders pipeline processes raw order dat...   \n",
              "13  [Data pipeline incident response procedures: 1...   \n",
              "14  [Data pipeline incident response procedures: 1...   \n",
              "\n",
              "                                             response  \\\n",
              "0   # Incident Brief: Sales Orders Pipeline Failur...   \n",
              "1   # Incident Brief: Customer Analytics Pipeline ...   \n",
              "2   # Incident Brief: Inventory Stock Level Manage...   \n",
              "3   # Incident Brief: Reporting Pipeline Disruptio...   \n",
              "4   # Incident Brief: Marketing Campaign Attributi...   \n",
              "5   # Incident Brief: Supplier Performance Measure...   \n",
              "6   # Incident Brief: HR Analytics Pipeline Disrup...   \n",
              "7   # Incident Brief: Sales Orders Pipeline Disrup...   \n",
              "8   # Incident Brief\\n\\n## 1. Incident Summary\\nOn...   \n",
              "9   # Incident Brief: Sales Orders Pipeline Disrup...   \n",
              "10  # Incident Brief: Data Pipeline Incident\\n\\n##...   \n",
              "11  # Incident Brief: Sales Orders Pipeline Incide...   \n",
              "12  # Incident Brief: Data Freshness SLA Violation...   \n",
              "13  # Incident Brief: Sales Orders Data Pipeline F...   \n",
              "14  # Incident Brief: Data Pipeline Incident\\n\\n##...   \n",
              "\n",
              "                                            reference  faithfulness  \\\n",
              "0   If the sales orders pipeline fails, follow the...      0.571429   \n",
              "1   The customer analytics pipeline segments custo...      0.078947   \n",
              "2   Inventory stock levels are managed with these ...      0.088235   \n",
              "3   The financial reporting pipeline implements co...      0.631579   \n",
              "4   The marketing attribution pipeline uses multip...      0.482759   \n",
              "5   Supplier performance is measured using key met...      0.111111   \n",
              "6   The HR analytics pipeline tracks comprehensive...      0.027778   \n",
              "7   The product analytics pipeline monitors real-t...      0.951220   \n",
              "8   Risk scoring is calculated across multiple dim...      0.705882   \n",
              "9   The compliance monitoring system monitors mult...      0.277778   \n",
              "10  Escalation procedures follow a structured path...      0.757576   \n",
              "11  Data quality monitoring uses automated checks ...      0.705882   \n",
              "12  Data freshness SLAs are tiered as follows: Rea...      0.909091   \n",
              "13  Common failure patterns include Data Quality I...      0.707317   \n",
              "14  Data pipeline incidents are classified into fo...      0.031250   \n",
              "\n",
              "    answer_relevancy  context_precision  context_recall  \n",
              "0           0.000000                0.5            0.00  \n",
              "1           0.892550                0.0            0.00  \n",
              "2           0.894840                0.0            0.00  \n",
              "3           0.845754                0.0            0.00  \n",
              "4           0.893890                0.0            0.00  \n",
              "5           0.879565                0.0            0.00  \n",
              "6           0.898012                0.0            0.00  \n",
              "7           0.786488                0.0            0.00  \n",
              "8           0.755445                0.0            0.00  \n",
              "9           0.748935                0.0            0.00  \n",
              "10          0.872640                0.0            0.00  \n",
              "11          0.769921                0.0            0.00  \n",
              "12          0.855317                0.0            0.00  \n",
              "13          0.830415                0.0            0.00  \n",
              "14          0.874996                0.0            0.25  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Extract results\n",
        "results_df = result.to_pandas()\n",
        "\n",
        "print(\"📊 RAGAS Evaluation Results:\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Display overall metrics\n",
        "overall_metrics = {\n",
        "    \"Faithfulness\": results_df['faithfulness'].mean(),\n",
        "    \"Answer Relevancy\": results_df['answer_relevancy'].mean(),\n",
        "    \"Context Precision\": results_df['context_precision'].mean(),\n",
        "    \"Context Recall\": results_df['context_recall'].mean()\n",
        "}\n",
        "\n",
        "print(\"\\n🎯 Overall Performance Metrics:\")\n",
        "for metric, score in overall_metrics.items():\n",
        "    print(f\"{metric:20}: {score:.3f}\")\n",
        "\n",
        "print(\"\\n📋 Detailed Results:\")\n",
        "display(results_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🔍 Debugging Results DataFrame:\n",
            "========================================\n",
            "DataFrame shape: (15, 8)\n",
            "Available columns: ['user_input', 'retrieved_contexts', 'response', 'reference', 'faithfulness', 'answer_relevancy', 'context_precision', 'context_recall']\n",
            "DataFrame head:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user_input</th>\n",
              "      <th>retrieved_contexts</th>\n",
              "      <th>response</th>\n",
              "      <th>reference</th>\n",
              "      <th>faithfulness</th>\n",
              "      <th>answer_relevancy</th>\n",
              "      <th>context_precision</th>\n",
              "      <th>context_recall</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>What should I do if the sales orders pipeline ...</td>\n",
              "      <td>[Sales orders pipeline processes raw order dat...</td>\n",
              "      <td># Incident Brief: Sales Orders Pipeline Failur...</td>\n",
              "      <td>If the sales orders pipeline fails, follow the...</td>\n",
              "      <td>0.571429</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>How does the customer analytics pipeline segme...</td>\n",
              "      <td>[Sales orders pipeline processes raw order dat...</td>\n",
              "      <td># Incident Brief: Customer Analytics Pipeline ...</td>\n",
              "      <td>The customer analytics pipeline segments custo...</td>\n",
              "      <td>0.078947</td>\n",
              "      <td>0.892550</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>What are the stock level management rules for ...</td>\n",
              "      <td>[Sales orders pipeline processes raw order dat...</td>\n",
              "      <td># Incident Brief: Inventory Stock Level Manage...</td>\n",
              "      <td>Inventory stock levels are managed with these ...</td>\n",
              "      <td>0.088235</td>\n",
              "      <td>0.894840</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>What financial controls are implemented in the...</td>\n",
              "      <td>[Sales orders pipeline processes raw order dat...</td>\n",
              "      <td># Incident Brief: Reporting Pipeline Disruptio...</td>\n",
              "      <td>The financial reporting pipeline implements co...</td>\n",
              "      <td>0.631579</td>\n",
              "      <td>0.845754</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>What attribution models are used for marketing...</td>\n",
              "      <td>[Sales orders pipeline processes raw order dat...</td>\n",
              "      <td># Incident Brief: Marketing Campaign Attributi...</td>\n",
              "      <td>The marketing attribution pipeline uses multip...</td>\n",
              "      <td>0.482759</td>\n",
              "      <td>0.893890</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                          user_input  \\\n",
              "0  What should I do if the sales orders pipeline ...   \n",
              "1  How does the customer analytics pipeline segme...   \n",
              "2  What are the stock level management rules for ...   \n",
              "3  What financial controls are implemented in the...   \n",
              "4  What attribution models are used for marketing...   \n",
              "\n",
              "                                  retrieved_contexts  \\\n",
              "0  [Sales orders pipeline processes raw order dat...   \n",
              "1  [Sales orders pipeline processes raw order dat...   \n",
              "2  [Sales orders pipeline processes raw order dat...   \n",
              "3  [Sales orders pipeline processes raw order dat...   \n",
              "4  [Sales orders pipeline processes raw order dat...   \n",
              "\n",
              "                                            response  \\\n",
              "0  # Incident Brief: Sales Orders Pipeline Failur...   \n",
              "1  # Incident Brief: Customer Analytics Pipeline ...   \n",
              "2  # Incident Brief: Inventory Stock Level Manage...   \n",
              "3  # Incident Brief: Reporting Pipeline Disruptio...   \n",
              "4  # Incident Brief: Marketing Campaign Attributi...   \n",
              "\n",
              "                                           reference  faithfulness  \\\n",
              "0  If the sales orders pipeline fails, follow the...      0.571429   \n",
              "1  The customer analytics pipeline segments custo...      0.078947   \n",
              "2  Inventory stock levels are managed with these ...      0.088235   \n",
              "3  The financial reporting pipeline implements co...      0.631579   \n",
              "4  The marketing attribution pipeline uses multip...      0.482759   \n",
              "\n",
              "   answer_relevancy  context_precision  context_recall  \n",
              "0          0.000000                0.5             0.0  \n",
              "1          0.892550                0.0             0.0  \n",
              "2          0.894840                0.0             0.0  \n",
              "3          0.845754                0.0             0.0  \n",
              "4          0.893890                0.0             0.0  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Debug: Check what columns are available in results_df\n",
        "print(\"🔍 Debugging Results DataFrame:\")\n",
        "print(\"=\" * 40)\n",
        "print(f\"DataFrame shape: {results_df.shape}\")\n",
        "print(f\"Available columns: {list(results_df.columns)}\")\n",
        "print(f\"DataFrame head:\")\n",
        "display(results_df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "📊 RAGAS Evaluation Summary Table:\n",
            "================================================================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Metric</th>\n",
              "      <th>Score</th>\n",
              "      <th>Interpretation</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Faithfulness</td>\n",
              "      <td>0.469189</td>\n",
              "      <td>How factually accurate are the responses?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Answer Relevancy</td>\n",
              "      <td>0.786585</td>\n",
              "      <td>How relevant are the responses to the questions?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Context Precision</td>\n",
              "      <td>0.033333</td>\n",
              "      <td>How precise is the retrieved context?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Context Recall</td>\n",
              "      <td>0.016667</td>\n",
              "      <td>How well does the context cover the ground truth?</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              Metric     Score  \\\n",
              "0       Faithfulness  0.469189   \n",
              "1   Answer Relevancy  0.786585   \n",
              "2  Context Precision  0.033333   \n",
              "3     Context Recall  0.016667   \n",
              "\n",
              "                                      Interpretation  \n",
              "0          How factually accurate are the responses?  \n",
              "1   How relevant are the responses to the questions?  \n",
              "2              How precise is the retrieved context?  \n",
              "3  How well does the context cover the ground truth?  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Create a summary table\n",
        "summary_table = pd.DataFrame({\n",
        "    \"Metric\": [\"Faithfulness\", \"Answer Relevancy\", \"Context Precision\", \"Context Recall\"],\n",
        "    \"Score\": [overall_metrics[\"Faithfulness\"], overall_metrics[\"Answer Relevancy\"], \n",
        "              overall_metrics[\"Context Precision\"], overall_metrics[\"Context Recall\"]],\n",
        "    \"Interpretation\": [\n",
        "        \"How factually accurate are the responses?\",\n",
        "        \"How relevant are the responses to the questions?\",\n",
        "        \"How precise is the retrieved context?\",\n",
        "        \"How well does the context cover the ground truth?\"\n",
        "    ]\n",
        "})\n",
        "\n",
        "print(\"\\n📊 RAGAS Evaluation Summary Table:\")\n",
        "print(\"=\" * 80)\n",
        "display(summary_table)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🔍 Safe Performance Analysis:\n",
            "==================================================\n",
            "Available metrics: ['faithfulness', 'answer_relevancy', 'context_precision', 'context_recall']\n",
            "\n",
            "📊 Overall Performance Metrics:\n",
            "faithfulness        : 0.469\n",
            "answer_relevancy    : 0.787\n",
            "context_precision   : 0.033\n",
            "context_recall      : 0.017\n",
            "\n",
            "📈 Performance Statistics:\n",
            "\n",
            "faithfulness:\n",
            "  Mean: 0.469\n",
            "  Std:  0.334\n",
            "  Min:  0.028\n",
            "  Max:  0.951\n",
            "\n",
            "answer_relevancy:\n",
            "  Mean: 0.787\n",
            "  Std:  0.224\n",
            "  Min:  0.000\n",
            "  Max:  0.898\n",
            "\n",
            "context_precision:\n",
            "  Mean: 0.033\n",
            "  Std:  0.129\n",
            "  Min:  0.000\n",
            "  Max:  0.500\n",
            "\n",
            "context_recall:\n",
            "  Mean: 0.017\n",
            "  Std:  0.065\n",
            "  Min:  0.000\n",
            "  Max:  0.250\n"
          ]
        }
      ],
      "source": [
        "# Safe performance analysis (handles missing columns)\n",
        "print(\"🔍 Safe Performance Analysis:\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Check what metrics are available\n",
        "available_metrics = [col for col in ['faithfulness', 'answer_relevancy', 'context_precision', 'context_recall'] \n",
        "                    if col in results_df.columns]\n",
        "\n",
        "print(f\"Available metrics: {available_metrics}\")\n",
        "\n",
        "if available_metrics:\n",
        "    # Calculate overall performance\n",
        "    overall_performance = {}\n",
        "    for metric in available_metrics:\n",
        "        overall_performance[metric] = results_df[metric].mean()\n",
        "    \n",
        "    print(\"\\n📊 Overall Performance Metrics:\")\n",
        "    for metric, score in overall_performance.items():\n",
        "        print(f\"{metric:20}: {score:.3f}\")\n",
        "    \n",
        "    # Calculate performance statistics\n",
        "    print(\"\\n📈 Performance Statistics:\")\n",
        "    for metric in available_metrics:\n",
        "        print(f\"\\n{metric}:\")\n",
        "        print(f\"  Mean: {results_df[metric].mean():.3f}\")\n",
        "        print(f\"  Std:  {results_df[metric].std():.3f}\")\n",
        "        print(f\"  Min:  {results_df[metric].min():.3f}\")\n",
        "        print(f\"  Max:  {results_df[metric].max():.3f}\")\n",
        "else:\n",
        "    print(\"❌ No metrics found in results DataFrame\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🔍 SAFE Performance Analysis:\n",
            "==================================================\n",
            "Available columns: ['user_input', 'retrieved_contexts', 'response', 'reference', 'faithfulness', 'answer_relevancy', 'context_precision', 'context_recall']\n",
            "⚠️ Question column not found - showing overall performance only\n",
            "\n",
            "📊 Overall Performance Summary:\n",
            "faithfulness: 0.469\n",
            "answer_relevancy: 0.787\n",
            "context_precision: 0.033\n",
            "context_recall: 0.017\n"
          ]
        }
      ],
      "source": [
        "# SAFE ALTERNATIVE: Skip the problematic cell above and use this instead\n",
        "print(\"🔍 SAFE Performance Analysis:\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Check what columns are actually available\n",
        "print(f\"Available columns: {list(results_df.columns)}\")\n",
        "\n",
        "# Check if question column exists\n",
        "if 'question' in results_df.columns:\n",
        "    print(\"✅ Question column found - can do question type analysis\")\n",
        "    # Add question categories safely\n",
        "    results_df['question_type'] = results_df['question'].apply(lambda x: \n",
        "        'Impact Analysis' if 'impacted' in x.lower() else\n",
        "        'Troubleshooting' if 'troubleshoot' in x.lower() or 'should i do' in x.lower() else\n",
        "        'Dependency Analysis' if 'depend' in x.lower() else\n",
        "        'SLA Query' if 'sla' in x.lower() else\n",
        "        'General'\n",
        "    )\n",
        "    \n",
        "    # Group by question type\n",
        "    available_metrics = [col for col in ['faithfulness', 'answer_relevancy', 'context_precision', 'context_recall'] \n",
        "                        if col in results_df.columns]\n",
        "    \n",
        "    if available_metrics:\n",
        "        performance_by_type = results_df.groupby('question_type')[available_metrics].mean().round(3)\n",
        "        print(\"\\n📊 Performance by Question Type:\")\n",
        "        display(performance_by_type)\n",
        "    else:\n",
        "        print(\"❌ No metrics found for grouping\")\n",
        "else:\n",
        "    print(\"⚠️ Question column not found - showing overall performance only\")\n",
        "    \n",
        "    # Show overall performance\n",
        "    available_metrics = [col for col in ['faithfulness', 'answer_relevancy', 'context_precision', 'context_recall'] \n",
        "                        if col in results_df.columns]\n",
        "    \n",
        "    if available_metrics:\n",
        "        print(\"\\n📊 Overall Performance Summary:\")\n",
        "        for metric in available_metrics:\n",
        "            print(f\"{metric}: {results_df[metric].mean():.3f}\")\n",
        "    else:\n",
        "        print(\"❌ No performance metrics found\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Performance Analysis and Conclusions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "🎯 Performance Strengths and Weaknesses:\n",
            "==================================================\n",
            "\n",
            "✅ Strongest Performance: answer_relevancy (0.787)\n",
            "❌ Weakest Performance: context_recall (0.017)\n",
            "\n",
            "📊 Overall Pipeline Score: 0.326\n",
            "🎯 Performance Level: Needs Improvement\n"
          ]
        }
      ],
      "source": [
        "# Identify strengths and weaknesses (with safety checks)\n",
        "print(\"\\n🎯 Performance Strengths and Weaknesses:\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Use the safe overall_performance from previous cell\n",
        "if 'overall_performance' in locals() and overall_performance:\n",
        "    # Find best and worst performing metrics\n",
        "    best_metric = max(overall_performance.items(), key=lambda x: x[1])\n",
        "    worst_metric = min(overall_performance.items(), key=lambda x: x[1])\n",
        "\n",
        "    print(f\"\\n✅ Strongest Performance: {best_metric[0]} ({best_metric[1]:.3f})\")\n",
        "    print(f\"❌ Weakest Performance: {worst_metric[0]} ({worst_metric[1]:.3f})\")\n",
        "\n",
        "    # Calculate overall score\n",
        "    overall_score = np.mean(list(overall_performance.values()))\n",
        "    print(f\"\\n📊 Overall Pipeline Score: {overall_score:.3f}\")\n",
        "\n",
        "    # Performance interpretation\n",
        "    if overall_score >= 0.8:\n",
        "        performance_level = \"Excellent\"\n",
        "    elif overall_score >= 0.7:\n",
        "        performance_level = \"Good\"\n",
        "    elif overall_score >= 0.6:\n",
        "        performance_level = \"Fair\"\n",
        "    else:\n",
        "        performance_level = \"Needs Improvement\"\n",
        "\n",
        "    print(f\"🎯 Performance Level: {performance_level}\")\n",
        "else:\n",
        "    print(\"⚠️ No performance data available for analysis\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Detailed Conclusions and Recommendations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "📋 Detailed Conclusions and Recommendations:\n",
            "============================================================\n",
            "\n",
            "🔍 Key Findings:\n",
            "--------------------\n",
            "1. Faithfulness (0.469): The system has significant factual accuracy issues that need attention.\n",
            "2. Answer Relevancy (0.787): Responses are generally relevant but may sometimes miss the mark.\n",
            "3. Context Precision (0.033): The system retrieves context with significant noise and irrelevance.\n",
            "4. Context Recall (0.017): The system often misses important context needed for accurate responses.\n"
          ]
        }
      ],
      "source": [
        "print(\"📋 Detailed Conclusions and Recommendations:\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(\"\\n🔍 Key Findings:\")\n",
        "print(\"-\" * 20)\n",
        "\n",
        "# Faithfulness analysis\n",
        "faithfulness_score = overall_metrics[\"Faithfulness\"]\n",
        "if faithfulness_score >= 0.8:\n",
        "    faithfulness_conclusion = \"The system generates highly factual and accurate responses.\"\n",
        "elif faithfulness_score >= 0.6:\n",
        "    faithfulness_conclusion = \"The system generates mostly accurate responses with some factual inconsistencies.\"\n",
        "else:\n",
        "    faithfulness_conclusion = \"The system has significant factual accuracy issues that need attention.\"\n",
        "\n",
        "print(f\"1. Faithfulness ({faithfulness_score:.3f}): {faithfulness_conclusion}\")\n",
        "\n",
        "# Answer relevancy analysis\n",
        "relevancy_score = overall_metrics[\"Answer Relevancy\"]\n",
        "if relevancy_score >= 0.8:\n",
        "    relevancy_conclusion = \"Responses are highly relevant to the questions asked.\"\n",
        "elif relevancy_score >= 0.6:\n",
        "    relevancy_conclusion = \"Responses are generally relevant but may sometimes miss the mark.\"\n",
        "else:\n",
        "    relevancy_conclusion = \"Responses often lack relevance to the specific questions asked.\"\n",
        "\n",
        "print(f\"2. Answer Relevancy ({relevancy_score:.3f}): {relevancy_conclusion}\")\n",
        "\n",
        "# Context precision analysis\n",
        "precision_score = overall_metrics[\"Context Precision\"]\n",
        "if precision_score >= 0.8:\n",
        "    precision_conclusion = \"The system retrieves highly precise and relevant context.\"\n",
        "elif precision_score >= 0.6:\n",
        "    precision_conclusion = \"The system retrieves reasonably precise context with some noise.\"\n",
        "else:\n",
        "    precision_conclusion = \"The system retrieves context with significant noise and irrelevance.\"\n",
        "\n",
        "print(f\"3. Context Precision ({precision_score:.3f}): {precision_conclusion}\")\n",
        "\n",
        "# Context recall analysis\n",
        "recall_score = overall_metrics[\"Context Recall\"]\n",
        "if recall_score >= 0.8:\n",
        "    recall_conclusion = \"The system retrieves comprehensive context that covers ground truth well.\"\n",
        "elif recall_score >= 0.6:\n",
        "    recall_conclusion = \"The system retrieves adequate context but may miss some important information.\"\n",
        "else:\n",
        "    recall_conclusion = \"The system often misses important context needed for accurate responses.\"\n",
        "\n",
        "print(f\"4. Context Recall ({recall_score:.3f}): {recall_conclusion}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "💡 Recommendations for Improvement:\n",
            "----------------------------------------\n",
            "• Improve factual accuracy by enhancing the knowledge base and fact-checking mechanisms\n",
            "• Enhance response relevance by improving question understanding and response generation\n",
            "• Improve context precision by refining retrieval algorithms and filtering mechanisms\n",
            "• Enhance context recall by expanding the knowledge base and improving retrieval coverage\n",
            "• Consider fine-tuning the LLM on domain-specific data\n",
            "• Implement feedback loops to continuously improve performance\n",
            "• Add more diverse test cases to the evaluation dataset\n",
            "• Consider ensemble methods for better response quality\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n💡 Recommendations for Improvement:\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "recommendations = []\n",
        "\n",
        "# Faithfulness recommendations\n",
        "if faithfulness_score < 0.8:\n",
        "    recommendations.append(\"• Improve factual accuracy by enhancing the knowledge base and fact-checking mechanisms\")\n",
        "\n",
        "# Relevancy recommendations\n",
        "if relevancy_score < 0.8:\n",
        "    recommendations.append(\"• Enhance response relevance by improving question understanding and response generation\")\n",
        "\n",
        "# Precision recommendations\n",
        "if precision_score < 0.8:\n",
        "    recommendations.append(\"• Improve context precision by refining retrieval algorithms and filtering mechanisms\")\n",
        "\n",
        "# Recall recommendations\n",
        "if recall_score < 0.8:\n",
        "    recommendations.append(\"• Enhance context recall by expanding the knowledge base and improving retrieval coverage\")\n",
        "\n",
        "# General recommendations\n",
        "if overall_score < 0.8:\n",
        "    recommendations.extend([\n",
        "        \"• Consider fine-tuning the LLM on domain-specific data\",\n",
        "        \"• Implement feedback loops to continuously improve performance\",\n",
        "        \"• Add more diverse test cases to the evaluation dataset\",\n",
        "        \"• Consider ensemble methods for better response quality\"\n",
        "    ])\n",
        "\n",
        "if recommendations:\n",
        "    for rec in recommendations:\n",
        "        print(rec)\n",
        "else:\n",
        "    print(\"• System performance is excellent - consider monitoring for consistency\")\n",
        "    print(\"• Expand the test dataset to cover more edge cases\")\n",
        "    print(\"• Implement A/B testing for continuous improvement\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "🎯 Overall Pipeline Effectiveness Assessment:\n",
            "==================================================\n",
            "\n",
            "📊 Summary Statistics:\n",
            "• Total Test Cases: 15\n",
            "• Overall Score: 0.326\n",
            "• Performance Level: Needs Improvement\n",
            "• Best Metric: answer_relevancy (0.787)\n",
            "• Worst Metric: context_recall (0.017)\n",
            "\n",
            "🔍 Key Insights:\n",
            "• The Traceback system demonstrates weak performance across all RAGAS metrics\n",
            "• The system shows good performance in answer_relevancy\n",
            "• Significant improvement needed in context_recall\n",
            "\n",
            "✅ Conclusion:\n",
            "The Traceback pipeline needs substantial improvements before production deployment.\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n🎯 Overall Pipeline Effectiveness Assessment:\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "print(f\"\\n📊 Summary Statistics:\")\n",
        "print(f\"• Total Test Cases: {len(evaluation_data)}\")\n",
        "print(f\"• Overall Score: {overall_score:.3f}\")\n",
        "print(f\"• Performance Level: {performance_level}\")\n",
        "print(f\"• Best Metric: {best_metric[0]} ({best_metric[1]:.3f})\")\n",
        "print(f\"• Worst Metric: {worst_metric[0]} ({worst_metric[1]:.3f})\")\n",
        "\n",
        "print(f\"\\n🔍 Key Insights:\")\n",
        "print(f\"• The Traceback system demonstrates {'strong' if overall_score >= 0.7 else 'moderate' if overall_score >= 0.6 else 'weak'} performance across all RAGAS metrics\")\n",
        "print(f\"• {'The system excels at' if best_metric[1] >= 0.8 else 'The system shows good performance in'} {best_metric[0].lower()}\")\n",
        "print(f\"• {'Significant improvement needed in' if worst_metric[1] < 0.6 else 'Some improvement possible in'} {worst_metric[0].lower()}\")\n",
        "\n",
        "print(f\"\\n✅ Conclusion:\")\n",
        "if overall_score >= 0.8:\n",
        "    conclusion = \"The Traceback pipeline is highly effective and ready for production deployment.\"\n",
        "elif overall_score >= 0.7:\n",
        "    conclusion = \"The Traceback pipeline shows good effectiveness with room for targeted improvements.\"\n",
        "elif overall_score >= 0.6:\n",
        "    conclusion = \"The Traceback pipeline demonstrates fair effectiveness but requires significant improvements.\"\n",
        "else:\n",
        "    conclusion = \"The Traceback pipeline needs substantial improvements before production deployment.\"\n",
        "\n",
        "print(conclusion)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Save Results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Results saved to: /Users/sandeepgogineni/ai-engineering/bootcamp/Traceback/data/ragas_evaluation_results.json\n",
            "📊 Evaluation completed successfully!\n",
            "🎯 Overall Pipeline Score: 0.326 (Needs Improvement)\n"
          ]
        }
      ],
      "source": [
        "# Save detailed results\n",
        "results_output_path = Path.cwd().parent / \"data\" / \"ragas_evaluation_results.json\"\n",
        "\n",
        "evaluation_summary = {\n",
        "    \"evaluation_date\": pd.Timestamp.now().isoformat(),\n",
        "    \"total_test_cases\": len(evaluation_data),\n",
        "    \"overall_metrics\": overall_metrics,\n",
        "    \"overall_score\": float(overall_score),\n",
        "    \"performance_level\": performance_level,\n",
        "    \"detailed_results\": results_df.to_dict('records'),\n",
        "    \"recommendations\": recommendations if recommendations else [\"System performance is excellent\"]\n",
        "}\n",
        "\n",
        "with open(results_output_path, 'w') as f:\n",
        "    json.dump(evaluation_summary, f, indent=2)\n",
        "\n",
        "print(f\"✅ Results saved to: {results_output_path}\")\n",
        "print(f\"📊 Evaluation completed successfully!\")\n",
        "print(f\"🎯 Overall Pipeline Score: {overall_score:.3f} ({performance_level})\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
