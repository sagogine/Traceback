{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Task 5: RAGAS Evaluation - Golden Test Data Set\n",
        "\n",
        "## ⚠️ Important Note\n",
        "If you encounter a `KeyError: 'question'` error, **skip the problematic cell** and use the \"SAFE ALTERNATIVE\" cell instead. The RAGAS results DataFrame may not include the original question column.\n",
        "\n",
        "## Objective\n",
        "Assess the Traceback pipeline using the RAGAS framework with key metrics:\n",
        "- **Faithfulness**: How factually accurate are the generated responses?\n",
        "- **Response Relevance**: How relevant are the responses to the questions?\n",
        "- **Context Precision**: How precise is the retrieved context?\n",
        "- **Context Recall**: How well does the context cover the ground truth?\n",
        "\n",
        "## Methodology\n",
        "1. Create a golden test dataset with ground truth answers\n",
        "2. Generate responses using our Traceback system\n",
        "3. Evaluate using RAGAS metrics\n",
        "4. Analyze results and draw conclusions about pipeline effectiveness\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Environment setup complete\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from typing import List, Dict, Any\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv()\n",
        "\n",
        "# Verify API keys\n",
        "if not os.getenv(\"OPENAI_API_KEY\"):\n",
        "    raise RuntimeError(\"OPENAI_API_KEY is not set. Create a .env file or export it in your shell.\")\n",
        "\n",
        "print(\"✅ Environment setup complete\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Imports complete\n"
          ]
        }
      ],
      "source": [
        "# Add src to path for imports\n",
        "sys.path.insert(0, str(Path.cwd().parent / \"src\"))\n",
        "\n",
        "# Import RAGAS components\n",
        "from ragas import evaluate\n",
        "from ragas.metrics import (\n",
        "    faithfulness,\n",
        "    answer_relevancy,\n",
        "    context_precision,\n",
        "    context_recall\n",
        ")\n",
        "from datasets import Dataset\n",
        "\n",
        "# Import our Traceback system\n",
        "from tracebackcore.core import traceback_graph, lineage_retriever, AgentState, initialize_system\n",
        "\n",
        "print(\"✅ Imports complete\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Initialize Traceback System\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🚀 Initializing Traceback system...\n",
            "🚀 Initializing Traceback system...\n",
            "✅ Traceback system initialized successfully\n",
            "✅ Traceback system initialized successfully\n"
          ]
        }
      ],
      "source": [
        "# Initialize the Traceback system\n",
        "print(\"🚀 Initializing Traceback system...\")\n",
        "initialize_system()\n",
        "print(\"✅ Traceback system initialized successfully\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Create Golden Test Dataset\n",
        "\n",
        "We'll create a comprehensive test dataset covering various incident scenarios with ground truth answers.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Created golden test dataset with 5 test cases\n"
          ]
        }
      ],
      "source": [
        "# Define our golden test dataset\n",
        "golden_test_data = [\n",
        "    {\n",
        "        \"question\": \"Job curated.sales_orders failed — who's impacted?\",\n",
        "        \"ground_truth\": \"The curated.sales_orders job failure impacts curated.revenue_summary and analytics.customer_behavior tables. These downstream systems depend on sales order data for revenue calculations and customer behavior analysis. The blast radius includes all analytics dashboards and reporting tools that rely on this data.\",\n",
        "        \"context\": [\n",
        "            \"Sales orders pipeline processes raw order data into curated datasets for analytics and reporting.\",\n",
        "            \"SELECT * FROM curated.sales_orders WHERE order_date >= CURRENT_DATE - 1\",\n",
        "            \"Data pipeline incident response procedures: 1. Acknowledge incident 2. Assess impact 3. Determine blast radius 4. Notify stakeholders\"\n",
        "        ]\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"What should I do if raw.sales_orders has quality issues?\",\n",
        "        \"ground_truth\": \"If raw.sales_orders has quality issues, you should: 1) Acknowledge the incident and notify stakeholders, 2) Assess the impact on downstream systems (curated.sales_orders, curated.revenue_summary, analytics.customer_behavior), 3) Investigate the root cause by reviewing data quality checks and logs, 4) Implement data cleansing procedures, 5) Reprocess the affected data through the pipeline, 6) Validate the corrected data, and 7) Monitor for future quality issues.\",\n",
        "        \"context\": [\n",
        "            \"Data pipeline incident response procedures: 1. Acknowledge incident 2. Assess impact 3. Determine blast radius 4. Notify stakeholders\",\n",
        "            \"Sales orders pipeline processes raw order data into curated datasets for analytics and reporting.\",\n",
        "            \"Data quality standards require validation of order completeness, customer information accuracy, and product details consistency.\"\n",
        "        ]\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"Which dashboards depend on curated.revenue_summary?\",\n",
        "        \"ground_truth\": \"Dashboards that depend on curated.revenue_summary include: 1) Revenue Performance Dashboard - shows daily/monthly revenue trends, 2) Sales Forecasting Dashboard - uses revenue data for predictive analytics, 3) Executive Financial Summary - provides high-level revenue metrics, 4) Monthly Revenue Reports - detailed revenue breakdowns by product/customer, and 5) Customer Analytics Dashboard - analyzes revenue per customer segment.\",\n",
        "        \"context\": [\n",
        "            \"Revenue summary aggregates sales data for executive reporting and business intelligence.\",\n",
        "            \"SELECT SUM(revenue) FROM curated.revenue_summary GROUP BY month\",\n",
        "            \"Executive dashboards require real-time revenue data for strategic decision making.\"\n",
        "        ]\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"How do I troubleshoot a data pipeline failure?\",\n",
        "        \"ground_truth\": \"To troubleshoot a data pipeline failure: 1) Check system logs for error messages and stack traces, 2) Verify data source availability and connectivity, 3) Validate input data quality and format, 4) Check resource utilization (CPU, memory, disk space), 5) Review pipeline configuration and dependencies, 6) Test individual pipeline components in isolation, 7) Check for schema changes or data format modifications, 8) Verify authentication and permissions, 9) Monitor downstream system health, and 10) Document findings and implement preventive measures.\",\n",
        "        \"context\": [\n",
        "            \"Data pipeline incident response procedures: 1. Acknowledge incident 2. Assess impact 3. Determine blast radius 4. Notify stakeholders\",\n",
        "            \"Pipeline monitoring includes checking data freshness, quality metrics, and processing times.\",\n",
        "            \"Common pipeline failures include data quality issues, resource constraints, and configuration errors.\"\n",
        "        ]\n",
        "    },\n",
        "    {\n",
        "        \"question\": \"What is the SLA for curated.sales_orders data freshness?\",\n",
        "        \"ground_truth\": \"The SLA for curated.sales_orders data freshness is 4 hours from source data availability. This means that once raw sales order data is available, the curated.sales_orders table should be updated within 4 hours. The SLA includes: 1) Data processing completion within 4 hours, 2) Data quality validation within 6 hours, 3) Downstream system updates within 8 hours, and 4) 99.5% uptime for the processing pipeline.\",\n",
        "        \"context\": [\n",
        "            \"Sales orders pipeline processes raw order data into curated datasets for analytics and reporting.\",\n",
        "            \"Data freshness SLA requires updates within 4 hours of source data availability.\",\n",
        "            \"Pipeline monitoring includes checking data freshness, quality metrics, and processing times.\"\n",
        "        ]\n",
        "    }\n",
        "]\n",
        "\n",
        "print(f\"✅ Created golden test dataset with {len(golden_test_data)} test cases\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Generate Responses Using Traceback System\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Response generation function defined\n"
          ]
        }
      ],
      "source": [
        "def generate_traceback_response(question: str) -> Dict[str, Any]:\n",
        "    \"\"\"Generate response using our Traceback system.\"\"\"\n",
        "    try:\n",
        "        # Create initial state\n",
        "        initial_state = AgentState(\n",
        "            question=question,\n",
        "            context=[],\n",
        "            impact_assessment=None,\n",
        "            blast_radius=None,\n",
        "            recommended_actions=None,\n",
        "            incident_brief=None,\n",
        "            current_step=\"supervisor\",\n",
        "            error=None\n",
        "        )\n",
        "        \n",
        "        # Run the workflow\n",
        "        result = traceback_graph.invoke(initial_state)\n",
        "        \n",
        "        # Get retrieved context from the lineage retriever\n",
        "        retrieved_docs = lineage_retriever.search_with_lineage(question, k=5)\n",
        "        retrieved_contexts = [doc.page_content for doc in retrieved_docs]\n",
        "        \n",
        "        # Extract relevant information\n",
        "        return {\n",
        "            \"answer\": result.get(\"incident_brief\", \"No response generated\"),\n",
        "            \"context\": retrieved_contexts,  # Use actual retrieved context\n",
        "            \"blast_radius\": result.get(\"blast_radius\", []),\n",
        "            \"impact_assessment\": result.get(\"impact_assessment\", {})\n",
        "        }\n",
        "    except Exception as e:\n",
        "        return {\n",
        "            \"answer\": f\"Error generating response: {str(e)}\",\n",
        "            \"context\": [],\n",
        "            \"blast_radius\": [],\n",
        "            \"impact_assessment\": {}\n",
        "        }\n",
        "\n",
        "print(\"✅ Response generation function defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🔄 Generating responses using Traceback system...\n",
            "Processing test case 1/5: Job curated.sales_orders failed — who's impacted?...\n",
            "Processing test case 2/5: What should I do if raw.sales_orders has quality i...\n",
            "Processing test case 3/5: Which dashboards depend on curated.revenue_summary...\n",
            "Processing test case 4/5: How do I troubleshoot a data pipeline failure?...\n",
            "Processing test case 5/5: What is the SLA for curated.sales_orders data fres...\n",
            "✅ Generated responses for 5 test cases\n"
          ]
        }
      ],
      "source": [
        "# Generate responses for all test cases\n",
        "print(\"🔄 Generating responses using Traceback system...\")\n",
        "\n",
        "evaluation_data = []\n",
        "for i, test_case in enumerate(golden_test_data):\n",
        "    print(f\"Processing test case {i+1}/{len(golden_test_data)}: {test_case['question'][:50]}...\")\n",
        "    \n",
        "    # Generate response\n",
        "    response = generate_traceback_response(test_case[\"question\"])\n",
        "    \n",
        "    # Prepare data for RAGAS evaluation\n",
        "    # Use actual retrieved context from our system, not predefined context\n",
        "    evaluation_data.append({\n",
        "        \"question\": test_case[\"question\"],\n",
        "        \"answer\": response[\"answer\"],\n",
        "        \"contexts\": response[\"context\"],  # Use actual retrieved context\n",
        "        \"ground_truth\": test_case[\"ground_truth\"]\n",
        "    })\n",
        "\n",
        "print(f\"✅ Generated responses for {len(evaluation_data)} test cases\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. RAGAS Evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "📊 RAGAS dataset created with 5 samples\n",
            "Dataset columns: ['question', 'answer', 'contexts', 'ground_truth']\n",
            "\n",
            "🔍 Sample data format verification:\n",
            "Question: Job curated.sales_orders failed — who's impacted?...\n",
            "Answer length: 3128 characters\n",
            "Contexts count: 4\n",
            "Contexts type: <class 'list'>\n",
            "First context: SELECT * FROM curated.sales_orders WHERE order_dat...\n",
            "Ground truth length: 312 characters\n"
          ]
        }
      ],
      "source": [
        "# Convert to RAGAS Dataset format\n",
        "ragas_dataset = Dataset.from_list(evaluation_data)\n",
        "\n",
        "print(f\"📊 RAGAS dataset created with {len(ragas_dataset)} samples\")\n",
        "print(f\"Dataset columns: {ragas_dataset.column_names}\")\n",
        "\n",
        "# Verify the data format\n",
        "print(\"\\n🔍 Sample data format verification:\")\n",
        "sample = ragas_dataset[0]\n",
        "print(f\"Question: {sample['question'][:50]}...\")\n",
        "print(f\"Answer length: {len(sample['answer'])} characters\")\n",
        "print(f\"Contexts count: {len(sample['contexts'])}\")\n",
        "print(f\"Contexts type: {type(sample['contexts'])}\")\n",
        "print(f\"First context: {sample['contexts'][0][:50]}...\")\n",
        "print(f\"Ground truth length: {len(sample['ground_truth'])} characters\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Note: RAGAS EvaluationResult Object\n",
        "RAGAS returns an `EvaluationResult` object, not a dictionary. To access the results, use:\n",
        "- `result.to_pandas()` to get a DataFrame\n",
        "- `result.samples` to get individual sample results\n",
        "- `result.metrics` to get metric names\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🧪 Testing RAGAS evaluation with first 2 samples...\n",
            "🔄 Running RAGAS evaluation on test subset...\n",
            "This may take a few minutes...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "85e800b1951642219d7a0a4520e2e01c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluating:   0%|          | 0/8 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ RAGAS test evaluation completed!\n",
            "Test result type: <class 'ragas.dataset_schema.EvaluationResult'>\n",
            "Test results shape: (2, 8)\n",
            "Test results columns: ['user_input', 'retrieved_contexts', 'response', 'reference', 'faithfulness', 'answer_relevancy', 'context_precision', 'context_recall']\n",
            "\n",
            "📊 Test Results Summary:\n",
            "faithfulness: 0.492\n",
            "answer_relevancy: 0.900\n",
            "context_precision: 0.333\n",
            "context_recall: 0.476\n"
          ]
        }
      ],
      "source": [
        "# Test with a smaller subset first to verify everything works\n",
        "print(\"🧪 Testing RAGAS evaluation with first 2 samples...\")\n",
        "\n",
        "# Create a small test dataset\n",
        "test_dataset = Dataset.from_list(evaluation_data[:2])\n",
        "\n",
        "# Define metrics to evaluate\n",
        "metrics = [\n",
        "    faithfulness,      # How factually accurate are the responses?\n",
        "    answer_relevancy, # How relevant are the responses to the questions?\n",
        "    context_precision, # How precise is the retrieved context?\n",
        "    context_recall     # How well does the context cover the ground truth?\n",
        "]\n",
        "\n",
        "print(\"🔄 Running RAGAS evaluation on test subset...\")\n",
        "print(\"This may take a few minutes...\")\n",
        "\n",
        "# Run evaluation on test subset\n",
        "test_result = evaluate(\n",
        "    test_dataset,\n",
        "    metrics=metrics\n",
        ")\n",
        "\n",
        "print(\"✅ RAGAS test evaluation completed!\")\n",
        "print(f\"Test result type: {type(test_result)}\")\n",
        "\n",
        "# Convert to pandas DataFrame to see the results\n",
        "test_df = test_result.to_pandas()\n",
        "print(f\"Test results shape: {test_df.shape}\")\n",
        "print(f\"Test results columns: {list(test_df.columns)}\")\n",
        "print(\"\\n📊 Test Results Summary:\")\n",
        "for metric in ['faithfulness', 'answer_relevancy', 'context_precision', 'context_recall']:\n",
        "    if metric in test_df.columns:\n",
        "        print(f\"{metric}: {test_df[metric].mean():.3f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "📋 Detailed Test Results:\n",
            "==================================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user_input</th>\n",
              "      <th>retrieved_contexts</th>\n",
              "      <th>response</th>\n",
              "      <th>reference</th>\n",
              "      <th>faithfulness</th>\n",
              "      <th>answer_relevancy</th>\n",
              "      <th>context_precision</th>\n",
              "      <th>context_recall</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Job curated.sales_orders failed — who's impacted?</td>\n",
              "      <td>[SELECT * FROM curated.sales_orders WHERE orde...</td>\n",
              "      <td># Incident Brief: Failure of Job `curated.sale...</td>\n",
              "      <td>The curated.sales_orders job failure impacts c...</td>\n",
              "      <td>0.166667</td>\n",
              "      <td>0.892469</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>0.666667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>What should I do if raw.sales_orders has quali...</td>\n",
              "      <td>[Sales orders pipeline processes raw order dat...</td>\n",
              "      <td># Incident Brief: Quality Issues in `raw.sales...</td>\n",
              "      <td>If raw.sales_orders has quality issues, you sh...</td>\n",
              "      <td>0.818182</td>\n",
              "      <td>0.906665</td>\n",
              "      <td>0.416667</td>\n",
              "      <td>0.285714</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                          user_input  \\\n",
              "0  Job curated.sales_orders failed — who's impacted?   \n",
              "1  What should I do if raw.sales_orders has quali...   \n",
              "\n",
              "                                  retrieved_contexts  \\\n",
              "0  [SELECT * FROM curated.sales_orders WHERE orde...   \n",
              "1  [Sales orders pipeline processes raw order dat...   \n",
              "\n",
              "                                            response  \\\n",
              "0  # Incident Brief: Failure of Job `curated.sale...   \n",
              "1  # Incident Brief: Quality Issues in `raw.sales...   \n",
              "\n",
              "                                           reference  faithfulness  \\\n",
              "0  The curated.sales_orders job failure impacts c...      0.166667   \n",
              "1  If raw.sales_orders has quality issues, you sh...      0.818182   \n",
              "\n",
              "   answer_relevancy  context_precision  context_recall  \n",
              "0          0.892469           0.250000        0.666667  \n",
              "1          0.906665           0.416667        0.285714  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "✅ Test PASSED: Ready for full evaluation\n"
          ]
        }
      ],
      "source": [
        "# Display detailed test results\n",
        "print(\"📋 Detailed Test Results:\")\n",
        "print(\"=\" * 50)\n",
        "display(test_df)\n",
        "\n",
        "# Check if test passed (all metrics > 0)\n",
        "test_passed = all(test_df[metric].mean() > 0 for metric in ['faithfulness', 'answer_relevancy'] if metric in test_df.columns)\n",
        "print(f\"\\n✅ Test {'PASSED' if test_passed else 'FAILED'}: Ready for full evaluation\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🚀 Running full RAGAS evaluation on all samples...\n",
            "This may take several minutes...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6c8a3208a42e461f94c728c83b110c6e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluating:   0%|          | 0/20 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ RAGAS evaluation completed!\n"
          ]
        }
      ],
      "source": [
        "# Run full evaluation on all samples\n",
        "print(\"🚀 Running full RAGAS evaluation on all samples...\")\n",
        "print(\"This may take several minutes...\")\n",
        "\n",
        "# Run evaluation on full dataset\n",
        "result = evaluate(\n",
        "    ragas_dataset,\n",
        "    metrics=metrics\n",
        ")\n",
        "\n",
        "print(\"✅ RAGAS evaluation completed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Results Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "📊 RAGAS Evaluation Results:\n",
            "==================================================\n",
            "\n",
            "🎯 Overall Performance Metrics:\n",
            "Faithfulness        : 0.655\n",
            "Answer Relevancy    : 0.873\n",
            "Context Precision   : 0.117\n",
            "Context Recall      : 0.190\n",
            "\n",
            "📋 Detailed Results:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user_input</th>\n",
              "      <th>retrieved_contexts</th>\n",
              "      <th>response</th>\n",
              "      <th>reference</th>\n",
              "      <th>faithfulness</th>\n",
              "      <th>answer_relevancy</th>\n",
              "      <th>context_precision</th>\n",
              "      <th>context_recall</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Job curated.sales_orders failed — who's impacted?</td>\n",
              "      <td>[SELECT * FROM curated.sales_orders WHERE orde...</td>\n",
              "      <td># Incident Brief: Failure of Job `curated.sale...</td>\n",
              "      <td>The curated.sales_orders job failure impacts c...</td>\n",
              "      <td>0.838710</td>\n",
              "      <td>0.892469</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>0.666667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>What should I do if raw.sales_orders has quali...</td>\n",
              "      <td>[Sales orders pipeline processes raw order dat...</td>\n",
              "      <td># Incident Brief: Quality Issues in `raw.sales...</td>\n",
              "      <td>If raw.sales_orders has quality issues, you sh...</td>\n",
              "      <td>0.806452</td>\n",
              "      <td>0.907872</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.285714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Which dashboards depend on curated.revenue_sum...</td>\n",
              "      <td>[Sales orders pipeline processes raw order dat...</td>\n",
              "      <td># Incident Brief: Curated Revenue Summary Depe...</td>\n",
              "      <td>Dashboards that depend on curated.revenue_summ...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.859911</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>How do I troubleshoot a data pipeline failure?</td>\n",
              "      <td>[Data pipeline incident response procedures: 1...</td>\n",
              "      <td># Incident Brief: Data Pipeline Failure - Sale...</td>\n",
              "      <td>To troubleshoot a data pipeline failure: 1) Ch...</td>\n",
              "      <td>0.771429</td>\n",
              "      <td>0.826410</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>What is the SLA for curated.sales_orders data ...</td>\n",
              "      <td>[SELECT * FROM curated.sales_orders WHERE orde...</td>\n",
              "      <td># Incident Brief: Curated Sales Orders Data Fr...</td>\n",
              "      <td>The SLA for curated.sales_orders data freshnes...</td>\n",
              "      <td>0.857143</td>\n",
              "      <td>0.880744</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                          user_input  \\\n",
              "0  Job curated.sales_orders failed — who's impacted?   \n",
              "1  What should I do if raw.sales_orders has quali...   \n",
              "2  Which dashboards depend on curated.revenue_sum...   \n",
              "3     How do I troubleshoot a data pipeline failure?   \n",
              "4  What is the SLA for curated.sales_orders data ...   \n",
              "\n",
              "                                  retrieved_contexts  \\\n",
              "0  [SELECT * FROM curated.sales_orders WHERE orde...   \n",
              "1  [Sales orders pipeline processes raw order dat...   \n",
              "2  [Sales orders pipeline processes raw order dat...   \n",
              "3  [Data pipeline incident response procedures: 1...   \n",
              "4  [SELECT * FROM curated.sales_orders WHERE orde...   \n",
              "\n",
              "                                            response  \\\n",
              "0  # Incident Brief: Failure of Job `curated.sale...   \n",
              "1  # Incident Brief: Quality Issues in `raw.sales...   \n",
              "2  # Incident Brief: Curated Revenue Summary Depe...   \n",
              "3  # Incident Brief: Data Pipeline Failure - Sale...   \n",
              "4  # Incident Brief: Curated Sales Orders Data Fr...   \n",
              "\n",
              "                                           reference  faithfulness  \\\n",
              "0  The curated.sales_orders job failure impacts c...      0.838710   \n",
              "1  If raw.sales_orders has quality issues, you sh...      0.806452   \n",
              "2  Dashboards that depend on curated.revenue_summ...      0.000000   \n",
              "3  To troubleshoot a data pipeline failure: 1) Ch...      0.771429   \n",
              "4  The SLA for curated.sales_orders data freshnes...      0.857143   \n",
              "\n",
              "   answer_relevancy  context_precision  context_recall  \n",
              "0          0.892469           0.250000        0.666667  \n",
              "1          0.907872           0.333333        0.285714  \n",
              "2          0.859911           0.000000        0.000000  \n",
              "3          0.826410           0.000000        0.000000  \n",
              "4          0.880744           0.000000        0.000000  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Extract results\n",
        "results_df = result.to_pandas()\n",
        "\n",
        "print(\"📊 RAGAS Evaluation Results:\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Display overall metrics\n",
        "overall_metrics = {\n",
        "    \"Faithfulness\": results_df['faithfulness'].mean(),\n",
        "    \"Answer Relevancy\": results_df['answer_relevancy'].mean(),\n",
        "    \"Context Precision\": results_df['context_precision'].mean(),\n",
        "    \"Context Recall\": results_df['context_recall'].mean()\n",
        "}\n",
        "\n",
        "print(\"\\n🎯 Overall Performance Metrics:\")\n",
        "for metric, score in overall_metrics.items():\n",
        "    print(f\"{metric:20}: {score:.3f}\")\n",
        "\n",
        "print(\"\\n📋 Detailed Results:\")\n",
        "display(results_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🔍 Debugging Results DataFrame:\n",
            "========================================\n",
            "DataFrame shape: (5, 8)\n",
            "Available columns: ['user_input', 'retrieved_contexts', 'response', 'reference', 'faithfulness', 'answer_relevancy', 'context_precision', 'context_recall']\n",
            "DataFrame head:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user_input</th>\n",
              "      <th>retrieved_contexts</th>\n",
              "      <th>response</th>\n",
              "      <th>reference</th>\n",
              "      <th>faithfulness</th>\n",
              "      <th>answer_relevancy</th>\n",
              "      <th>context_precision</th>\n",
              "      <th>context_recall</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Job curated.sales_orders failed — who's impacted?</td>\n",
              "      <td>[SELECT * FROM curated.sales_orders WHERE orde...</td>\n",
              "      <td># Incident Brief: Failure of Job `curated.sale...</td>\n",
              "      <td>The curated.sales_orders job failure impacts c...</td>\n",
              "      <td>0.838710</td>\n",
              "      <td>0.892469</td>\n",
              "      <td>0.250000</td>\n",
              "      <td>0.666667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>What should I do if raw.sales_orders has quali...</td>\n",
              "      <td>[Sales orders pipeline processes raw order dat...</td>\n",
              "      <td># Incident Brief: Quality Issues in `raw.sales...</td>\n",
              "      <td>If raw.sales_orders has quality issues, you sh...</td>\n",
              "      <td>0.806452</td>\n",
              "      <td>0.907872</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.285714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Which dashboards depend on curated.revenue_sum...</td>\n",
              "      <td>[Sales orders pipeline processes raw order dat...</td>\n",
              "      <td># Incident Brief: Curated Revenue Summary Depe...</td>\n",
              "      <td>Dashboards that depend on curated.revenue_summ...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.859911</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>How do I troubleshoot a data pipeline failure?</td>\n",
              "      <td>[Data pipeline incident response procedures: 1...</td>\n",
              "      <td># Incident Brief: Data Pipeline Failure - Sale...</td>\n",
              "      <td>To troubleshoot a data pipeline failure: 1) Ch...</td>\n",
              "      <td>0.771429</td>\n",
              "      <td>0.826410</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>What is the SLA for curated.sales_orders data ...</td>\n",
              "      <td>[SELECT * FROM curated.sales_orders WHERE orde...</td>\n",
              "      <td># Incident Brief: Curated Sales Orders Data Fr...</td>\n",
              "      <td>The SLA for curated.sales_orders data freshnes...</td>\n",
              "      <td>0.857143</td>\n",
              "      <td>0.880744</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                          user_input  \\\n",
              "0  Job curated.sales_orders failed — who's impacted?   \n",
              "1  What should I do if raw.sales_orders has quali...   \n",
              "2  Which dashboards depend on curated.revenue_sum...   \n",
              "3     How do I troubleshoot a data pipeline failure?   \n",
              "4  What is the SLA for curated.sales_orders data ...   \n",
              "\n",
              "                                  retrieved_contexts  \\\n",
              "0  [SELECT * FROM curated.sales_orders WHERE orde...   \n",
              "1  [Sales orders pipeline processes raw order dat...   \n",
              "2  [Sales orders pipeline processes raw order dat...   \n",
              "3  [Data pipeline incident response procedures: 1...   \n",
              "4  [SELECT * FROM curated.sales_orders WHERE orde...   \n",
              "\n",
              "                                            response  \\\n",
              "0  # Incident Brief: Failure of Job `curated.sale...   \n",
              "1  # Incident Brief: Quality Issues in `raw.sales...   \n",
              "2  # Incident Brief: Curated Revenue Summary Depe...   \n",
              "3  # Incident Brief: Data Pipeline Failure - Sale...   \n",
              "4  # Incident Brief: Curated Sales Orders Data Fr...   \n",
              "\n",
              "                                           reference  faithfulness  \\\n",
              "0  The curated.sales_orders job failure impacts c...      0.838710   \n",
              "1  If raw.sales_orders has quality issues, you sh...      0.806452   \n",
              "2  Dashboards that depend on curated.revenue_summ...      0.000000   \n",
              "3  To troubleshoot a data pipeline failure: 1) Ch...      0.771429   \n",
              "4  The SLA for curated.sales_orders data freshnes...      0.857143   \n",
              "\n",
              "   answer_relevancy  context_precision  context_recall  \n",
              "0          0.892469           0.250000        0.666667  \n",
              "1          0.907872           0.333333        0.285714  \n",
              "2          0.859911           0.000000        0.000000  \n",
              "3          0.826410           0.000000        0.000000  \n",
              "4          0.880744           0.000000        0.000000  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Debug: Check what columns are available in results_df\n",
        "print(\"🔍 Debugging Results DataFrame:\")\n",
        "print(\"=\" * 40)\n",
        "print(f\"DataFrame shape: {results_df.shape}\")\n",
        "print(f\"Available columns: {list(results_df.columns)}\")\n",
        "print(f\"DataFrame head:\")\n",
        "display(results_df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "📊 RAGAS Evaluation Summary Table:\n",
            "================================================================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Metric</th>\n",
              "      <th>Score</th>\n",
              "      <th>Interpretation</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Faithfulness</td>\n",
              "      <td>0.654747</td>\n",
              "      <td>How factually accurate are the responses?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Answer Relevancy</td>\n",
              "      <td>0.873481</td>\n",
              "      <td>How relevant are the responses to the questions?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Context Precision</td>\n",
              "      <td>0.116667</td>\n",
              "      <td>How precise is the retrieved context?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Context Recall</td>\n",
              "      <td>0.190476</td>\n",
              "      <td>How well does the context cover the ground truth?</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              Metric     Score  \\\n",
              "0       Faithfulness  0.654747   \n",
              "1   Answer Relevancy  0.873481   \n",
              "2  Context Precision  0.116667   \n",
              "3     Context Recall  0.190476   \n",
              "\n",
              "                                      Interpretation  \n",
              "0          How factually accurate are the responses?  \n",
              "1   How relevant are the responses to the questions?  \n",
              "2              How precise is the retrieved context?  \n",
              "3  How well does the context cover the ground truth?  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Create a summary table\n",
        "summary_table = pd.DataFrame({\n",
        "    \"Metric\": [\"Faithfulness\", \"Answer Relevancy\", \"Context Precision\", \"Context Recall\"],\n",
        "    \"Score\": [overall_metrics[\"Faithfulness\"], overall_metrics[\"Answer Relevancy\"], \n",
        "              overall_metrics[\"Context Precision\"], overall_metrics[\"Context Recall\"]],\n",
        "    \"Interpretation\": [\n",
        "        \"How factually accurate are the responses?\",\n",
        "        \"How relevant are the responses to the questions?\",\n",
        "        \"How precise is the retrieved context?\",\n",
        "        \"How well does the context cover the ground truth?\"\n",
        "    ]\n",
        "})\n",
        "\n",
        "print(\"\\n📊 RAGAS Evaluation Summary Table:\")\n",
        "print(\"=\" * 80)\n",
        "display(summary_table)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🔍 Safe Performance Analysis:\n",
            "==================================================\n",
            "Available metrics: ['faithfulness', 'answer_relevancy', 'context_precision', 'context_recall']\n",
            "\n",
            "📊 Overall Performance Metrics:\n",
            "faithfulness        : 0.655\n",
            "answer_relevancy    : 0.873\n",
            "context_precision   : 0.117\n",
            "context_recall      : 0.190\n",
            "\n",
            "📈 Performance Statistics:\n",
            "\n",
            "faithfulness:\n",
            "  Mean: 0.655\n",
            "  Std:  0.367\n",
            "  Min:  0.000\n",
            "  Max:  0.857\n",
            "\n",
            "answer_relevancy:\n",
            "  Mean: 0.873\n",
            "  Std:  0.032\n",
            "  Min:  0.826\n",
            "  Max:  0.908\n",
            "\n",
            "context_precision:\n",
            "  Mean: 0.117\n",
            "  Std:  0.162\n",
            "  Min:  0.000\n",
            "  Max:  0.333\n",
            "\n",
            "context_recall:\n",
            "  Mean: 0.190\n",
            "  Std:  0.294\n",
            "  Min:  0.000\n",
            "  Max:  0.667\n"
          ]
        }
      ],
      "source": [
        "# Safe performance analysis (handles missing columns)\n",
        "print(\"🔍 Safe Performance Analysis:\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Check what metrics are available\n",
        "available_metrics = [col for col in ['faithfulness', 'answer_relevancy', 'context_precision', 'context_recall'] \n",
        "                    if col in results_df.columns]\n",
        "\n",
        "print(f\"Available metrics: {available_metrics}\")\n",
        "\n",
        "if available_metrics:\n",
        "    # Calculate overall performance\n",
        "    overall_performance = {}\n",
        "    for metric in available_metrics:\n",
        "        overall_performance[metric] = results_df[metric].mean()\n",
        "    \n",
        "    print(\"\\n📊 Overall Performance Metrics:\")\n",
        "    for metric, score in overall_performance.items():\n",
        "        print(f\"{metric:20}: {score:.3f}\")\n",
        "    \n",
        "    # Calculate performance statistics\n",
        "    print(\"\\n📈 Performance Statistics:\")\n",
        "    for metric in available_metrics:\n",
        "        print(f\"\\n{metric}:\")\n",
        "        print(f\"  Mean: {results_df[metric].mean():.3f}\")\n",
        "        print(f\"  Std:  {results_df[metric].std():.3f}\")\n",
        "        print(f\"  Min:  {results_df[metric].min():.3f}\")\n",
        "        print(f\"  Max:  {results_df[metric].max():.3f}\")\n",
        "else:\n",
        "    print(\"❌ No metrics found in results DataFrame\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# SAFE ALTERNATIVE: Skip the problematic cell above and use this instead\n",
        "print(\"🔍 SAFE Performance Analysis:\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Check what columns are actually available\n",
        "print(f\"Available columns: {list(results_df.columns)}\")\n",
        "\n",
        "# Check if question column exists\n",
        "if 'question' in results_df.columns:\n",
        "    print(\"✅ Question column found - can do question type analysis\")\n",
        "    # Add question categories safely\n",
        "    results_df['question_type'] = results_df['question'].apply(lambda x: \n",
        "        'Impact Analysis' if 'impacted' in x.lower() else\n",
        "        'Troubleshooting' if 'troubleshoot' in x.lower() or 'should i do' in x.lower() else\n",
        "        'Dependency Analysis' if 'depend' in x.lower() else\n",
        "        'SLA Query' if 'sla' in x.lower() else\n",
        "        'General'\n",
        "    )\n",
        "    \n",
        "    # Group by question type\n",
        "    available_metrics = [col for col in ['faithfulness', 'answer_relevancy', 'context_precision', 'context_recall'] \n",
        "                        if col in results_df.columns]\n",
        "    \n",
        "    if available_metrics:\n",
        "        performance_by_type = results_df.groupby('question_type')[available_metrics].mean().round(3)\n",
        "        print(\"\\n📊 Performance by Question Type:\")\n",
        "        display(performance_by_type)\n",
        "    else:\n",
        "        print(\"❌ No metrics found for grouping\")\n",
        "else:\n",
        "    print(\"⚠️ Question column not found - showing overall performance only\")\n",
        "    \n",
        "    # Show overall performance\n",
        "    available_metrics = [col for col in ['faithfulness', 'answer_relevancy', 'context_precision', 'context_recall'] \n",
        "                        if col in results_df.columns]\n",
        "    \n",
        "    if available_metrics:\n",
        "        print(\"\\n📊 Overall Performance Summary:\")\n",
        "        for metric in available_metrics:\n",
        "            print(f\"{metric}: {results_df[metric].mean():.3f}\")\n",
        "    else:\n",
        "        print(\"❌ No performance metrics found\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Performance Analysis and Conclusions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🔍 Performance Analysis by Question Type:\n",
            "==================================================\n"
          ]
        },
        {
          "ename": "KeyError",
          "evalue": "'question'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
            "\u001b[36mFile \u001b[39m\u001b[32m~/ai-engineering/bootcamp/Traceback/.venv/lib/python3.13/site-packages/pandas/core/indexes/base.py:3812\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3811\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3812\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3813\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
            "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:167\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:196\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7096\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
            "\u001b[31mKeyError\u001b[39m: 'question'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[44]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m50\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Add question categories\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m results_df[\u001b[33m'\u001b[39m\u001b[33mquestion_type\u001b[39m\u001b[33m'\u001b[39m] = \u001b[43mresults_df\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mquestion\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m.apply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \n\u001b[32m      7\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mImpact Analysis\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mimpacted\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m x.lower() \u001b[38;5;28;01melse\u001b[39;00m\n\u001b[32m      8\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mTroubleshooting\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mtroubleshoot\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m x.lower() \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mshould i do\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m x.lower() \u001b[38;5;28;01melse\u001b[39;00m\n\u001b[32m      9\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mDependency Analysis\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mdepend\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m x.lower() \u001b[38;5;28;01melse\u001b[39;00m\n\u001b[32m     10\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mSLA Query\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33msla\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m x.lower() \u001b[38;5;28;01melse\u001b[39;00m\n\u001b[32m     11\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mGeneral\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     12\u001b[39m )\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# Group by question type\u001b[39;00m\n\u001b[32m     15\u001b[39m performance_by_type = results_df.groupby(\u001b[33m'\u001b[39m\u001b[33mquestion_type\u001b[39m\u001b[33m'\u001b[39m).agg({\n\u001b[32m     16\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mfaithfulness\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33mmean\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     17\u001b[39m     \u001b[33m'\u001b[39m\u001b[33manswer_relevancy\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33mmean\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     18\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mcontext_precision\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33mmean\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     19\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mcontext_recall\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33mmean\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     20\u001b[39m }).round(\u001b[32m3\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/ai-engineering/bootcamp/Traceback/.venv/lib/python3.13/site-packages/pandas/core/frame.py:4113\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns.nlevels > \u001b[32m1\u001b[39m:\n\u001b[32m   4112\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem_multilevel(key)\n\u001b[32m-> \u001b[39m\u001b[32m4113\u001b[39m indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4114\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[32m   4115\u001b[39m     indexer = [indexer]\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/ai-engineering/bootcamp/Traceback/.venv/lib/python3.13/site-packages/pandas/core/indexes/base.py:3819\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3814\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   3815\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc.Iterable)\n\u001b[32m   3816\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[32m   3817\u001b[39m     ):\n\u001b[32m   3818\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[32m-> \u001b[39m\u001b[32m3819\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   3820\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   3821\u001b[39m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[32m   3822\u001b[39m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[32m   3823\u001b[39m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[32m   3824\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n",
            "\u001b[31mKeyError\u001b[39m: 'question'"
          ]
        }
      ],
      "source": [
        "# Analyze performance by question type\n",
        "print(\"🔍 Performance Analysis by Question Type:\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Add question categories\n",
        "results_df['question_type'] = results_df['question'].apply(lambda x: \n",
        "    'Impact Analysis' if 'impacted' in x.lower() else\n",
        "    'Troubleshooting' if 'troubleshoot' in x.lower() or 'should i do' in x.lower() else\n",
        "    'Dependency Analysis' if 'depend' in x.lower() else\n",
        "    'SLA Query' if 'sla' in x.lower() else\n",
        "    'General'\n",
        ")\n",
        "\n",
        "# Group by question type\n",
        "performance_by_type = results_df.groupby('question_type').agg({\n",
        "    'faithfulness': 'mean',\n",
        "    'answer_relevancy': 'mean',\n",
        "    'context_precision': 'mean',\n",
        "    'context_recall': 'mean'\n",
        "}).round(3)\n",
        "\n",
        "print(\"\\n📊 Performance by Question Type:\")\n",
        "display(performance_by_type)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Identify strengths and weaknesses (with safety checks)\n",
        "print(\"\\n🎯 Performance Strengths and Weaknesses:\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Use the safe overall_performance from previous cell\n",
        "if 'overall_performance' in locals() and overall_performance:\n",
        "    # Find best and worst performing metrics\n",
        "    best_metric = max(overall_performance.items(), key=lambda x: x[1])\n",
        "    worst_metric = min(overall_performance.items(), key=lambda x: x[1])\n",
        "\n",
        "    print(f\"\\n✅ Strongest Performance: {best_metric[0]} ({best_metric[1]:.3f})\")\n",
        "    print(f\"❌ Weakest Performance: {worst_metric[0]} ({worst_metric[1]:.3f})\")\n",
        "\n",
        "    # Calculate overall score\n",
        "    overall_score = np.mean(list(overall_performance.values()))\n",
        "    print(f\"\\n📊 Overall Pipeline Score: {overall_score:.3f}\")\n",
        "\n",
        "    # Performance interpretation\n",
        "    if overall_score >= 0.8:\n",
        "        performance_level = \"Excellent\"\n",
        "    elif overall_score >= 0.7:\n",
        "        performance_level = \"Good\"\n",
        "    elif overall_score >= 0.6:\n",
        "        performance_level = \"Fair\"\n",
        "    else:\n",
        "        performance_level = \"Needs Improvement\"\n",
        "\n",
        "    print(f\"🎯 Performance Level: {performance_level}\")\n",
        "else:\n",
        "    print(\"⚠️ No performance data available for analysis\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Detailed Conclusions and Recommendations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"📋 Detailed Conclusions and Recommendations:\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(\"\\n🔍 Key Findings:\")\n",
        "print(\"-\" * 20)\n",
        "\n",
        "# Faithfulness analysis\n",
        "faithfulness_score = overall_metrics[\"Faithfulness\"]\n",
        "if faithfulness_score >= 0.8:\n",
        "    faithfulness_conclusion = \"The system generates highly factual and accurate responses.\"\n",
        "elif faithfulness_score >= 0.6:\n",
        "    faithfulness_conclusion = \"The system generates mostly accurate responses with some factual inconsistencies.\"\n",
        "else:\n",
        "    faithfulness_conclusion = \"The system has significant factual accuracy issues that need attention.\"\n",
        "\n",
        "print(f\"1. Faithfulness ({faithfulness_score:.3f}): {faithfulness_conclusion}\")\n",
        "\n",
        "# Answer relevancy analysis\n",
        "relevancy_score = overall_metrics[\"Answer Relevancy\"]\n",
        "if relevancy_score >= 0.8:\n",
        "    relevancy_conclusion = \"Responses are highly relevant to the questions asked.\"\n",
        "elif relevancy_score >= 0.6:\n",
        "    relevancy_conclusion = \"Responses are generally relevant but may sometimes miss the mark.\"\n",
        "else:\n",
        "    relevancy_conclusion = \"Responses often lack relevance to the specific questions asked.\"\n",
        "\n",
        "print(f\"2. Answer Relevancy ({relevancy_score:.3f}): {relevancy_conclusion}\")\n",
        "\n",
        "# Context precision analysis\n",
        "precision_score = overall_metrics[\"Context Precision\"]\n",
        "if precision_score >= 0.8:\n",
        "    precision_conclusion = \"The system retrieves highly precise and relevant context.\"\n",
        "elif precision_score >= 0.6:\n",
        "    precision_conclusion = \"The system retrieves reasonably precise context with some noise.\"\n",
        "else:\n",
        "    precision_conclusion = \"The system retrieves context with significant noise and irrelevance.\"\n",
        "\n",
        "print(f\"3. Context Precision ({precision_score:.3f}): {precision_conclusion}\")\n",
        "\n",
        "# Context recall analysis\n",
        "recall_score = overall_metrics[\"Context Recall\"]\n",
        "if recall_score >= 0.8:\n",
        "    recall_conclusion = \"The system retrieves comprehensive context that covers ground truth well.\"\n",
        "elif recall_score >= 0.6:\n",
        "    recall_conclusion = \"The system retrieves adequate context but may miss some important information.\"\n",
        "else:\n",
        "    recall_conclusion = \"The system often misses important context needed for accurate responses.\"\n",
        "\n",
        "print(f\"4. Context Recall ({recall_score:.3f}): {recall_conclusion}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n💡 Recommendations for Improvement:\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "recommendations = []\n",
        "\n",
        "# Faithfulness recommendations\n",
        "if faithfulness_score < 0.8:\n",
        "    recommendations.append(\"• Improve factual accuracy by enhancing the knowledge base and fact-checking mechanisms\")\n",
        "\n",
        "# Relevancy recommendations\n",
        "if relevancy_score < 0.8:\n",
        "    recommendations.append(\"• Enhance response relevance by improving question understanding and response generation\")\n",
        "\n",
        "# Precision recommendations\n",
        "if precision_score < 0.8:\n",
        "    recommendations.append(\"• Improve context precision by refining retrieval algorithms and filtering mechanisms\")\n",
        "\n",
        "# Recall recommendations\n",
        "if recall_score < 0.8:\n",
        "    recommendations.append(\"• Enhance context recall by expanding the knowledge base and improving retrieval coverage\")\n",
        "\n",
        "# General recommendations\n",
        "if overall_score < 0.8:\n",
        "    recommendations.extend([\n",
        "        \"• Consider fine-tuning the LLM on domain-specific data\",\n",
        "        \"• Implement feedback loops to continuously improve performance\",\n",
        "        \"• Add more diverse test cases to the evaluation dataset\",\n",
        "        \"• Consider ensemble methods for better response quality\"\n",
        "    ])\n",
        "\n",
        "if recommendations:\n",
        "    for rec in recommendations:\n",
        "        print(rec)\n",
        "else:\n",
        "    print(\"• System performance is excellent - consider monitoring for consistency\")\n",
        "    print(\"• Expand the test dataset to cover more edge cases\")\n",
        "    print(\"• Implement A/B testing for continuous improvement\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n🎯 Overall Pipeline Effectiveness Assessment:\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "print(f\"\\n📊 Summary Statistics:\")\n",
        "print(f\"• Total Test Cases: {len(evaluation_data)}\")\n",
        "print(f\"• Overall Score: {overall_score:.3f}\")\n",
        "print(f\"• Performance Level: {performance_level}\")\n",
        "print(f\"• Best Metric: {best_metric[0]} ({best_metric[1]:.3f})\")\n",
        "print(f\"• Worst Metric: {worst_metric[0]} ({worst_metric[1]:.3f})\")\n",
        "\n",
        "print(f\"\\n🔍 Key Insights:\")\n",
        "print(f\"• The Traceback system demonstrates {'strong' if overall_score >= 0.7 else 'moderate' if overall_score >= 0.6 else 'weak'} performance across all RAGAS metrics\")\n",
        "print(f\"• {'The system excels at' if best_metric[1] >= 0.8 else 'The system shows good performance in'} {best_metric[0].lower()}\")\n",
        "print(f\"• {'Significant improvement needed in' if worst_metric[1] < 0.6 else 'Some improvement possible in'} {worst_metric[0].lower()}\")\n",
        "\n",
        "print(f\"\\n✅ Conclusion:\")\n",
        "if overall_score >= 0.8:\n",
        "    conclusion = \"The Traceback pipeline is highly effective and ready for production deployment.\"\n",
        "elif overall_score >= 0.7:\n",
        "    conclusion = \"The Traceback pipeline shows good effectiveness with room for targeted improvements.\"\n",
        "elif overall_score >= 0.6:\n",
        "    conclusion = \"The Traceback pipeline demonstrates fair effectiveness but requires significant improvements.\"\n",
        "else:\n",
        "    conclusion = \"The Traceback pipeline needs substantial improvements before production deployment.\"\n",
        "\n",
        "print(conclusion)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Save Results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save detailed results\n",
        "results_output_path = Path.cwd().parent / \"data\" / \"ragas_evaluation_results.json\"\n",
        "\n",
        "evaluation_summary = {\n",
        "    \"evaluation_date\": pd.Timestamp.now().isoformat(),\n",
        "    \"total_test_cases\": len(evaluation_data),\n",
        "    \"overall_metrics\": overall_metrics,\n",
        "    \"overall_score\": float(overall_score),\n",
        "    \"performance_level\": performance_level,\n",
        "    \"detailed_results\": results_df.to_dict('records'),\n",
        "    \"recommendations\": recommendations if recommendations else [\"System performance is excellent\"]\n",
        "}\n",
        "\n",
        "with open(results_output_path, 'w') as f:\n",
        "    json.dump(evaluation_summary, f, indent=2)\n",
        "\n",
        "print(f\"✅ Results saved to: {results_output_path}\")\n",
        "print(f\"📊 Evaluation completed successfully!\")\n",
        "print(f\"🎯 Overall Pipeline Score: {overall_score:.3f} ({performance_level})\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
